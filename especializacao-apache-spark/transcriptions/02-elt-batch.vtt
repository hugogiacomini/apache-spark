Então, pessoal, só para a gente
começar de novo aqui, como está
sendo gravado,
eu vou explicar para vocês um
pouquinho sobre o Kubernetes antes da
gente começar a aula de hoje.
Então, o que acontece? Por que
se faz necessário e por que
o Kubernetes hoje é tão utilizado?
O Kubernetes, existem várias formas de
você dissecar sobre ele, de falar
sobre ele,
mas ele é um sistema de
infraestrutura self -healing, ou seja, ele
é curável, ele é autocurável.
E essa é a questão mais
linda e mais sexy de você
trabalhar com ele.
Então, o que acontece? No mundo
anterior a isso, a gente passou
por três grandes booms em infraestrutura.
O primeiro foi ter máquinas físicas.
Então, a gente tinha hardware ali,
máquinas físicas que estavam dentro da
infraestrutura local.
Depois a gente viu o seguinte,
cara, eu preciso ter alguma tecnologia
que aproveite essas características dessas
máquinas.
Por quê?
Porque ao longo dos anos a
gente começou a ter super máquinas
potentes.
Muito poder de processamento, muito poder
de armazenamento e muita capacidade de
você processar as informações em uma
máquina só, em um servidor com
a Ruby.
Então, hoje a gente tem servidores
de 4 teras de RAM, a
gente tem servidores de 8 teras
de RAM,
a gente tem máquinas que processam,
cara, com Massive Parallel Processing, que
processam diversos processos e threads dentro
de núcleos,
a gente tem afinidade de núcleo,
a gente tem várias coisas legais
em relação a...
A horsepower e a configuração de
máquinas físicas.
Então, o próximo boom foi o
quê?
Cara, a gente vai pegar uma
máquina, né?
Então, a gente vai pegar...
Para com a unha.
A gente vai pegar aqui uma
máquina.
Então, vamos lá.
A gente vai pegar uma máquina
aqui.
Vou pegar uma physical machine.
E o que eu vou fazer?
Eu vou criar...
Vai...
Várias máquinas em cima dela.
Que a gente chama o quê?
De VM, né?
Virtual Machine.
Então, VM1, VM2, VM3.
Então, pensando grotescamente aqui, a gente
está falando de 3 máquinas distintas.
Então, eu vou dar cores diferentes
para elas.
Beleza?
Que são completamente distintas por quê?
Porque possui o seu próprio OS.
Então, aqui pode ser o Windows.
Aqui pode ser Linux, por exemplo.
Aqui pode ser...
Sei lá, um Ubuntu.
Aqui pode ser um Fedora.
Beleza?
O que essas máquinas contêm em
comum a elas?
Bem, elas têm o quê?
CPU.
Estou sendo sucinto aqui, tá?
E elas têm RAM.
Vamos supor que essa máquina aqui
tem, sei lá, 4 cores e
128 GB de RAM.
Essa máquina tem 2 cores.
E 64 GB de RAM.
Boa noite, tudo bom?
Boa noite.
E essa...
Vocês já começaram? Desculpa, só...
Já, eu estou explicando sobre Kubernetes.
Não comecei a aula de hoje
ainda, não.
Ah, muito obrigado.
E aqui eu tenho uma outra
com 6, beleza?
Então, o que acontece?
Essas configurações são herdadas da mãe,
da máquina mãe.
Então, a gente está falando de
6, 12 cores.
E a gente está falando aqui
de 256.
Mais 128, mais 64.
Alguém faz o cálculo para mim
aí.
312, 484, sei lá.
Então, a gente está falando disso
aqui, né?
Beleza?
Legal.
Só que, qual é o problema
aqui agora?
Bem, para a perspectiva de infraestrutura,
isso aqui é maravilhoso, beleza?
Só que, na parte de desenvolvimento...
A gente começou a ter um
sério problema.
Então, vamos supor que eu estou
desenvolvendo aqui uma aplicação.
Então, eu tenho o usuário 1,
o dev 1 e o dev
2.
Então, eu vou trazer aqui o
exemplo de por que container surgiu.
Para vocês entenderem o que acontece
na realidade.
Deixa eu pegar esse carro aqui.
Então, você tem o dev 1.
E o container
E o container 4.
E você tem o dev 2.
Vamos supor que eles estão desenvolvendo
na mesma máquina, né?
O servidor de aplicação.
Aqui é um servidor de aplicação,
tá?
E aí, está tudo funcionando.
Beleza.
Eles estão ali codando, está tudo
certo.
E aí, de repente, esse cara
pega uma nova biblioteca e ele
vai instalar uma nova...
Uma new library.
Beleza?
E essa nova biblioteca, ela vai
ser instalada onde?
Na máquina.
Física.
Certo?
Desculpa, na máquina virtual.
Ela vai estar ali dentro.
Só que mal sabe ele que,
na verdade, isso vai quebrar o
código do nosso coleguinha aqui.
Tá?
Então, isso é um problema no
software, na engenharia de software, muito
foda.
Por quê?
Porque quando você aumenta o time,
quando você estava quebrando, estava tentando
quebrar
aquela sua grande aplicação monolita, não,
monolito, dentro de microserviços, enfim,
você começava a lidar com esse
problema de, cara, dependências na aplicação,
problemas de você gerenciar times, quebrar
isso, fazer com que todas as
dependências funcionem.
A gente está falando de aplicações
gigantescas, né?
E aí, o que veio?
Cara, já é de muito tempo,
tá?
A gente está falando, cara, no
início de Java, enfim, o Java
trabalhava muito com essa
ideia de containers em si.
A Google pegou, basicamente, esse conceito
e falou, cara, e se a
gente saísse desse modelo
aqui e fosse para um modelo
aonde eu tenho uma infraestrutura, que
a minha infraestrutura
ainda continua.
Então, eu tenho uma infraestrutura que
que eu tenho uma infraestrutura que
Se é nas minhas VMs, né,
ou minhas máquinas virtuais, ou as
minhas máquinas físicas,
hoje tanto faz, tá?
Hoje realmente tanto faz, mas eu
vou botar máquina física, que fica
mais fácil de
vocês entenderem aqui, tá?
Então, eu vou pegar aqui a
minha máquina física, né, na minha
infraestrutura aqui,
eu tenho a máquina física.
E aí, dessa máquina física, o
que que eu vou fazer?
Eu vou criar, né, na verdade,
e aí, o que mudou muito
o gaming aqui foi a minha
foi o Docker. Então, o que
o Docker é? O Docker é
uma forma fácil
feita pelo Solomon Cain
em 2000 e, eu não me
lembro
especificamente, está no livro que a
gente está escrevendo,
eu não me lembro, mas o
que ele fez
foi o seguinte, ele pegou essa
complexidade
e abstraiu de uma forma de
um layer.
Então, ele falou o seguinte, cara,
em vez de você virtualizar
um sistema operacional,
você vai conternizar uma
aplicação. Então, imagina agora que você
não precisa instalar, o quê? Um
servidor,
botar todos os bits
ali, instalar o sistema operacional,
cara, configurar esse sistema operacional
para você ter a sua aplicação
rodando.
Não, você vai herdar
essas características do
host,
então o Docker abstrai as configurações
do host e aí você pode
virtualizar as aplicações. É como se
você
tivesse um SQL
Server virtualizado,
é como se você tivesse um
Kafka
virtualizado, é como se você
tivesse um Spark
virtualizado, é como se você tivesse
um
MongoDB virtualizado, é como se você
tivesse uma aplicação
virtualizada, beleza? Então,
você virtualiza a aplicação. Cara,
isso resolveu um trilhão de problemas.
Primeiro, utilização de software,
utilização de redução
de custo, conternização
daquela aplicação. Agora não tem mais
aquela parada
de works on my machine, funciona
na minha
máquina. Não, agora você pega
sua aplicação, você conterniza
aqui e você joga ela em
qualquer
outro lugar e ela vai funcionar,
porque
ela é uma unidade atômica, ela
está
conternizada. Então, está todos os
bits, tudo que precisa funcionar está
dentro
desse contêinerzinho. E você coloca
ele para rodar
num runtime, num container runtime.
Então, o que o Docker é?
Um container
runtime.
Naquela época, o Docker
explodiu, porque ele foi o container
runtime
que mais foi abstraído, que foi
mais fácil
de acontecer, tá? Beleza.
Isso é muito legal.
Então, se você está desenvolvendo hoje,
qual
é o santo grau de desenvolvimento?
Cara, você tem a sua máquina
local,
você abre o Docker, você escreve
a sua
aplicação aqui local, você conterniza
essa aplicação, a gente fez isso
até ontem,
tá? A gente conterniza essa
aplicação e a gente cria o
bitzinho
dessa aplicação, tá lá, conternizado.
Legal. Só que o que
acontece? O Docker é muito foda.
Só que se sua máquina cair,
se o seu sistema operacional aqui
cair, o seu
host, se ele cair,
você tem o quê? Dá um
time.
Beleza?
Porque o Docker caiu.
E
é aí que vem
toda a história do que a
gente chama
de um cara chamado
Kubernetes. O que
que o Kubernetes é? O
Kubernetes nada mais é
do que um orquestrador
de containers.
Então, é como se você
tivesse esse velejador
aqui, você tivesse
vários containers aqui
dentro,
então, várias aplicações
aqui conternizadas, né? Então,
parar aqui pra pensar no
app tal, tal, tal, tal, tal,
tal, tal, tal.
Então, app 1,
né?
O que eu quero fazer aqui?
É, eu quero duplicar.
App 2, app 3.
Então, o que que
a gente poderia ficar horas e
dias falando
sobre isso aqui, mas basicamente...
O filme da capitão...
E aí,
Matheus, se tiver alguma coisa pra
adicionar, me avisa,
tá? Se eu tiver esquecendo alguma
coisa. Então,
aqui estão as aplicações dentro
do Kubernetes. O que que o
Kubernetes garante?
O Kubernetes, na verdade,
embaixo dele, o que que você
tem?
Várias máquinas
que estão aqui
deployadas
pra sustentar. Então, aqui é uma
Physical Machine 1,
Physical Machine 2,
e Physical Machine 3.
São máquinas físicas ali.
Então, quando essa máquina
cai, nada acontece.
Por que que nada acontece?
Porque o Kubernetes vai orquestrar
o seu aplicativo. Olha que coisa
foda.
Vamos supor que
está desse
jeito aqui, ó. Vamos supor,
e eu vou mostrar pra vocês
na prática agora, tá?
Vamos supor o seguinte.
Você tem um aplicativo que você
conterinizou e botou dentro do Kubernetes,
e ele
tá lá alocado no nó,
que a gente chama de nó,
Node 1,
ele tá nesse nó aqui, ó.
No nó, Fee 1.
Beleza? O app 2 tá no
node
Fee 2, e o app 3
tá no
node Fee 3. Vamos supor
que esse nó, ele cai. E
aí
é a hora do choro, né?
O que que
o Kubernetes é fudido? Porque
quando isso aqui cair, automaticamente
o Kubernetes vai identificar que isso
caiu
e ele vai mover esse
aplicativo pra um outro
nó. E aí,
não tem downtime na sua
aplicação. E aí que é a
hora que a gente
explode a cabeça. Mas por que
que não tem
downtime na sua aplicação? Porque
o que que acontece?
Tô pegando algumas notas aqui, tá,
gente,
do treinamento. A gente não tem
downtime
basicamente por causa do conceito
de serviços, de services. Eu vou
mostrar
isso aqui na prática pra vocês,
tá?
E o conceito de
services é uma das coisas que
mais me impressionam
no Kubernetes, né?
Por quê? Porque ele abstrai
a conexão
da aplicação
com o
serviço.
Então, o que que é um
serviço
no Kubernetes? Você não faz
com que a sua aplicação, olha
só que foda,
a sua aplicação nunca,
você nunca vai dar
aplicação pro seu usuário. Seu usuário
nunca vê a aplicação dentro do
Kubernetes. Ele só vê o quê?
Um serviço. Então,
na verdade, o usuário
bate num serviço e o serviço
redireciona pra onde é que ele
pode estar.
Então, se você chega na sua
aplicação
e fala assim, aplicação, eu quero
que você
esteja conterinizado 60 vezes.
Ele vai replicar a sua aplicação
60
vezes e vai botar um serviço
em cima. Então, se 30
caírem, 30 aplicações suas
caírem, ninguém vai saber
que caiu. Se cai 60, 70,
80, ninguém vai saber que caiu.
Porque o serviço vai automaticamente
redirecionar as requisições que
estão chegando para os pods que
estão vivos, né? Então, essa
é a grande sacada do Kubernetes.
A gente entrando em mais detalhes,
tá? Pra eu fechar aqui, aí
vocês
podem fazer perguntas. Eu vou mostrar
pra
vocês isso na prática, que aí
fica mais
fácil de vocês visualizarem.
Matheus, alguma coisa que eu esqueci
aqui?
Não?
Então, vamos lá.
Eu só vou na pessoa antes
de você começar. Eu só vou
fazer uma paradinha aqui.
Eu vou mutar
todo mundo, porque aí fica mais
fácil.
Acho que você não é mutado,
não.
Pode desmutar você.
Desmuta você aí.
Deixa eu te desmutar.
Tá, beleza.
Eu tenho que entrar no...
Sim. O que que acontece?
Pra gente começar do começo, né?
First things first.
Vou compartilhar aqui.
Desculpa.
Eu vou entrar aqui no portal
do Azuri
e vou mostrar pra vocês
o seguinte. Vocês precisam entender
como funciona o Kubernetes?
Interessante, tá? Claro.
É um conhecimento interessante de você
ter.
Da infraestrutura dele,
cara, é interessante, mas não
obrigatório. Por quê? Porque as nuvens
hoje
te oferecem esse serviço gerenciado.
Então, dentro do Azuri, você vem
aqui
em AKS, que é Azure
Kubernetes Service. Inclusive,
um dos caras que é o
líder desse time,
que virou vice -presidente agora, um
dos caras que
criaram o Kubernetes, então ele é
dentro da Microsoft,
que é o Brandon Burns.
E você vem simplesmente aqui,
cria um...
serviço Kubernetes, só lá.
Então, eu vou criar um cluster
Kubernetes. Fala qual é
a subscription que você vai criar.
Você descompartilhou a tela.
Fala qual é...
Fala qual é aqui o...
Acho que agora tá...
Fala aqui qual é
o local, qual é o tipo
que você
quer, OWSHQ,
TRN,
Spark, qual o nome do cluster.
Você fala em qual região ele
está,
East -West, tá?
Se ele vai estar autodisponível
em três zonas diferentes dentro do
mesmo
data center, então, cara, aqui é
basicamente
você tem 99 .95 %
de disponibilidade. Isso quer dizer
que você pode ter um downtime
de
9 minutos e 37, algo desse
tipo, durante um ano. Então, a
sua aplicação
pode ficar 9 .30 e tantos
minutos fora, tá?
Geralmente não fica, mas pode acontecer.
9 .95 de uptime, eu acho
que é isso.
Isso é 25 minutos, algo desse
tipo, tá?
Aqui você vai escolher, ó, as
máquinas que você quer. Então, você
vai falar, cara, eu quero
um cluster de Kubernetes que tenha
mil
nós, né? Ou não, caralho,
eu quero um que tenha
44 nós,
tá? E aí você escolhe o
tipo
da máquina, cria esse cluster
e aí em 15, 20 minutos
você vai
ter acesso a esse cluster.
Vai aparecer aqui pra você
um cluster criado,
tá? Beleza?
E aí o que você faz?
Como que a gente
interage com o Kubernetes? A gente
interage
por uma CLI chamada
kubectl, né?
E aqui você vai conectar com
ele. Então, você vai
acessar lá a sua CLI,
vai digitar e vai se conectar
ao cluster, né?
Quando você se conectar ao cluster,
você vai estar dentro do contexto
para acessá -lo. Então, o que
que vai
acontecer? Vai acontecer
que você vai ter acesso ao
seu cluster
na sua,
no seu CLI.
Então,
eu vou usar o kubectl
pra achar qual é o cluster,
porque eu tenho mais do que
um.
Então, eu vou usar aquele cluster
que eu mostrei
pra vocês aqui agora.
Esse cluster aqui.
Beleza?
Mudei pro cluster, legal.
A primeira coisa que eu quero
ver no cluster,
quando eu digitar k aqui,
ó, k, é uma abreviação
de kubectl.
Get nodes.
Tá? Então, quais são os nós
que eu tenho?
Três máquinas físicas, ó.
VMs, aparecem como VMs aqui na
infraestrutura.
A infraestrutura do Azure vai ser
máquinas virtuais, mas
são máquinas. Então, se você dá
um k,
describe node,
você vai ter a informação desse
nó aqui, ó, que fala, cara,
isso é um nó Linux, Ubuntu,
que tem
a seguinte configuração.
Tem quatro CPUs, tem 14
gigas de RAM, e a gente
tem
um limite de 110 pods que
podem
estar dentro dessa máquina.
Atualmente, esses são os pods
que estão dentro
e você tem tanto de utilização
de
recursos aqui, ó.
115%, 8 %
e tá tudo bem.
Então, você tem o quê? Um
cluster
com três nós.
E aí, você vai pro conceito
de pod.
Eu não vou entrar muito em
detalhe em conceito de pod
porque é propriamente do Kubernetes,
mas é a unidade atômica
do Kubernetes, é o pod, tá?
E dentro do pod tem uns
containers.
Então, dentro de um pod, você
pode ter vários containers.
Só que o que é foda
do Kubernetes é isso aqui, ó.
Eu vou chegar e falar, cara,
vamos supor que
por um desastre,
esse pod
vai morrer.
Não, na verdade eu vou fazer
melhor.
Eu vou acessar o nosso cluster
de produção.
Vou deletar o Kafka.
Kubens ingestion.
E aqui eu tenho um Kafka
instalado. Olha o Kafka aqui, ó.
Um, dois, três.
Então, eu vou chegar e vou
dar um k delete
EDH
zero.
Puxa, eu deletei logo.
Foi o controller, tá?
K delete pod.
Ops.
Tá deletando os pod do Kafka
aí?
É, vou deletar o k delete.
Pode?
Vou deletar um, né? Talvez não
seja o controller.
Deletei ele, tá vendo?
Então, eu tô aqui simulando
uma falha. Qual é o foda
do Kubernetes?
É isso aqui, ó.
Você deletou, ele vai cair em
outro lugar.
Então, o Kubernetes
trabalha com o conceito de estado
desejado.
Isso é uma das coisas mais
lindas
que existem na TI pra mim.
Por isso que o sistema
mais lindo que existe no mundo
é o Kubernetes.
O que que é o estado
desejado?
Eu falei para o Kubernetes
que eu queria três instâncias de
Kafka.
Então, o que que vai acontecer?
Ele vai sempre, sempre
deixar as três
instâncias de Kafka up and running.
Além de ser extremamente rápido, né?
Então, você consegue subir um cluster
de Kafka em dois, três minutos.
Coisa que com o VM você
subiria em 45 minutos,
50 minutos, por exemplo.
Principalmente quando você tá trabalhando com
alta,
com downtime e assim por diante.
Então, o que que o Kubernetes
é?
Ele é um sistema de infraestrutura
autocurável
que ele se recupera.
E ele atende o estado desejado
da sua aplicação.
Então, você fala, olha, eu quero
três aplicações.
Eu quero dez aplicações.
Eu quero mil aplicações.
E ele vai deixar isso. E,
cara,
aqui dentro é um mundo, né?
Então, você pode falar
que aquela aplicação pode escalar
horizontalmente.
Verticalmente.
Você pode falar que essa aplicação
pode ter storage.
Ela pode guardar informação.
Você pode falar, por exemplo, que
essa aplicação
ela pode estar exposta pra internet.
Então, você pode criar um load
balancer.
E aí, as aplicações externas
podem, os acessos externos podem
bater dentro dessa infraestrutura e ser
acessado.
E por aí vai, tá?
Então, isso é o Kubernetes.
Ficou claro aí?
Antes da gente começar.
Me respondam aí no chat.
Se vocês têm alguma dúvida.
Como eu me comunico
com as aplicações dentro dos pods?
Eles têm um endereço físico com
portas específicas?
Exatamente. Então, dentro do pod
você tem os próprios endereçamentos
que ele tem a própria infraestrutura
dele e ele faz isso.
Dizem que um pod foi feito
pra morrer.
Exatamente. O período de um pod
ele é aproximadamente quantos milissegundos?
De alguns segundos, mais ou menos.
Dá uma olhada. Vê se acha
essa informação.
Mas tem uma informação na internet
que fala
que o ciclo de vida do
pod ele é muito rápido.
Por quê, tá?
O pod, basicamente falando,
ele é efêmero.
Isso quer dizer o quê? Ele
não guarda estado
inicialmente no Kubernetes. Começou a acontecer
depois de 2016. Então,
o pod não tem problema ele
morrer.
Porque ele vai ser, ele vai
ser na verdade
ele vai aparecer em outro
nó. Então, às vezes quando
você tem vários pods dentro
do mesmo nó,
o que acontece às vezes é
a
saturação de recursos.
Às vezes o Kubernetes, ele usa
o
scheduler para rebalancear os pods.
Às vezes acontece que muitas requisições
estão acontecendo aqui, dependendo do min,
do max de configuração, ele olha
os mínimos que estão interferindo, ele
raspa, ele deleta, mas não tem
problema. No final das contas, ele
vai deixar
a sua aplicação open running do
outro lado.
É, todo mundo precisa estudar
Kubernetes, com certeza. Quando você carrega
faz, quando você carrega faz um
uma configuração para vários workers,
tudo isso no Dockerfile. Então,
Julia, quando você faz isso, a
gente está
falando de trazer o Docker
para dentro do Kubernetes. Então,
é como se você pegasse e
fizesse...
Ah, Welker, tá. É como se
você
pegasse, por exemplo, e você está
falando
o seguinte, olha, eu criei essa
minha aplicação local
aqui, ela está funcionando, eu containerizei
ela, e agora eu quero que
ela execute
em um ambiente seguro. Por quê?
Porque se ela cair, se tiver
algum
downtime, se tiver algum problema, por
exemplo,
isso não vai acontecer, tá?
Então, automaticamente ele vai cuidar
da sua aplicação. Então, você vai
falar, olha, eu desejo
que dentro do Kubernetes
eu tenha três instâncias dessa aplicação.
Então, ele vai criar três instâncias.
Então, quando as aplicações, quando o
serviço estiver
em cima dessas suas instâncias
e elas estiverem consultando,
ele automaticamente vai balanceando
para onde ele vai levar as
pessoas.
Então, bate no servidor, ele manda
para esse.
Bate no serviço, ele manda para
esse.
Bate no serviço, ele manda para
esse.
Vamos supor que no momento que
bateu no serviço,
ele sabe que aquela aplicação
naquele nó caiu. Então, ele,
enquanto está provisionando outra,
ele vai mudar para os outros
dois, e assim por diante.
Então, sua aplicação sempre vai estar
operacional.
Quanto a máquina física volta, ele
balanceia novamente?
Isso é uma boa pergunta. Na
verdade,
não. Assim como qualquer sistema distribuído
que existe, ele não faz isso
automaticamente.
Muito raramente são sistemas distribuídos
que fazem isso. Por quê?
Porque se um nó cai
e ele volta, você não pode
garantir
que aquele nó que voltou
está em healthy state. Ele pode
ser
em um estado intermitente.
Então, qualquer tecnologia de big data,
qualquer sistema distribuído, não confia na
hora
que ele volta, a não ser
que você diga para ele
que sim. E aí, cada tecnologia
vai ter o seu tipo de
especificação.
Então, no Kubernetes,
como funciona é o seguinte. Ele
tem
um cara que fica olhando todo
mundo ali
e chega uma hora que
realmente tem um nó que está
cheio de
coisa ali dentro. E aí, automaticamente
ele redistribui aqueles pods. Então,
você não pode pedir para ele
redistribuir. Isso pode
acontecer tanto de forma manual, como
nem pode acontecer de forma automática.
Mas a nuvem endereça isso para
você.
Por exemplo, o Google GKE,
que é o Google Kubernetes Engine,
é o primeiro engine que você
pode habilitar
uma flag chamada Autopilot,
que ele vai pilotar o Kubernetes
para você.
Então, a gente já não tinha
preocupação nenhuma
com o Kubernetes quando você ligou
o Autopilot
do GKE. Aí que você realmente
não precisa
se preocupar com nada, porque ele
vai fazer isso tudo
para vocês.
Beleza? Gente, de novo, eu poderia
passar horas
falando sobre isso.
Mas a gente tem
um treinamento de Kubernetes.
Mas
vamos para o que interessa aqui.
Eu espero
que tenha ficado claro para vocês
o que vocês tenham entendido, pelo
menos
a superfície do Kubernetes.
Beleza?
Então,
tá bom. Então, vamos lá.
Tá vendo bem minha tela aí,
Matheus?
Sim.
Então, a gente vai falar hoje.
Ontem a gente
passou pelo processo de
entender, na verdade,
o que é o Spark.
Quais são as capacidades do Spark.
E aí, antes da gente começar,
eu queria saber
das 35 pessoas que estão aqui
na sala, são 35, é isso
mesmo?
Das 36 pessoas que estão aqui,
35
sem mim. Matheus, todo mundo,
me coloca aí qual foi a
parte
mais legal do dia de ontem.
Quero saber o que vocês acharam
e se tem
alguma dúvida enquanto eu bebo uma
água.
Sim.
O que vocês acharam de ontem?
Qual foi a parte mais legal?
O que destravou em você?
A estrutura da aplicação.
O Spark chama no SQL realmente.
A tecnologia, a parte que permeia
todas
as nuvens e soluções. Gostei
muito da contextualização da história.
Legal também. Saber que
o core do Spark SQL a
gente vai ver mais em
detalhes ainda hoje, Bruno. Então,
a gente vai ver literalmente como
isso funciona
ali por debaixo das cortinas
e como tudo isso
interage
no ambiente.
Do Spark. Então, a gente vai
ver isso hoje
em detalhes.
Mas o que, gente?
Conheci a história. Sim, a história
eu também acho muito importante pra
gente
contextualizar sobre a tecnologia.
É muito importante.
Gostei mesmo do código rodando em
diferentes
ambientes. É, isso também
bem fera. Concordo
com você. É muito bom
isso porque vocês já setaram
a cabeça no dia 1 de
que
não é sobre a tecnologia,
mas sim sobre a solução. Então,
se
você vai sair aqui do treinamento
sabendo o seguinte.
Ah, legal. O Databricks é muito
legal.
A gente vai olhar o Databricks
hoje em detalhe.
Ah, a experiência de notebook
é muito foda realmente. Enfim.
Cara, mas eu sei escrever uma
aplicação
Spark. Então,
independentemente do ambiente, eu estou
apto a operacionalizar, a
trabalhar com o Spark. Então, é
isso que eu quero que vocês
tenham em mente. Beleza?
Cara, o Spark
evoluiu e não é mais necessário
fazer a configuração manual. Como conhecer
estruturão diferencial. Perfeito. Ali a gente
não
precisa mais gastar tempos, horas
e, cara, incontáveis momentos
configurando, botando isso.
Principalmente para quem vem de on
-prem.
Sabe muito bem como isso é
doloroso, né?
Então, sim. Concordo também.
Para você, Matheus.
Qual a parte mais legal do
treinamento
de jogo?
Eu gosto muito dessa parte do
conceito,
né? O entendimento de driver,
executor. É uma
coisa que parece que o pessoal
acha que
perante para quem é de
infra, né? É bem legal.
O pessoal acha que não precisa
saber, mas
isso é um diferencial
ferrado nos edge cases, lá na
parte avançada. Quando você realmente
começa a fazer troubleshooting de Spark.
O que acontece, né? Não tem
jeito. Não tem como fugir disso,
não.
Com certeza. Mais alguém, pessoal?
Antes da gente começar? Alguma dúvida
antes da gente começar?
Eu quero que vocês comecem
clean, né? Comecem limpos
do dia de ontem. Esvaziem a
cabeça,
né? Eu acho que todo mundo
que trabalhou o dia todo teve
um dia estressante,
cansativo, às vezes não dormiu bem.
Mas o que eu quero deixar
claro, eu tava até me preparando
pra poder falar sobre isso pra
vocês,
é que é muito importante, gente,
que vocês
que tão aqui, todo mundo que
tá aqui,
quero que vocês entendam o seguinte,
é o momento de vocês para
vocês. Se permitam,
entendeu? Por que que eu tô
falando isso?
Porque às vezes a gente faz
aquisição de um treinamento,
às vezes a gente
não faz aquele extra mile,
aquela milha extra, sabe?
E é muito importante que vocês
façam essa
extra, essa milha extra, né?
No treinamento. Importante que vocês, cara,
se vocês conseguem, né? Coloca
o telefone em modo avião,
tenta focar aqui, cada minuto dessa
incepção, desse momento que a gente
tá junto
aqui, tô tentando transmitir pra
vocês, é importante vocês capturarem,
tentar digerir essa informação,
tentar perguntar
por quê? Porque isso vai ser
o fundamento do que você vai
aprender daqui pra frente. Então, em
vez de
você fazer o treinamento três, quatro
vezes,
porque você literalmente teve
dificuldade, você vai
fazer esse treinamento mais duas, três
vezes, você vai
ver essa gravação
pensando nas coisas que podem ser
apresentadas no seu conhecimento. Então, se
permita, cara, prestar atenção,
se permita tá aqui fazer, dar
o melhor de você. Essa milha
extra, às vezes, pra certas coisas,
é muito
importante. No meu dia, eu sempre
tenho um momento da milha extra,
né?
Então, eu faço várias atividades durante
o dia,
assim como vocês,
eu também trabalho consideravelmente
bastante, eu faço muitas coisas
ao mesmo tempo, eu tenho que
tomar muito cuidado,
nas coisas que realmente são importantes.
Tudo que eu faço é muito
importante, mas
não dá pra você pegar tudo
e priorizar
como high priority, né? Não tem
como fazer isso.
Mas durante o seu dia, tem
como você
elencar o seguinte, cara, eu vou
deixar minha energia aqui
pra esse tipo de coisa. Eu
sugiro que vocês
façam isso, porque cada dia desse
treinamento, ele é desenhado pra desbloquear
uma coisa dentro da sua cabeça,
né? Então, ontem
o desbloqueio que eu quero que
vocês tenham
é que Spark é pra todo
mundo, Spark
é uma tecnologia que existe
em qualquer empresa de Big Data,
né?
E Spark não é Databricks,
Spark não é, literalmente,
algo diferentemente, tá?
O Spark é, de fato,
o quê? Uma engine de processamento
distribuída que você precisa saber se
você tá
em uma área de engenharia de
dados.
E você pode utilizar ela em
qualquer nuvem,
em qualquer local, contanto que você
saiba escrevê -lo. Esse é o
desbloqueio
do dia 1. No dia 2,
a gente vai ver
tudo que a gente precisa
pra fazer o quê? Batch. Batch
com as melhores práticas, batch
seguindo um pattern, eu vou mostrar
pra vocês
um pattern que vocês vão usar
em todos
os projetos, literalmente todos.
É uma casca, é um shallow,
é o que a gente chama
do skeleton, ou do
scaffold, o quê que é isso?
É uma base
que a gente tem um processo
que você vai seguir esse processo,
sabe? Primeiro eu pego o dado,
depois eu faço isso, depois eu
processo
aqui, depois eu cuspo o dado.
Então, esse é o
ciclo de vida do Spark. Então,
na hora que você entender, isso
vai
ficar mais fácil na hora de
como você vai
trabalhar essa ferramenta. No dia 3,
a gente vai ter outro desbloqueio,
no dia 4
e no dia 5 também. Então,
eu quero que vocês
fiquem super focados e se vocês
puderem interagir o máximo possível,
melhor, porque isso quer dizer que
vocês estão
comentando o quê? Essa informação
dentro de vocês. Então, a melhor
maneira
de você realmente aprender, não é
só
escutar, mas é falar, é botar
aqui no chat,
eu tô acompanhando o chat aí,
Matheus,
e é realmente você interagir aqui,
beleza?
Então, falando nisso,
eu não achei o link do
Discord, beleza?
Matheus vai passar aí o processo
do Discord pra vocês, tá?
Então, vamos lá.
Primeiro ponto que a gente tem
que entender
é o seguinte. Cara,
será que a gente entende de
fato o que
é o Data Lake? Coloca aí
pra mim
o que vocês acham sobre Data
Lake,
por que eu tô dizendo isso?
Eu tive uma reunião
hoje com um cliente muito grande
e amanhã eu vou ter uma
reunião com um cliente
muito grande, onde os meus
tech leads e os meus sales
managers,
eles me chamam pra
traduzir o conceito de Data Lake
pra
cliente. Por quê? Porque
é um conceito muito convoluted.
O que é um conceito convoluted?
É um
conceito complexo.
Data Lake, simploriamente
falando, ele significa muito
pra uma organização. E às vezes
com facetas diferentes pro
business. E a gente sempre
tem que entender e tá focado
no que o business
diz e como você vai traduzir
isso pra tecnologia
ou pro conceito que você vai
utilizar.
Por que que a gente precisa
entender
o que Data Lake é?
A gente precisa entender o Data
Lake...
Muito bom, Vinícius, é um object
storage raw data.
Então, o que que
acontece? O que que vocês precisam
entender?
São duas facetas do Data Lake.
Primeiro, o que ela significa
pra perspectiva do negócio.
Isso é uma. E segundo,
o que ela significa pra perspectiva
da implementação
técnica. São coisas diferentes.
O business pode ter diversas
visões do que o Data Lake
é.
E o que ele habilita, né?
É o que a gente chama
de data enablement.
O que ele vai fazer no
final das contas
na sua...
O seu ingrediente mais,
eu diria, mais
puro, seria o que?
A governância dos dados.
Tá? Então,
aqui, quando a gente fala
de ter um Data Lake, a
gente tá falando
de disponibilizar
diversas informações
para um local
centralizado. E quando a gente
pensa nisso, talvez, às vezes,
fica um pouco difícil da gente
tecnicamente entender por que que isso
tem
que acontecer. Isso tem que acontecer
hoje justamente porque, pelo gancho
de ontem, a gente tá falando
de uma
empresa que cresce e ela
cresce horizontal e vertical. E
é claro que se você entrar
numa empresa de médio
e grande porte, você nunca vai
ver o seguinte, cara,
a gente usa, por exemplo,
uma tecnologia pra ingestão
de dados. A gente usa
uma tecnologia pra processamento
de dados. A gente usa somente
um Data Warehouse. A gente usa
somente uma
ferramenta de visualização.
Se fosse assim, não existiriam milhares
de ferramentas no mercado. Então, na
verdade,
o que acontece na vida real
é que as empresas, elas têm
silos, tá? Isso muito foi
criado pelo conceito de Data Mart
e de Data Warehouse, que a
gente vai
olhar ali como que isso
desencadeou esse problema, resolveu alguns e
trouxe alguns outros, né? E como
que o Lake
House trabalha isso. Mas
o Lake, ele vem justamente
pra tentar
fazer o que a gente chama
de Data
Democratization. É tentar democratizar
o dado. Por quê?
Porque hoje, por exemplo, se você
for na sua empresa
e fala o seguinte, cara, quanto
que eu vendi ontem?
Uma pergunta
teoricamente simples pra
perspectiva do negócio. Você tem
produtos. Esses produtos são vendidos
diariamente, por exemplo, por um canal
de vendas, ou por um omnichannel,
ou enfim.
E no final das contas, você
quer saber
o resultado dessa operação. Cara,
isso é uma pergunta extremamente
complexa de ser feita e
dependendo da área que você perguntou,
você vai ter
diferentes respostas, né?
Quem nunca viu isso, né? Me
diz aí se vocês já
passaram por isso, né? Dentro do
seu
negócio, você tem exatamente isso acontecendo
agora. Você tem, cara, o time
de
vendas, com o time de produto,
com o time de finance,
com o time de HR, você
faz uma pergunta
e, na verdade, cada um deles
tem respostas
diferentes, né? Às vezes a gente
não vê isso
porque a gente tá ali dentro
de um silo.
Mas o importante é, quando a
gente
traz o conceito de Data Lake,
ele
inicialmente vem pra resolver o quê?
O problema
de volume, né? Por quê?
Porque hoje, numa empresa, né?
A gente tá falando que, cara,
tudo é driveado
pelo desenvolvimento. Então hoje os
desenvolvedores
fazem o quê?
Os desenvolvedores,
eles desenvolvem
aplicações da forma mais
rápida e efetiva que existe.
Então isso quer dizer o quê,
cara?
Usar muito bleeding edge
technology, usar muita tecnologia de ponta,
testar muitas coisas
novas, utilizar diversos frameworks,
utilizar diversas maneiras
pra poder operacionalizar aquela aplicação
e fazer com que seja possível
dele entregar o mais rápido
possível. Isso implica hoje
no mundo de, cara, diferentes
fontes de dados, diferentes magnitudes
da informação. Então, por exemplo, você
tem
banco de dados na sua empresa,
e não somente um,
você vai ter, cara, vários. No
SQL
você vai ter vários também,
né? Aplicações, você vai ter aplicação
web, você vai ter aplicação mobile,
que talvez tem backends diferentes,
muitas vezes tem backends iguais, mas
talvez
tenham regras específicas pra cada um
deles,
né? Você tem arquivos, cara,
CSV, TXT, XML,
JSON, então você tem uma variedade
de fontes diferentes.
E onde você coloca essas fontes
diferentes?
Então, antigamente, quando a gente tá
falando
antes de 2000, a gente tinha
banco de dados
relacionais que funcionavam muito bem, porque
eles estavam
de dados estruturados de um formato
só.
Hoje a gente tá falando de
vários tipos
de formatos. Então fica muito importante
o seguinte, eu encontrar
um ambiente onde eu possa fazer
um processo de
ETL ou ELT, a gente vai
entrar em detalhe aqui,
e ter um ambiente unificado pra
isso.
E é aí que o Data
Lake
entra realmente na perspectiva técnica.
O Data Lake é um ambiente
corporativo
onde você compartilha tipos
de arquivos diferentes ou tipos de
fontes
diferentes no seu modo
cru, no seu modo
raw. Por quê?
Agora a gente vai entender o
detalhe
do porquê o dado
tem que ser cru.
É importante que ele seja cru.
Ele vai ser
enriquecido ao longo do tempo, ele
vai ser
melhorado, mas é muito importante que
você
retenha esse dado cru.
Um dos grandes fatores de você
reter o dado cru
é que, por exemplo, em sistemas
tradicionais
você não tem a habilidade de
reter infinitamente.
Por exemplo, se você olhar pra
um banco de dados
relacional, você não retém
infinitamente dentro de um banco de
dados relacional.
Existe uma limitação de um
banco de dados relacional em relação
a
armazenamento de dados.
Raramente você vai ver bancos de
dados
de 30 teras, de 50 teras,
de 70 teras e assim por
diante.
É difícil você encontrar isso.
É muito raro. Eu já trabalhei,
por exemplo,
o maior banco de dados que
eu já trabalhei
na minha vida, que é um
banco de dados
absurdamente gigante,
que é um banco de dados
de 23 terabytes.
E isso é
absurdo. Não só na perspectiva
de tamanho, mas na perspectiva
de gerenciamento. O banco
de dados relacional não foi desenhado
inicialmente
pra ter essa quantidade de volumes.
Um banco de dados no SQL
não foi
desenhado pra ter transações
distribuídas ou transações
ácidas ali dentro. Ele não é
desenhado
pra isso. Eventualmente o que vai
acontecer
é que cada um vai querer
ser o outro.
Então cada vez mais você vai
ver bancos de dados relacionais
tentando ter características no SQL
Outra coisa, você vai ter vários
no SQL tendo características relacionais
mas isso não quer dizer
que eles são bons naquilo que
eles fazem.
Não quer dizer que seja o
melhor sistema
pra você operacionalizar isso.
E formato arquivo é um grande
problema. Por quê?
Porque o formato arquivo
te traz openness, abertura.
E a abertura é muito boa
pro lado da perspectiva de você
jogar dados dentro de um local.
Mas na perspectiva de você processá
-los
é um grande problema.
Então o Data Lake
ele vem pra desobstruir
e explicar
algumas faces muito importantes
no processo da nossa
aquisição, processamento e entrega
do dado. Luan, existe
algum nome para aquela arquitetura bronze,
prata e silver? Eu vou chegar
lá.
Lá na parte de Delta eu
explico sobre isso.
Rotinas de expulso para bancos de
dados relacionais. Sim, a gente faz
muito, né? Pra quem trabalha com
bancos de dados
relacionais sabe que a gente faz
bastante data purge.
Expulso de dados.
Que é retirar informações. Hoje
você pode colocar essas informações
num tier mais barato. Ou
fazer o offload desses dados
pra um storage na nuvem, pra
ficar
mais barato e eventualmente utilizar uma
outra
característica pra consultar essas
informações.
Ou marcar essas informações
como deletadas ou marcadas
como não utilizáveis,
particionar, comprimir, colocar num
lado diferente ali ou em tabelas
diferentes.
Existem várias formas que os DBAs
trabalham com isso. Mas no final
das contas a gente tá falando
na perspectiva de business, isso não
é
cap, isso é ops. Então é
muito importante
que a gente como engenheiro de
dados
entenda o que é valioso da
gente
realmente trabalhar, tá? Até
porque a gente falava há cinco
anos
atrás, seis anos atrás, a gente
falava
muito disso que o DBA
ia morrer, né? E cara, a
gente
vê cada vez mais isso acontecendo,
né?
Realmente o papel do DBA
de gerenciar banco, de criar
infraestrutura, de provisão de alta
disponibilidade,
fazer backup, enfim, esses dias
já são contados. Eu digo isso
por experiência
mais do que de vida, né?
Eu trabalho
numa empresa fora do país há
nove anos e eu
comecei na área de banco de
dados relacional
e eu vi essa transição acontecendo
diante dos meus olhos, né?
Mais de 150 clientes já saíram
do que a gente chama de
managed service,
dos serviços gerenciados,
pra ir para serviços de soluções
e produtos e analytics. Porque na
verdade
você não precisa gerenciar mais nada.
Hoje você tem a nuvem
que faz isso pra você. Hoje
você tem sistemas
completamente gerenciados, basicamente 100%.
Você não precisa fazer absolutamente nada
na perspectiva do banco de dados.
Só realmente fazer
o que é importante pro negócio,
que é o quê? Codificar.
Escrever regras de negócio,
extrair informação e entregar
valor. É isso que importa pra
um negócio.
Então, existem
três coisas muito importantes aqui,
tá?
A primeira, na verdade cinco coisas
que eu vou
falar aqui, tá? Primeiro ponto.
O que que é o conceito
de data lake, tá? E aí
é uma coisa muito importante que
vocês entendam
agora,
que eu quero ver realmente se
vocês... Eu vou falar
dessa arquitetura, tá gente? Pode ficar
tranquilo.
Então, não precisa sair
daqui do treinamento não. Pode ficar
aqui que eu vou
falar dessa arquitetura da medalha em
marketing.
Eu vou fazer uma pergunta
pra vocês. Vamos ver se vocês
entendem
mesmo.
Data lake é um
repositório cruz de dados, tá?
Esse termo foi travo, coined,
há mais ou menos,
mais ou menos em 2010.
Onde a ideia é democratizar as
informações
que estavam stacked up em silos.
O que que é isso?
Lá em meados de 2005, 2006,
2007, né? A gente tem
um conceito muito forte de
data warehouse e de data mart.
Eventualmente, a gente tinha duas pessoas
que brigavam muito sobre
esses termos, né? O conceito de
data mart e o conceito de
data warehouse.
Que é o top down
e o bottom up.
É um processo criado
por o pai do data warehouse,
que é o Bill Iman, e
um outro
conceito trago pelo
concorrente, né?
Dele, que eu me esqueci o
nome agora.
Até tava vendo como eu não
tinha esquecido esse nome.
Se alguém...
Ralph Kimball, tá? Do Ralph Kimball.
Então a gente tem Ralph Kimball
e Bill Iman sendo os
grandes percursores do conceito que explodiu,
um dos maiores conceitos que explodiu
em TI, que é o data
mart e o data warehouse.
Eventualmente, as pessoas começaram
a utilizar muito o approach do
Ralph Kimball
e criar data
marts departamentais. O que aconteceu
ao longo dos anos, né? A
gente adiantando aí
o tempo, é que a gente
começou
a ter silos de dados. VIEs,
da informação em vários lugares
diferentes da empresa, com vários
significados diferentes do mesmo
dado. E isso fez com que
nós precisássemos de um processo
ou de uma
arquitetura, ou de uma forma de
compartilhar
essas informações entre
os silos, né? E aí daí
vem muito forte o conceito de
data lake.
Mas quando a gente olha o
conceito de
data lake, é muito importante que
vocês entendam que data lake
é um conceito, um data lake
ele não é uma tecnologia, por
exemplo.
Pergunta pra vocês.
Eu consigo fazer
um data lake
dentro...
Eu vou ser bem
grosso, tá?
Eu consigo fazer
um data lake
dentro de um banco de dados
poliglota, por exemplo?
Vamos supor que eu trabalho
dentro do Azure e eu uso
o Cosmos DB
e eu quero ter um data
lake dentro do Cosmos DB.
Isso é possível?
Eu consigo ter um data lake
dentro de um banco de dados
no SQL?
Um banco de dados poliglota é
um banco de dados
que responde por vários tipos de
linguagens
ou de formas diferentes, né?
O Cosmos DB é um banco
de dados único
por causa disso. Então,
por exemplo, o Thiago e a
Juliana
acertaram. E isso é foda.
Por quê? Lucas também. Vocês estão
preparados
a sentar e conversar
sobre o seguinte tal caminho. Vamos
dividir.
Qual é o conceito de data
lake?
Democratizar os dados
na sua forma crua.
Legal. Isso é uma coisa.
Segundo ponto. O que é o
data lake?
O data lake, tecnicamente
falando, é um ambiente onde te
possibilita
você armazenar qualquer tipo de dado
da sua empresa. Então, gente,
parabéns, tá? Por exemplo,
Bruno está falando ali que consegue
fazer isso no Mongo.
O que que
acontece? O data lake é um
conceito.
Como você
faz isso é
outros 500. Claro que, por exemplo,
quando a gente olha as pessoas
utilizando o conceito de data lake
no MongoDB, utilizando o conceito de
data lake
no Cosmos ou em
outros lugares, tá? Isso
soa um pouco grotesco
porque existem alguns pré -requisitos
do que se realmente se fala
no dado cru na sua incepção
normal.
Mas sim, você pode fazer, né?
Isso não quer dizer que seja
a melhor
forma de fazer. Por que que
a gente atribui
isso para um object store? A
gente vai
entender agora. Porque, na verdade,
dentro de um object store,
que são exatamente os data lakes
de mercado,
né? Eles contêm
o que? Qualquer tipo de
dado. Então isso é uma das
grandes
características de um data lake.
É você conseguir botar dado estruturado,
semi -estruturado e não estruturado no
mesmo local.
E quem te entrega isso de
uma
forma com que você consiga fazer
isso muito barato,
escalável e eficiente
na perspectiva do armazenamento?
Blob storage,
data lake geração 2,
Amazon S3, o conceito de AWS
Lake Formation, onde você consegue ter
metadados
e tudo lineage dessa informação,
Google Cloud Storage e assim por
diante. Então esses são os data
lakes
de mercado que a gente mais
conhece.
Mas não atribua
o conceito de data lake ao
object store. Entenda que
data lake é uma coisa,
a tecnologia que você vai usar
para poder endereçar
isso são outras. Normalmente,
95 % dos casos,
você vai usar esses caras que
estão aqui na tela.
Mas eu quero que você entenda
que são
coisas diferentes.
Tá? Conceito
da tecnologia.
Existem três diferenças muito marcantes
que a gente tem que entender.
A diferença
de um data mart para um
data house
para um data lake. Basicamente,
o data mart é um pedaço
da informação enviesada
departamentalmente
falando. Tá? E quando eu
falo enviesada, é exatamente
isso que eu quero que vocês
entendam.
Tá? Então eu pego um dado
que
ele é relacional,
eu dou um significado ao negócio,
ao departamento e eu entrego isso
para as pessoas que vão consumir.
No data house,
teoricamente, tá? A gente vai falar
teoricamente porque
a gente não conhece aí, trabalhando
mais de 14 anos na área,
eu não conheço
nenhum data house que funciona
100 % de
fim a fim e você tem
isso dentro
de uma organização gigante, isso funcionando
e você tendo uma fonte da
verdade.
Então, o que deveria
ser o conceito de data house
é um local aonde
todo mundo consegue traduzir
o que o dado é
no seu conceito
transacional,
ser convertido para uma visão
da empresa como um
todo e que todos os grãos
façam sentido em todos
os departamentos e que todo mundo
tem uma fonte da verdade.
E é aí um ponto importante.
A gente
durante muitos anos
falou que o data
house era uma fonte da verdade.
Só que a gente sempre falhou
miseravelmente nisso, porque o data house
nunca vai ser uma fonte da
verdade.
Ele vai ser uma fonte da
verdade
para um viés do que você
quer entregar.
Ele nunca vai ser uma fonte
da verdade
para o que realmente o negócio
te diz
na visão 360. Por quê?
Porque esse dado está limitado,
esse dado está enviesado de alguma
forma, porque você aplicou um processo
de ETL ou de ELT nele.
Diferentemente
se você pensar no data lake.
O data lake pode ser
a sua fonte da verdade.
E cara, o quanto
de empresas que falharam nisso
eu não consigo nem te dizer
de dedo, de mão.
Tinha que ser um polvo aqui,
uma hidra
com milhares de cabeças para poder
falar para vocês
o grande erro que aconteceu na
tecnologia
como um todo quando a gente
começou a achar
que o data house era um
ambiente onde a gente
conseguiria ter a verdade do dado.
Não, a gente não consegue fazer
isso
com ele. O data lake
é esse local. Só que infelizmente
eu vejo muitos consultores
falando o seguinte, cara não, esquece
data house, vamos para o data
lake.
Long live data lake.
E cara, não é assim.
A gente não substitui uma coisa
para outra.
Coisas são para serem juntadas.
Aquela coisa que a gente fala
é muito engraçada a gente falar.
Cara, você vai numa empresa grande
imagina que o seu objetivo, por
exemplo
é você trazer uma nova arquitetura
360 de uma
empresa gigantesca
que tem 120 departamentos.
Você vai chegar lá e falar,
gente, não
joga tudo fora aí e vamos
fazer isso aqui tudo?
Não, as coisas não funcionam assim.
Se você é realmente
inteligente, você vai tentar juntar,
você vai tentar aproveitar, você vai
tentar
fazer com que a adoção das
pessoas
não seja muito alta, com que
as pessoas não
sofram na ponta para fazer
essa transição ou que as transições
que elas
façam sejam para melhoria da vida
delas.
Então a gente sempre tem que
se juntar em vez
de destruir. Então não pense
dessa forma. Pense em como a
gente une
essas informações. E quando a gente
unificar
isso vai ficar muito mais fácil
na imagem
que eu vou mostrar aqui para
vocês agora.
Entretanto, a gente
traz alguns problemas
muito sérios que estão acontecendo hoje
nas grandes empresas.
Esses quatro pontos que eu vou
citar aqui são quatro pontos que
hoje
atualmente, em 2022, a gente sofre
bastante e talvez vocês
que já estão na área de
Analytics têm visto
bastante esse movimento de
empresas criarem ferramentas
de governança de dados, porque hoje
é o the next thing,
é o grande
momento dessas tecnologias, de
criarem ferramentas de como você vai
consolidar, de como você vai ver
a informação
todo, como você vai governar,
como você vai ter
a responsabilidade daquela informação
e todo o lineage daquele
dado fim a fim. Quando você
implementa
um Data Lake, o primeiro ponto
que você vai começar a notar
é que
você vai ter o que a
gente chama de Data Swamp.
O Swamp é um lago
de dados. Então, cara, pensa aí
numa
lagoa e pensa ali
que você está andando, aí está
dando
pé, de repente você afunda, tem
cinco
metros para baixo, depois tem uma
parada que te puxa
e assim por diante. Então,
o que que acontece? É muito
importante
que você entenda que o Data
Lake
traz eventualmente
esse problema, tá? E não é
algo que você consiga resolver.
Tá? Ele vai acontecer.
Sooner or later
isso vai acontecer na sua empresa.
Você vai
ter problema de Swamp de dados.
Luan, porque que eu vou ter
problema de Swamp
de dados mesmo que eu use
as melhores práticas?
Porque na verdade
não vai ser só você que
controla
o Data Lake, até porque se
a gente está falando de
democratizar e a gente
coloca um time responsável,
você teoricamente não está democratizando,
né? Assim, né? Então, assim,
por mais que você tinha um
time que
faça segurança, que faça toda
a captação do perímetro, enfim,
você está falando de um pedaço
do seu business,
você não está falando dele 360.
Então, isso
é um dos problemas da
descentralização, né? Ou seja,
você é descentralizado e você vai
centralizar. Por isso que isso vai
completamente o oposto do conceito
de Data Mesh, que a gente
não vai entrar
aqui em detalhe porque não tem
nada a ver
se a gente entrar. É só
para você ver
que são operações que são rumos
completamente distintos. Claro que
para falar de Data Lake, eu
vou ter que falar
de Lake House, de Delta Lake,
enfim, eu vou endereçar tudo isso
aqui no dia de hoje,
pode ficar tranquilo, tá?
Continue perguntando.
Mas o time faz a ingestão
em perímetro,
a parte de acesso seria democratização,
não? Não, na verdade não, Velke.
Existem empresas onde isso
está ligado e existem empresas que
isso está
completamente descentralizado.
Então, não existe
uma verdade unificada e não
existe nada escrito
aonde você vai olhar ali e
falar, cara,
existem melhores práticas, mas não existe
um pattern que todo mundo segue
porque isso é muito, o que?
É empresarial, isso
é muito de empresa para
empresa. Tem como a gente
reduzir o Swamp? Tem. Tem como
a gente
acabar com ele? É impossível, tá?
É um problema que você não
consegue resolver,
dada ao problema
que a gente está tentando resolver.
A mesma coisa de eu chegar
para você e falar o seguinte,
cara,
dentro de um data mart,
quais problemas eu resolvo?
Esse, esse, esse. Quais os problemas
que eu não resolvo? Esse, esse,
esse.
Então, existem problemas
que você resolve e coisas que
você traz com isso. Então, eventualmente
você vai ter Swamp, tá? Isso
vai
acontecer, de novo, cedo ou tarde.
Você pode fazer um trabalho muito
bom
de organização, de perímetro, enfim,
mas isso é muito difícil
estar no controle 100 %
completo de todas as vertentes da
empresa.
Principalmente quando você começa
a abrir isso para Story Party
Applications, quando você começa a abrir
isso
para silos diferentes, cada pessoa trata
e
vê a informação de um jeito,
escreve de um jeito,
estrutura pastas
de uma forma. Se você
chegar numa empresa grande
e você falar o seguinte, olha,
para que vocês possam acessar o
Data Lake
eu, vocês tem que
passar por um comitê, eu tenho
que passar por
o que acontece geralmente com empresas
grandes.
Você tem que seguir esse padrão?
Cara, isso
pode ajudar você, mas isso não
vai
impedir de que pessoas
utilizem de uma forma mais
rápida para conseguir certos tipos de
cadência de acesso.
Então, eu digo isso porque essa
é a realidade
da vida, entendeu? Então, assim,
por mais que a gente mostre
melhores
práticas, enfim, as grandes
empresas, realmente, pegando as grandes
empresas, é muito impossível
você ver, cara, um Data Lake
maravilhoso,
que você não vai ter pastinha
01,
você não vai ter pastinha 02,
você não vai
ver dado de um formato X,
de um formato Y,
você não vai ver um sistema
de ingestão
jogando numa pasta completamente diferente do
time
de aquisição que faz, principalmente quando
você quebra
em times e aí cada time
não
consegue ver o que os outros
vão fazer
e isso fica muito pior. Por
exemplo,
vou dar um exemplo aqui do
que eu estou
passando, que eu já passei nas
últimas três consultorias.
Você tinha um time de ingestão
de dados, você tinha
um time de processamento de dados.
O time de
ingestão de dados eram separados em
duas squads,
o time 1, ele usava uma
ferramenta
X, que era o NiFi, e
o outro time utilizava
a ferramenta Y, que era
se eu não me engano, o
5Tram.
E aí, em alguns momentos,
dependendo do tipo de
dado que entrava, ele
salvava um dado ou em JSON
ou em
CSV. Só que a tecnologia
que processava esses
dados era o Spark, e a
gente nunca processava
esses dados em JSON ou CSV.
Então, na verdade, o que acontecia
é que a gente precisava reconciliar
essas informações, colocar num outro local
para ser processado. Só que depois
as pessoas começaram a adicionar
novas tecnologias em cima disso.
DBT, Flink
e assim por diante. E isso
começou a virar
um samba do crioulo louco. Ou
seja,
de algo organizado virou algo completamente
Swamp. Então, eu poderia passar aqui
também horas para vocês, mas essa
é
literalmente o que vocês precisam
entender da experiência de
campo, que é importante vocês
entenderem. A gente pode tentar
reduzir, a gente pode tentar melhorar,
a gente pode tentar vir com
melhores práticas,
mas o Swamp é algo que
o data
lake vai trazer pelas características
do data lake em entrega, beleza?
Bruno colocou
aqui, pois é, concordo, por isso
que a
ideia de ter um catálogo de
dados corporativos
sem duplicação de dados é uma
utopia. Eu gostei da palavra, porque
eu usei hoje também, na verdade,
uma das
das managers
do escritório,
do headquarters de Nova York, da
empresa
que a gente está fazendo consultoria,
e falou exatamente
cara, isso é uma utopia total.
A gente tem um
ambiente centralizado, onde todo mundo
pega, você tem uma fonte da
verdade,
aquele dado reverbera para 35
sistemas diferentes, principalmente
quando alguém colocou aqui, o Cheferson
colocou
questões de DPR, por exemplo.
Cara, isso foi um dos problemas
mais
difíceis de uma empresa hoje, cara.
Por quê? Porque a verdade
nua e crua é, você não
tem uma fonte
da verdade na empresa, essa é
a verdade,
é que você não tem uma
fonte da verdade,
e quando você precisa literalmente marcar
o registro para ser excluído ou
para ser removido
em si, geralmente
aquela mudança
na estrutura atual das grandes empresas
é que aquele dado, ele passa
por uma cascata waterfall, ele passa
por um processo, ele não é
distribuído.
Então você não tem, por exemplo,
um
sistema que recebe, ele reverbera
para todos os consumidores daquilo, né?
Isso seria o santo grau, porque
você tem
um sistema completamente quadrado
e waterfall, que segue
vários processos, e normalmente o que
acontece
é que aquele dado se perde.
Então
o GDPR tem causado muita dor
de
cabeça para as empresas, principalmente
que estão no legal, que trabalham
com o dado e com segurança,
DevSecOps
e assim por diante, de como
eles estruturam
toda a aquisição de dados e
trabalham
essa informação.
Nossa, eu poderia passar mais horas
aqui, hein?
Vamos andar aqui, senão eu acabo
três horas
da manhã. Governança de dados é
uma das
coisas que tem acontecido bastante, vocês
vão ver
bastante a gente falar disso aqui
também.
Qualidade de dado, logicamente, com várias
quantidades entrando na qualidade de dados,
vai ser reduzido, então é muito
importante
que você tenha isso em mente.
De novo,
é muito importante que você tenha
esses flags
e esses warnings dentro da sua
mente,
mas já bota na tua cabeça
que tu não vai resolver,
beleza? Então assim, não acha
que, porra, eu vou botar um
sistema de
governança na minha empresa e vou
resolver todos os problemas
da minha empresa. Começa por
pedaços, começa que nem um Jack,
é melhor,
tá? Porque, cara, se você vai
nesse
sistema utópico aí do Bruno, realmente
você não consegue fazer nada, porque
é muito grande
quando a gente começa a pensar
na perspectiva.
E não só grande pela complexidade,
mas
grande porque você tá
falando de departamentos, de pessoas,
de permissões, de níveis de
autoridade diferentes. Então,
dificilmente você vai chegar na sua
empresa e falar
oi, eu tenho um sistema aqui
de governança,
eu preciso acessar todas as bases,
tudo isso, puxar, não beleza, tá
certo,
toma aqui o acesso. Não, vai
chegar um DBA ali
que você vai passar seis meses
pra conseguir acesso.
Então, isso é realidade, né?
Então é muito importante que você
tente começar
de baixo pra cima nesses approaches,
porque isso traz uma visão
com que as pessoas possam aderir.
Se você quer
vender um produto dentro da sua
empresa
e considere você vendendo um produto,
faça com que esse
produto seja bom para o consumidor,
não para você, né? Quais são
as
vantagens que o meu consumidor, ou
seja, como
que o departamento A e tal,
vai ganhar por
ter esse novo produto? Então, assim
que a adoção das empresas, e
culturalmente
falando ela muda, porque na verdade
você
está trazendo algo que vai facilitar
e não só trazer, mas mostrar
ao longo
do tempo quais são os benefícios.
Eu tô trazendo toda essa visão
tópica
porque é isso que você precisa
como
desenvolvedor. Às vezes você se pergunta
assim, porra, mas
caralho, eu vim pro treinamento
de Spark, o cara tá divagando
aqui.
Não, é porque na vida real
você vai passar
talvez
70 % do seu tempo pensando
e
30 % executando. Ou, cara,
no pior do cenário, se você
estiver fazendo certo, você vai passar
50 %
a 50%. Se você estiver só
codando, cara, hands down,
louco aqui, olhando, caralho,
com o cara, é, heads
above the water, sempre lotado
de trabalho ali, respirando e tal,
provavelmente,
tá, é você não
você poderia estar fazendo melhor.
Porque, na verdade, a engenharia de
dados
ela desabilita, ela faz
um unlocking pra que você possa
fazer com que o negócio
possa escalar, literalmente.
O código, as visões, enfim,
então é sempre bom você
desenvolver pensando no negócio.
Desenvolver na melhor
forma de chegar no negócio. Então,
por exemplo, se eu tiver que
gastar Luan,
se você perguntar pra mim, cara,
eu conheço
muito bem Spark, me sinto muito
confortável,
vou trabalhar com Spark. Mas, por
exemplo,
tem vezes no meu dia a
dia
que eu olho e falo, cara,
eu posso resolver isso
fazendo aqui uma função
simples, que não vai escalar muito
em Python, eu posso parar aqui
e criar
cara, uma parada em
escala, não sei o que, foda
-se, vou fazer em Python.
Talvez, vale a pena.
Ou vai ter casos que eu
vou falar, não, realmente
vou ter que parar, eu vou
ter que criar alguma coisa.
E esse poder
crítico é o que eu tô
tentando trazer pra vocês.
Esse poder analítico de você conseguir
escrever código e ver o que
vai trazer valor e o que
não vai trazer
valor. Por exemplo, eu ia falar
isso na sexta,
mas isso aqui traz o seguinte
link.
Tem vezes que, cara, seu código
tá lento. Lento assim,
vamos supor, seu código executado em
cinco minutos, mais dado
quinze minutos, mais dado
meia hora, mais dado, tá executando
em duas
horas, o SLA tá batendo,
e aí você precisa pegar aquele
código que tem, sei lá,
trezentas linhas de código e debugar
aquele código,
enfim, você pode fazer isso.
Ou dependendo
do seu problema, dependendo
do que você tá fazendo,
você pode simplesmente adicionar um nó
extra
e resolver o seu problema.
E quem vai te dizer o
que
é certo fazer?
Às vezes você tem, literalmente,
como tech lead ou como engenheiro
que tá cuidando
disso, você tem
a responsabilidade de resolver o problema.
Então, o que que vale mais
a pena
falar?
Se você pensar rapidamente, você vai
falar
caraca, Luan, você tá me dando
um péssimo
conselho, né? Então, o certo seria
o quê?
Parar, debugar a aplicação
toda, porra, ver o long hanging
fruits, né, quais são os problemas
menores,
resolver aqueles problemas ali e tal.
Legal, mas deixa
eu te perguntar, será que isso
realmente traz
valor pro negócio no fim do
mundo, no final
do dia? Será que você não
poderia
simplesmente resolver naquele
momento aquele problema adicionando um novo
nó,
trazendo vinte dólares a mais no
seu
dia a dia, mas entretanto usando
aquele
tempo que você tava fazendo pra
resolver um problema muito mais crítico
que
traz um retorno de investimento muito
mais rápido?
Então,
o que que é mais
importante, né? O Bruno acabou falando,
eu já estou cobrando, vai entender
o seu debug?
Então, é isso que eu quero
que vocês
entendam, tá? Eu quero que vocês
tenham esse poder analítico no treinamento
quando sair daqui, de saber o
seguinte, cara, não é só
sobre escrever código. É sobre escrever
código,
mas não é só sobre escrever
código, entende?
É sobre escrever as coisas
pensando num bem maior,
olhando pro negócio e
sabendo ser estrategicamente
inteligente, beleza? É isso
que vai fazer você sair de
uma vaga do Brasil pra trabalhar
em qualquer
empresa do mundo. É esse
mindset, tá? Não é um mindset
de
cara, já trabalhei com um gênio,
já trabalhei com gente que tem
QI, cara, duas
vezes maior. Não é quem
é o mais inteligente. Não é.
Se vocês ainda não entenderam isso,
que pena. Não é sobre quem
é o mais
inteligente, não é sobre quem é
o mais rápido,
não é como quem é o
mais forte.
É sobre quem é mais quem?
É sobre quem é o mais
adaptável
para o problema. Sempre foi isso.
Darwin
sempre disse isso, cara. Essa é
a maior verdade
que existe. Você tem que ser
analítico,
você tem que ser inteligente.
Inteligente não usar força? Às vezes
é, às vezes
não. Então, é isso que eu
quero que vocês entendam.
Pra vocês alcançarem
um lugar de prestígio,
uma diferença, pra que as
pessoas consigam te olhar de frente,
é muito bom você ser foda
no código,
você ser, porra, super tech savvy,
ser aquele cara que escreve. É
bonito, você vai
chegar lá. Mas é um processo
muito
mais vinculado a você trabalhar o
seu
yin yang, tá? Do que na
verdade
exatamente, ser um canivete suíço
com várias opções do que
trabalhar somente o tech.
Então, soft skills, cara, ele é
muito
mais importante do que o tech
skills,
mas é longe, tá? E eu
digo isso
não sou eu não. Cara, a
gente tem várias entrevistas
aí com o iFood,
time da XP, enfim, toda vez
que a gente pergunta a mesma
coisa, cara,
Luiza Labs, enfim, a gente tem
o mesmo retorno.
Cara, soft skills, soft skills.
Eu preciso de soft skills pra
habilitar
o que eu vou fazer no
meu tech skills, né?
Eu fui, eu ia fazer isso
na sexta -feira,
mas às vezes uma coisa vai
puxando a outra. Eu espero que
vocês
entendam a relação que isso tem
com data lake,
que é o quê? No final
do dia, é entender o que
é
tecnologia e o que é conceito
e saber abstrair isso
de uma forma inteligente pro seu
negócio.
Entenda que você
vai ser um engenheiro de dados
e você
vai se comunicar com todo mundo.
Então,
você vai ter que ter vários
bonés.
Você vai ter que, cara, quando
você conversar
com o seu gerente, você veste
um boné ali
de traduzir o problema do negócio
tecnicamente
pra um negócio, de uma forma
com que ele
entenda o seu linguajar. Quando você
vai conversar com um
arquiteto de dados, você traz os
seus skills
que são mais direcionados a ver
a figura
como um todo, quais são os
problemas que você vai resolver
habilitando aquilo. Quando você vai conversar
com um engenheiro de dados do
seu lado,
você vai vestir o boné técnico,
aí
você vai arregaçar, vai falar da
função,
vai falar que ali você vai
fazer um ODF, que você vai
compilar o código assim, que você
vai usar tal
coisa que é muito foda e
tal. Quando você vai falar
com seu CEO, você vai se
remodelar e vai falar de uma
outra forma do que
vocês estão fazendo e o que
realmente você vai
trazer de negócio pro...
o que vai trazer de valor
pro negócio.
Por que que eu tô falando
isso pra vocês?
Porque é isso que vai determinar
o quão excelentes vocês são
no trabalho. É exatamente a tradução
disso tudo. É você conseguir
fazer isso. Cara, eu conheço várias
pessoas que eu adoro
de paixão, mas cara, o cara
não consegue se
comunicar com vários níveis da empresa.
Então fica difícil essa pessoa evoluir.
Porque a gente precisa de comunicadores.
Beleza?
Então, para pra gente pensar
o seguinte. Legal, a gente tinha
um conceito
de data mart, de data lake
muito
engessado na nossa vida.
Que tal se a gente começasse
a pensar da seguinte
forma? Vamos continuar utilizando data
mart, vamos continuar utilizando
data lake, mas vamos fazer assim?
Que tal?
Que tal agora a gente
pivotear um pouco a nossa mente
e
pensar dessa seguinte forma?
E se por exemplo eu tenho
aplicações ali mobile
ou aplicações que estão consultando e
periodicamente elas estão jogando o dado
no data lake em vez do
banco de dados?
Cara, vamos supor que um banco
de dados mongo ali está mandando
dados
ou a aplicação que se comunica
com o mongo está
mandando dados e de tempos em
tempos você tem um sistema de
ingestão que traz os dados e
joga no data lake.
Se você tem o seu SQL
Server que tem
um agent job ou que tem
alguma coisa configurada
um SSS ou qualquer coisa que
joga
o dado no data lake. E
cara,
tem informações que vem de parceiros
ali, de
terceiros que caem na FTP de
24, 24
horas, de 12 em 12 horas
e eles
caem no data lake. Olha só
que interessante.
Ao invés de você ter
colocado
essas informações
dentro de um perímetro, SQL,
mongo, FTP
no mobile
com aquela sua aplicação, com aquele
eu estou falando da perspectiva da
engenharia de dados.
Que tal abrir essa
informação? E na hora que aquele
dado entrar na sua fonte de
dados crua
porque o dado que estava na
aplicação
caiu, você pode
delegar o que você quer fazer
depois.
E aí você tem a sua
fonte
da verdade. Por quê? Porque agora
os dados estão caindo num ambiente
aonde eu tenho
o entry first, processing later.
Ele entra em um local
e ele é processado
após isso. Então daqui
da sua fonte de dados, você
pode
fazer processos pra enriquecer
o dado, filtrar pra dar
uma visualização específica pro cliente,
criar um modelo de machine learning,
criar o seu data house baseado
nas fontes que vem de diversos
locais,
fazer a visualização
do dado. Então, pensa
na verdade que o Data Lake
vai ser
o seu ponto central
pra que tudo possa ser
sumarizado e colocado pra que depois
as visões possam acontecer. Isso
quer dizer que todos os 50
sistemas
que se plugarem no Data Lake
sempre vai ter uma fonte de
destino
e uma fonte de início. Então
você
sempre vai conseguir fazer o traceback
ou ETL reverso, ou você vai
conseguir voltar e você
vai conseguir saber qual é o
significado
daquele registro. Então
pensando em termos como
GDPR, por exemplo, uma das propostas
é os sistemas que estão trabalhando
ali independentemente, beleza? Pensa
no seguinte, cara, tem um desenvolvedor,
desenvolvedor tá desenvolvendo, beleza,
maravilhosamente,
o que ele vai implementar na
camada de software
dele é o seguinte, cara, de
tempos
em tempos, ou no final do
dia, ou de batches
em batches, eu vou mandar
um batch pra dentro do Data
Lake
falando o que aconteceu naquele
dia. E na hora que isso
acontecer
isso vai reverberar o que?
Por exemplo, um GDPR. Então se
chegou lá na minha aplicação e
eu marquei
esse dado, entrou dentro do Lake
ele foi marcado, eu sei quais
são os
sistemas que estão plugados a isso
e ele
reverbera essas mudanças pra ali.
Isso é uma das formas que
você poderia trabalhar.
Na prática, todos os dados de
todos os
bancos vão ser mandados para o
Data Lake? Perfeito,
Douglas, exatamente.
Você tá certo, é isso mesmo,
na prática é
exatamente isso. Tudo que vai chegando
no SQL Server ali, de tempos
em tempos, vai
caindo no Data Lake, vai caindo
no Data Lake,
vai caindo no Data Lake. Isso
traz
tanto use case foda, quer ver
um use case
foda pra programa de dados relacional?
A gente já economizou milhões em
um cliente
assim, de dólares, que é o
seguinte,
o cara, ele era
regional, ele foi pra uma proposta
global
e ele saiu de 5 SQL
Servers pra 45 SQL Servers.
O que que você tem que
fazer quando você compra
um SQL Server? Você tem que
licenciar esse cara.
E aí ele começou a pensar
no processo, caraca, eu vou sair
de
15 pra 45?
Putz, meu billing vai
quadriplicar. Cara,
mas eu preciso acessar esses dados
em vários
lugares?
Cara, por que que você não
usa o Data Lake?
Vai ser 100 vezes mais barato.
Então você
ainda pode ter um local central,
dispersar essas informações e não
ter que garantir que todo seu
usuário
de Dev ou qualquer time que
queira experimentar
o dado, visualizar essa informação,
esteja plugado no seu sistema online.
Então isso é uma das coisas
que o Data Lake
traz que é um tesão. Que
é o que?
Cara, você desacopla
e você deixa os seus sistemas
online
trabalhando com os sistemas de backbones
que ele tem. Entretanto,
seu time de analytics, seu time
de
engenharia, seu time de machine learning,
seu time
de exploração, seu time que faz
processamento
de dados, ele não precisa acessar
essa fonte. Ele só
precisa ver o Data Lake. Porque
o dado
é cru. Ele é exatamente o
que aconteceu no negócio
em determinado tempo.
Interessante, né? Quando tu dizes
fazer o Data Visualization usando o
Data Lake e já
estamos a falar de Gold Layer,
não necessariamente. A gente pode falar
de Gold Layer, mas não é
um requisito.
Por isso
que alguém citou ali em cima
do Canivete
Suíça. É muito importante você entender
o processo como um todo. Porque
a gente
fala da Gold Layer, e eu
falo
bastante da Gold Layer, o quanto
é importante.
Quando eu chegar no Gold Layer,
eu vou discutir
muito sobre o Gold Layer.
Mas, às vezes, você não precisa
de um Gold Layer. Às vezes,
dependendo do que
você está criando, especificamente,
você vai ter que andar naquela
balança
de, cara, eu gasto
tempo padronizando ou faço
uma exploração de dados, dou valor
e
eventualmente eu crio um processo para
padronizar
isso. Então, depende.
Se não dependesse, não existiria
o que a gente chama de
Data Exploration,
que é bastante utilizado hoje
fora do país. Hoje, cada vez
mais
você vê as empresas, vocês veem
as empresas
utilizando ferramentas de exploração de dados.
Porque, cara, o cara tem um
puta Data Lake
gigante, ele não quer,
velho, fazer um processo GTL gigantesco
para ver um dado. Beleza,
ele quer padronizar isso, mas primeiro
ele quer experimentar. Ele quer ver
o que ele tem dentro do
Data Lake, ele quer cruzar dados,
ele quer ver quais são as
capacidades
que o Data Lake traz para
o departamento,
enfim. E aí você entrega ferramentas
que possam desabilitar isso para ele
rapidamente. Por exemplo, o Dremio.
Você entrega o Dremio para o
cara, ele vê
o Data Lake como um todo,
faz query
em cima de tudo, cria reflections
e tem uma visualização e fala,
porra, que foda.
Isso é interessante. Passa para o
time de
engenharia de dados que vai criar
um pipeline
para que atenda esse problema em
massive scale. Então, isso é uma
das formas
de trabalhar também. Por isso que
a exploração de dados
é muito importante você ter quando
você está
trabalhando com Data Lake. E ele
fica um pouquinho
lá atrás, tá? Isso é uma
super boa regra
de utilização do Data Lake.
O Thiago está falando que lá
na empresa dele é assim.
Isso é muito legal. O Dremio
é foda
também, né?
Mas essa questão de todos os
dados, na minha opinião,
eu acho que acelera a transformação
em
Data Swamp. Talvez não faria
mais sentido trazer dados que realmente
serão
utilizados ou fazem algum sentido.
O problema, Bruno, de você falar
que faz
sentido é porque depende
da lente que você está usando.
O que que é fazer sentido?
Entendeu? Então, por exemplo, talvez
faça sentido para você
trazer tabela 2, 3 e 4
do SQL Server, mas
para uma outra vertente do seu
business
vale a pena trazer o banco
de dados inteiro.
Por quê? Porque,
na verdade, essa pessoa tem uma
visão completamente
diferente do seu...
do que você está vendo naquele
momento.
Então, é muito difícil você determinar
o que faz sentido, tá?
O que faz sentido normalmente
é tudo. Infelizmente.
Ah, tudo. Quantas vezes
você entra numa empresa e o
cara fala, cara, eu quero tudo.
Então, as pessoas sempre querem tudo
e elas utilizam 10 % daquilo.
Mas se você, na sua empresa,
consegue saber qual é o valor
do sentido e consegue entender
qual é, vale a pena você
trazer
somente o que faz sentido.
Entretanto, o problema de você trazer,
por exemplo, coisas que fazem
sentido para você,
é porque, posteriormente,
se você tiver que aumentar
esse processo,
pode acontecer nas suas
mãos ou pode acontecer que vai
acontecer como muitas empresas grandes fazem.
Você tem 30 ETLs que puxam
da mesma
tabela. E aí, você vai querer
o quê?
Você vai querer um ETL unificado
que traz tudo e você consiga
criar
um processo cultural de um sistema
de
ingestão unitário? Ou você quer
simplesmente trazer o que é importante
para
você no seu viés, criar o
que
você precisa criar, mas, eventualmente,
quando um outro departamento, uma outra
visão da sua empresa precisar ser
saudável, ele vai ter
que criar um outro processo de
ETL e trazer
o dado. Então, depende, entende?
Vai depender da sua empresa. Não
existe
certo e errado aqui. É isso
que eu quero que vocês entendam.
Entenda o problema.
Esse é o problema do Bruno.
É o que faz
sentido para ele. Está certo e
está errado?
Não. Não está certo
nem está errado. Depende
do processo. Por isso que é
importante
você documentar muito bem
as razões do porquê.
Porque, geralmente, pessoas de TI têm
vários problemas mentais sérios.
Nós temos. Eu estou me incluindo.
A gente tem essa mania de
olhar para as
coisas que os outros fazem e
falar,
porra, tá uma merda. Porra,
tá errado. Nossa, mas por que
o cara fez assim?
Então, existe um motivo.
Às vezes, não o
motivo certo, mas existe um motivo.
Então,
é muito bom você, como profissional,
sentar, criar um documento
de requisito, criar um processo que
fala o
seguinte, cara, qual é o approach
que a gente vai
fazer? É essa arquitetura aqui.
Cara, documenta na ingestão, é assim,
assim, assim. Processamento é assim,
assim. Estamos fazendo isso por causa
disso, disso,
disso. Eu te garanto que, quando
você
fizer isso, a entrega do seu
trabalho
ele muda um bilhão de porcentos.
Então, as pessoas vão começar a
ver
o valor que você traz para
a empresa
com esses detalhes que são pequenos,
que, na verdade, são a cereja
do bolo.
É você identificar o seguinte, cara,
por que
eu uso o Data Lake? Por
que
eu optei por esse processo? Por
que a gente
faz a carga incremental ou full?
Ah, eu optei pela full porque
durante
muito tempo do meu business
eu trabalhei com tanta cadência de
dados,
então não vale a pena eu
ter que
aplicar um processo incremental
dentro do Integration Service, porque o
que eu vou ter que
fazer? Ah, isso, isso, isso, isso
aqui.
Hoje é muito mais fácil eu
truncar tudo e carregar tudo.
Está errado? Não, não está errado.
Depende de como você mostra o
problema. Depende da tecnologia que você
está utilizando. Depende de um milhão
de coisas. Então, se você tiver
um documento
que mostra isso e que
esclarece o que você fez na
sua
arquitetura, no que você está trabalhando,
não tenta resolver o problema do
mundo. Resolve
o problema do seu departamento primeiro,
da onde você trabalha, do seu
time e
depois isso vai exponencialmente crescendo,
cara. E daqui a pouco você
está escrevendo o
processo e visualizando a empresa como
360. E isso aconteceu com várias
pessoas que trabalham com mídia, isso
acontece comigo.
É um processo. Você vai trazendo
qualidade
de pouco em pouco e as
pessoas vão ficando
surpresas com a qualidade que aquilo
leva
a outras coisas. Coisas boas vão
puxando coisas boas e assim por
diante. Então,
isso são dicas,
trabalhando nove anos fora, que foram
dicas determinísticas
para que eu tivesse sucesso na
minha vida. Eu vou falar uma
sexta -feira,
por exemplo, como que eu
consegui cinco aumentos em um ano
não sendo técnico. E eu
passei cinco anos focando no técnico
e ganhei
um aumento de 3%. E depois
quando eu foquei no meu soft
skills, eu ganhei
cinco aumentos no tempo de um
ano e meio. E aí você
começa a entender
caralho, não é só sobre ser
um técnico
foda, nunca foi. E realmente
não é. Eu passei por isso,
vários amigos
passaram por isso e a gente
vê
cada vez mais isso sendo uma
realidade.
Então, pense fora da caixa.
Seja chato no que você
faça, seja perfeccionista naquilo ali que
você está fazendo e que isso
vai fazer
diferença para você, principalmente quando
você tem o embasamento. Então,
isso aqui é embasamento que você
está ganhando.
Isso aqui, cara, é munição para
você
sentar com o cara e falar,
vamos fazer um data lake?
Vamos. Cara, como você está
pretendendo fazer? Assim, assim, não, mas
olha só,
você pode fazer assim, você pode
usar isso aqui, você pode
tal, tal, tal. Pegue esses PPTs,
escreve
do seu jeito, fale com
você, melhore o seu pitch,
se venda, aprenda como utilizar
isso aqui para melhorar
o seu processo com o seu
time.
Beleza?
Mas, tudo pode ser
traduzido a um padrão. A gente
sempre faz, o americano faz muito
isso, né?
O americano é o rei de
padronização
das coisas. Não sei se você
já viu isso, né?
Um dos padrões americanos
é tipo, tudo você tem que
botar numa caixinha, tudo você tem
que
botar dentro de uma lista, tudo
que você tem que botar
dentro de alguma coisa. O americano
trabalha
assim. E realmente funciona
para 95 % dos casos.
Então, a gente vai atacar os
95 %
dos casos aqui, porque isso
resolve o nosso problema na maioria
das vezes.
Beleza? Então, o que que a
gente
quer fazer? A gente quer garantir
que o dado
ele vai chegar num data lake,
ele vai ser processado.
E a gente tem várias formas
de depenar
o gato aqui, né? Então, a
gente vai ver
todas elas. Todas elas
referentes às nuvens e propostas
que eu tô trazendo pra vocês,
pra vocês
entenderem que o que importa
é a caixa que você tá.
Não importa
a tecnologia nesse momento.
Olha só. Imagina que você
tem
batch, né? Então,
a arquitetura lambda, que a gente
viu lá
na segunda -feira, ontem,
a lambda, quando você desenha lambda,
a primeira coisa que você faz
quando você desenha uma arquitetura
lambda é você olha pra fonte
de dados e você fala o
seguinte,
fonte de dados, tu és
batch ou tu és streaming?
É assim que você desenha lambda.
Então, você vai botar todo mundo
no bloquinho
que é batch. Ah, então, cara,
eu tenho que
puxar do SQL Server ali de
duas horas,
eu tenho que puxar do MySQL
a cada três horas,
eu tenho um processo no Postgres
também
que tem umas aplicações que escrevem
ali que eu tenho que
puxar a cada cinco horas. E,
cara,
tem umas paradas no MongoDB agora
que tem uns
desenvolvedores novos utilizando ali pra gravar
a usuário, informações de estado de
usuário
que eu preciso cruzar também, mas
é batch.
Eu não preciso trazer isso em
tempo real. Show.
Então, primeira coisa que eu
vou fazer, eu vou ter que
ter um sistema
de ingestão e de orquestração, ou
de
ingestão. Eu preciso ter um sistema
que
se conecte a essas fontes de
dados e tragam elas. Existe
um trilhão de ferramentas no mercado.
No Azure, por exemplo, o cara
que você vai
usar pra isso é o Azure
Data Factory.
Ele é o cara desenvolvido
pra que você possa
trazer dados de fontes externas
para dentro do Azure, pra dentro
do
ecossistema.
Segundo ponto. Poxa,
beleza. Vocês estão
me escutando? Acho que travou de
novo.
Não, tá normal, Luan.
Tô te escutando. Tá bravo.
Travou sua tela e seu
vídeo agora.
Deu aquela travada de novo.
Eu vou
me dar três minutinhos, pessoal,
que eu acho que vou ter
que reiniciar o Mac.
Eu não sei, acho que tem
alguma coisa acontecendo.
Tá no mesmo horário.
É, então, eu acho que tem
alguma
relação com o tempo do Zoom,
com alguma aplicação que tá ligada,
enfim, eu já volto aí.
Pode falar.
Olha o meu tito.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
As perguntas eu já respondi, tá?
Porque eles estavam te aguardando.
Todas respondidas.
Tá me escutando bem, Matheus?
Tá me escutando bem?
Beleza.
Então, vamos lá, gente.
Aconteceu alguma coisa aí que eu
não sabia?
Não.
O pessoal só pediu alguns esclarecimentos
aqui e eu já respondi todos.
Show.
Então, vamos lá.
Vocês estão vendo a minha tela
aí?
Sim.
Então, por que tudo isso faz
sentido para você que vai desenvolver
em Spark?
Porque você vai desenvolver em qualquer
nuvem.
Esse treinamento é para que você
possa entender e desenvolver em qualquer
nuvem.
E entender que o Spark...
E principalmente entender aonde o Matheus
até foi o som ontem.
Aonde o Spark é bom e
aonde ele não é bom.
Então, você vai entender aonde é
que ele realmente é foda.
E, claro, eventualmente você vai sair
do treinamento com essa visão ampla
do que você precisa estudar, do
que você precisa saber também de
extra
para complementar o Spark.
Não dá para você fazer tudo
com o Spark.
Então, vamos lá.
Primeiro, eu olhei para o dado
de batch.
Agora.
Agora, eu vou olhar para o
dado de streaming.
Então, eu vou olhar ali, por
exemplo.
Vou pegar um exemplo de uma
aplicação front -end
que tem um back -end de
uma aplicação em Python.
Que vai também mandar esse dado
para algum local.
Então, esse dado vai ter que
ser ingerido num sistema...
Não vai ter, mas geralmente é
ingerido num sistema de tempo real.
A gente vai ver amanhã sistemas
de tempo real.
De armazenamento de tempo real.
Por exemplo, o Kafka é o
grande pai disso.
Você tem Kafka, Pulsar, Azure Event
Hubs,
Sky Inicius na AWS.
PubSub no Google.
Que significa a mesma coisa e
que vai estar aqui na mesma
caixa.
Ou seja, se eu estou recebendo
dado em streaming.
Dados ali que estou recebendo da
minha aplicação ali.
E eu quero enviar esse dado
em tempo real.
Vou precisar de um sistema de
armazenamento em tempo real.
E amanhã a gente vai ver
em detalhes sobre isso.
Esses dados podem eventualmente colidir aonde?
No Data Lake.
Olha só o que a gente
estava falando.
De ter uma arquitetura que de
forma singular se complementa.
Então, você tem diversos sistemas.
Esses sistemas são ingeridos.
Podem ser por diversas ferramentas.
E elas caem dentro do Data
Lake.
Olhando essa perspectiva até agora.
Isso faz sentido para o que
a gente estava falando?
Me coloca aqui no chat.
Quero saber o que vocês...
Qual é o take de vocês
em relação a isso?
Para a gente conseguir trazer o
que?
Reduzir um pouco dos silos.
Reduzir um pouco de você ter
essas informações em locais que não
estão acessados.
Então, a gente conseguir distribuir isso.
E botar de um lado.
Onde esse cara vai ser democratizado.
E aí, amigo, a gente pode
fazer o que?
Na camada de processamento.
Ou seja, no Step 6 aqui.
Utilizar o Spark para processar esses
dados.
Então, olha só como é importante.
Então, a gente não estava falando
de Spark desde o início.
Olha onde o Spark entra muito
bem.
Ele entra aqui.
Ele entra para processar esses dados
que chegaram dentro do Data Lake.
E que, eventualmente, eles vão ser
colocados dentro de um engine para
análise de dados.
Ou seja, o Spark é o
caso.
Então, ele é um cara que
está entre o Data Lake.
Está entre a fonte de dados,
a entrada de dados.
E está entre a saída do
dado.
Então, ele é um sistema que
a gente chama de In and
Out.
Ele é um cara que recebe,
processa e joga.
Recebe, processa e joga.
E é aqui na parte 6
onde toda a regra de negócio
pode estar.
Vocês tinham essa visão aqui exatamente
do que o Spark é e
de como ele se encaixa no
ecossistema como um todo?
Me coloca aqui.
Eu quero saber.
A gente tem 37 pessoas no
salão.
Opa, muito bom.
Batch e streaming?
Exatamente, Vinicius.
A gente vai ver que o
Spark faz os dois.
Por isso que isso é muito
lindo.
Por isso que o Spark brilha
muito os olhos.
Porque você vai conseguir tanto fazer
streaming como batch.
Aqui a gente está vendo que
a gente está colocando o Spark
num sentido mais batch -ish.
Por quê?
Porque ele está dentro do Data
Lake.
Eu consigo plugar o Spark direto
num sistema de real -time store?
Consigo.
Eu consigo, por exemplo, fazer processamento
no Data Lake em tempo real?
Consigo.
É a melhor forma?
Não.
Então a gente vai ver isso
também.
Fica tranquilo.
Ele faz o T, exatamente.
Então ele faz a transformação.
Só que a gente olhou isso
no Azure, beleza?
Olha só.
Estou olhando no Azure.
Vamos para a AWS.
O que mudou aqui?
Mudou as tecnologias.
Olha que coisa legal.
Então quando você começa a pensar
no modelo 360 Fit View,
de olhar isso tudo como uma
grande bola de solução,
você começa a ver que não
é sobre as tecnologias teoricamente,
mas é como você vai extrair
isso.
Então aqui na AWS, mesma arquitetura,
mesmo processo, tecnologias diferentes.
Por exemplo, aqui eu estou utilizando
o Glue.
Aqui eu estou utilizando o Kinesis.
Aqui eu estou utilizando o S3.
Aqui eu estou utilizando o Glue
de novo,
porque ele pode fazer além de
agendamento, processamento também.
E olha aqui, estou utilizando o
Redshift.
Mas é a mesma coisa, é
o mesmo processo.
Sistema de ingestão.
O Glue é o sistema de
ingestão da AWS.
Sistema de ingestão de armazenamento em
storage, em tempo real.
Kinesis.
Data Lake.
No Azure, é o Glob Storage.
No AWS, é o S3.
Processamento.
O Glue, que roda Spark por
debaixo.
E o MDW, que no AWS
é o Redshift.
Após, durante a fase de processamento,
geralmente você usa o Data Lake.
Vamos chegar lá.
Calma aí que a gente vai
chegar lá.
Outro.
GCP.
Mesma coisa.
Cloud Data Fusion.
A UI aqui, parecida com o
Data Factory.
Um pouquinho parecido, remete um pouquinho,
não muito ao Glue,
mas mais ao Data Factory.
Você traz o dado para dentro
do Data Lake deles, que é
o Google Cloud Storage.
Aqui embaixo, a aplicação se conecta
ao PubSub.
Esses dados chegam dentro do Google
Cloud Storage.
A gente utiliza Spark aqui, que
é o Dataproc gerenciado,
que tem Spark aqui dentro.
E entrega essas informações no Data
Warehouse.
No Modern Data Warehouse.
Ah, e eu quero agora uma
proposta open source.
Tudo bem.
A gente pode ter o Apache
NiFi para poder trazer os dados
em batch.
Então, eu poderia ter ele para
Near Real Time.
Ele é muito bom também para
isso.
Eu posso ter o Kafka, que
é totalmente desenhado para isso.
A gente vai ver na quarta
-feira, amanhã.
Esse dado cai dentro do HDFS,
do Hadoop Distributed File System.
A gente usa Spark e a
gente usa The Data.
O que está em comum entre
todas essas arquiteturas?
Spark.
Pegou a ideia?
Então, em todas essas arquiteturas, você
tem Spark no Azure.
Você tem Spark na AWS.
Você tem Spark na Azure.
Na Google, você tem Spark no
open source.
Pegou a ideia?
Me diz aí.
Tu vê algo contra usar o
BigQuery como Process Engine?
O BigQuery, assim, depende.
Eu não vejo nada contra.
Eu acho que, principalmente agora, com
o conceito do Big Lake,
ele traz o processo de processamento
também para dentro dele.
Então, está tudo bem.
Mas está tudo certo.
Esse elefante abelha medonho e eu
raivo.
O Spark é o Ronaldinho Gaúcho.
Joga em qualquer time.
Perfeito.
É isso que eu quero que
vocês entendam.
Legal.
Então, existem algumas formas de você
organizar seu Data Lake.
Qual é a melhor forma?
Depende.
Uma das formas que a gente
vai ver,
e eu vou deixar essa demo
para quando a gente estiver passando
no Data Lake,
realmente,
é a gente estruturar em zonas.
A gente usa muito para a
perspectiva de processamento.
Trabalhar com zonas.
E a gente vai ver isso
aqui em detalhes
quando a gente estiver fazendo processamento
de zonas.
Que a gente vai fazer daqui
a pouco.
Pit Stop.
E aí, alguma dúvida? Alguma pergunta?
Respira.
Vamos lá, que eu preciso de
vocês atentos,
porque de 9h10, 9h15 até 10h20,
10h30 é só demonstração.
A gente vai construir um sistema
inteiramente em batch.
Fim a fim.
Com todas as melhores práticas, com
todos os patterns,
com tudo.
Então, eu preciso que cada Pit
Stop desse,
você comite.
Faz o flush.
Não, entendi.
Beleza.
O Data Lake, a gente...
Caralho.
Desculpa por ter falado tanto,
mas eu prefiro pecar por trazer
toda a experiência,
todo o conteúdo.
Se vocês acharem que estão demais
as minhas vizas aqui,
que eu fico um pouco mais
técnico do que um pouco mais
soft,
eu só trago essas coisas,
porque eu acho que é muito
importante vocês entenderem
o conceito como um todo.
Não só a tecnologia.
Eu ia ficar muito doido aqui.
Que ferramenta é essa com o
logo de gota do Open Source?
Ah, isso é uma parte na
IFAI.
Depois dá uma pesquisada.
Muito legal.
Beleza?
Tudo certo?
Alguma dúvida?
Me coloca aqui, gente.
A gente tem 38 pessoas.
Não, não.
Sim, sim, sim.
Dúvidas, sim, perguntas.
Vamos lá.
Pit Stop para a gente.
De novo, vai ficando mais difícil.
Eu estou avisando.
Provavelmente.
Vai ficando mais difícil.
É, vai ficando mais massivo o
negócio.
Então, quatro pessoas.
Vamos lá, gente.
Tudo certo aí?
Vou falar.
Já falei já, gente.
Fica tranquilo.
Não se apressem.
Vou falar sobre data,
vou falar sobre Delta Lake,
vou falar sobre tudo isso.
Fiquem tranquilos.
De novo, vou falar sobre Delta,
mas eu vou falar principalmente o
mais importante.
A gente vai usar Delta em
tudo aqui, né?
Mas assim, vocês precisam entender por
que Delta.
A gente vai entender.
Então, fiquem tranquilos.
Teremos durante o curso também como
escolher o Provider.
Teremos durante o curso sobre como
escolher o Provider.
Vai, vai.
Na sexta -feira vocês vão aprender.
Aprender os nomes ou até falar
mais bonitinho agora.
Arquitetura lambida.
É, arquitetura lambida.
Bora lá.
Beleza.
Então, isso aqui é primordial que
um engenheiro de dados
que não seja um animal de
teta voadora entenda.
Animal de tetas.
Entendam para que o Spark serve.
É isso que você vai entender
aqui.
Tá?
Eu estou brincando, tá, gente?
Já são oito e meia.
Acordei cinco da manhã.
Então, relâmpago.
Gente, vamos entender, tá?
Diferente disso vai funcionar?
Vai da melhor forma?
Não.
Então, vamos entender qual o ciclo
de vida do Spark.
Beleza?
Qual é o ciclo?
Para que eu uso em um
PPT?
Porra, mas por que eu não
pagava só esse PPT?
Eu tenho que pagar o treinamento
mó caro e não sei o
quê.
Você vai entender a porra toda.
Mas, assim, esse PPT te diz
para que você usa o Spark.
Vamos pensar.
O dado chega no Data Lake.
Baseado no que a gente está
discutindo aqui hoje, né?
No dia de terça.
Na mente de terça que a
gente tem, a gente está olhando
na perspectiva de, cara, a gente
vai centralizar os dados,
vai colocar esses dados no Data
Lake e vai botar isso tudo,
enfim.
E esse dado vai entrar dentro
do Data Lake.
Beleza.
Então, eu vou ter os dados
chegando ali
por ferramentas de ingestão e tal.
Beleza.
O dado caiu no meu Data
Lake.
Por quê?
O que eu irei fazer agora?
Ah, agora eu vou utilizar uma
engine distribuída escalável.
Mas, Luan, por que que o
Spark, cara, a gente viu ontem,
distribuída, open source, está em qualquer
provider de nuvem,
diferente das outras engines de processamento.
Já vê todas as outras também?
Tinha que falar de tudo, gente.
Sabe o que é tudo?
É tudo.
Na sexta -feira vocês vão ver
comparação de todas.
Você vai ver quando...
Eu desenho.
Eu vou dar as arquiteturas que
a gente usa na Piffin para
vocês.
Vocês vão ter acesso a tudo
isso.
Então, fique tranquilo.
É um pedaço de cada vez.
Esse treinamento, ele é desenhado gradativo
no Storytelling
para você entender tudo.
Então, eu te garanto que a
dúvida que você tem aqui, ela
está no PPT em algum lugar.
Mas pode perguntar aqui que eu
vou falar para você onde está.
Então, o dado chega, é processado.
Por que que o Spark está
em qualquer nuvem?
Simplifica High Level API e Low
Level API.
Você pode utilizar várias engenharias.
Você pode usar linguagens de programação.
Você pode escrever local e escalar.
Você pode utilizar para Spark agora
unificado.
Você pode utilizar SQL.
Você pode processar batch streaming.
Está vendo?
É por isso que a gente
usa Spark.
Beleza.
E agora o dado é cuspido
para fora.
Aí, agora vem uma parada muito
importante para você entender, mano.
Muita gente entende que a vida
tem que ser assim.
Data Lake, Spark, Data Warehouse.
Não é um animal de teta.
Não tem que ser, tá?
Não é porque está escrito aqui
que tem que ser.
Então, eu vou ensinar vocês a
pensar fora da caixa.
Porque o que eu já vi,
Matheus,
nego gastando milhares de dólares, milhares
de reais em Data Warehouses
que você não precisa naquele momento
ou que não é o caso
de uso para você,
é porque é uma arquitetura.
Sim, mas não é porque está
estampado lá no site da Microsoft.
Não é porque está estampado lá
no site da Google.
Não é porque...
Ou seja, vocês vão ter o
que?
O discernimento analítico e crítico para
falar, opa, calma aí.
Eu preciso jogar isso dentro de
um MPP?
Caraca, animal de teta.
É a última vez que eu
falo isso que eu vou falar
no mundo.
Não, você não precisa.
Entendeu?
Você não precisa fazer isso.
Então, assim, só para você ter
uma ideia,
a recomendação de você utilizar um
Massive Parallel Processing,
um Massive Parallel Processing,
principalmente a Microsoft dá essa recomendação
a partir de um tera.
Você começa o seu projeto com
um tera de dado.
Aí você vai lá, só, cara,
eu digo isso porque tem tanto
cliente,
que o cara, não, mas eu
uso aqui o Azure SQL Data
House
que a Microsoft me recomendou.
Cara, bonito, legal, você está todo
estiloso.
E quanto você gasta?
Ah, eu estou gastando 45 mil
dólares por mês.
Show.
O que você tem aí dentro?
Ah, a gente tem 300 gigas
aqui, a gente usa e não
sei o que tal, massa.
Eu pego um Azure SQL Database,
mil dólares,
eu ganho e a diferença de
segundos é de 4 a 5
segundos de diferença.
Você prefere para alugar 47 mil
dólares ou 1 .500 dólares?
E amanhã vocês...
E na quinta -feira vocês vão
ver isso, na vida real.
A aprender a como desenhar a
sua solução.
Então, não é tipo, porque você
está vendo isso aqui,
que é e você precisa utilizar
isso.
Por isso que vocês têm que
entender o que é Data Lake,
o que é Spark, o que
é Data Warehouse, enfim.
Então, a gente vai entender Data
Lake, processamento,
a gente vai entender todos esses
componentes.
Mas, teoricamente, o dado entra no
Lake, ele é processado,
ele é colocado dentro do Data
Warehouse
e depois ele é visualizado pelas
ferramentas de engenharia
e da gestão.
E aí, o que o Spark
começou a sentir?
O Spark começou a ficar triste.
Por quê?
Ele falou, cara, o nego me
usa e depois me joga fora,
né?
Eu estou brincando, mas eu estou
falando sério, tá?
É isso que aconteceu, de verdade.
A galera me pega aqui, me
usa, me processa
e depois eu coloco um Data
Warehouse.
Tá, beleza.
Desses três aqui, qual é o
mais caro?
De uma arquitetura de Big Data,
vocês conseguiriam chutar aí,
qual é o mais caro dessas
três?
Opa, a milha final, olha que
a galera já está entendendo, né?
Ou seja, as nuvens são bastante
inteligentes, né?
Ou seja, o Data Lake é
super barato, traz o dado.
O sistema de processamento, se você
processar e destruir,
se você fizer técnicas de economia
de dinheiro, saber utilizar,
as máquinas corretas, o que é
econômico ou não,
você economiza muito bem.
E no Data Warehouse, Marcos?
Tirão na cabeça, meu filho.
Aí é aqui que o pau
come.
Aí o bolso dói.
Aí que o bolso dói.
Aqui que o bolso mói, entendeu?
Será que você quer construir uma
solução que o bolso mói
ou você quer construir uma solução
escalável
e que você use o valor
de acordo com a necessidade que
você entrega?
Então, eu não vou te ensinar
a você usar um MPP sem
ter necessidade.
Você vai entender o que é
um MPP, vai entender quais são
os de mercado
e vai saber quando utilizar um
MPP, quando utilizar outras tecnologias
e vai ver a comparação entre
elas.
Para você, como engenheiro de Spark,
não só desenhar a solução que
está no Spark,
mas construir uma solução de arquitetura
de dados como um todo.
Porque você vai precisar saber disso.
Então, o Data Warehouse é o
lugar mais caro.
Então, vamos lembrar disso aqui agora.
Por quê?
O Spark falou, cara, o Data
Lake é econômico.
O Data Lake é o futuro
das organizações, e realmente é.
É onde todas as grandes, pequenas,
médias e gigantescas empresas estão indo.
É o conceito de democratização de
Data Lake, de processar em escala,
por ser barato, por você conseguir
tierizar o seu dado,
por você conseguir democratizar essa informação
entre os silos.
Beleza, o Data Lake faz muito
sentido e é o lugar que
eu quero.
Mas eu sei que é um
lugar não estruturado,
é um lugar complexo para Analytics,
para o final do Analytics.
E o Spark se posicionou e
falou o seguinte,
cara, a gente precisa trazer a
próxima geração.
A gente precisa fazer o quê?
A gente precisa escolher um sistema
de storage,
porque eu sou um sistema de
quê? De processamento.
Eu preciso achar um sistema de
storage para casar comigo.
Eu preciso achar um sistema de
storage
com que eu consiga utilizar e
processar giga, tera e petabytes de
forma eficiente.
Eu consigo utilizar o Data Lake,
mas eu não consigo extrair o
melhor da velocidade que eu tenho.
Então, o Spark começou,
como ser humano, a olhar essas
informações como um todo, beleza?
Então, coisas que ele precisa para
ter um storage decente por debaixo
dos panos.
Primeiro, escalabilidade e performance.
Ou seja, eu tenho que ter
escalabilidade e performance de um Data
Lake.
Eu tenho que ter suporte à
transação. Por quê?
Porque qual a diferença de um
Data Lake para um Data Warehouse,
por exemplo?
Bem, obviamente falando, é que dentro
de um sistema de Data Lake,
você não tem acidez.
Você não tem o suporte transacional.
Diferentemente de um Data Warehouse,
onde você tem um processo transacional,
exatamente.
E isso muda o jogo.
Isso é um game changer. Por
quê?
Porque, às vezes, se você estiver
escrevendo um arquivo,
você tiver um glitch de network,
um problema de network,
você pode sim perder dados que
saíram da sua fonte,
que eram extremamente importantes para o
seu negócio
e que eles, de alguma forma,
não vão aparecer ou vão estar
corruptos.
Então, como você garante isso num
Data Warehouse?
Isso não é um Data Lake.
Você não garante isso num Data
Lake.
Vocês sabiam disso? Não é garantido.
Pode acontecer.
Agora, se eu tenho um sistema
de banco de dados relacional,
isso pode acontecer? Negativo.
A primeira palavra do ácido é
o A. É atômico.
Ou é tudo ou é nada.
Então, o Spark precisava de algo
assim também.
Mas, além de ter as características
de um banco de dados relacional,
eu precisaria ter o quê? A
diversidade de formatos de arquivo.
Por quê?
Porque isso é muito interessante.
Você ter aberturas, poder gravar em
vários formatos,
utilizar vários formatos e assim por
diante.
Eu também preciso endereçar vários tipos
de necessidades
do meu cliente final, do negócio.
Ou seja, meu negócio precisa ser
usuários que consomem dados em SQL,
usuários que escrevem em batch,
usuários que fazem gestão de streaming
para criar sistemas de alerta.
A gente vai falar amanhã de
sistemas muito legais, inclusive.
Eu também tenho que ter meu
time de machine learning
para poder criar algoritmos, para poder
prever, para poder melhorar o meu
churn,
para poder melhorar as minhas predições,
para poder fazer com que a
cereja do bolo aconteça dentro da
minha organização.
Então, além de tudo isso, eu
preciso suportar vários workloads.
E, por fim, eu preciso de
abertura.
Eu preciso fazer com que tudo
isso seja integrável.
Eu preciso com que tudo isso
seja aberto.
E eu preciso que isso se
integre com o que já existe
hoje.
Então...
O Spark precisava de algo assim.
E aí, a primeira coisa que
ele pensou é o seguinte.
Cara, será que a gente vai
criar um banco de dados para
o Spark
que funcione 100 % integrado com
o Spark?
Bem, geralmente o fluxo de um
sistema online segue dessa forma.
E esse é um dos grandes
problemas da escalabilidade de um banco
de dados relacional.
Você write once with many.
Então, você escreve um local e
ele reverbera para várias réplicas.
Mas você tem um ponto centralizado
de escrita, tá?
E aí, se você quiser ter
uma visão analítica disso,
você vai criar vários processos de
data marketing.
Quais são as limitações aqui?
A um certo ponto, você tem,
cara, um tamanho de dados onde
não vai caber.
Dentro de um banco de dados
vai ficar muito diferente.
Você não aceita vários analytics diferentes
ao mesmo tempo.
É caro para escalar ao longo
do tempo, porque você precisa de
licença,
você precisa de infraestrutura para fazer
isso.
E você tem um suporte não
muito bom para o que seja
diferente de SQL.
No outro lado da moeda, se
você olhasse o Data Lake
para ser realmente a paixão do
Spark,
você realmente conseguiria habilitar vários use
cases,
vários arquivos e vários tipos de
sistema.
Ou seja, várias fontes escrevem dentro
do Data Lake
e esse dado fica ali dentro.
Entretanto, por mais que os object
storages consigam fazer isso,
você tem alguns problemas muito importantes.
Primeiro, atomicidade e isolamento.
Você pode ter dado corrompido.
E consistência.
Você pode ter inconsistência do dado
que está sendo escrito.
Por quê?
Se você lê um arquivo que
ainda não está sendo escrito,
ou que ainda não terminou de
ser escrito,
ou que está mal formado,
esse dado que você está trazendo
para a sua camada de processamento,
ela não é correta.
Como você garante isso?
Então, você está vendo que precisava
existir alguma coisa entre esses dois
ou o melhor dos dois?
E é aí que, na verdade,
a gente teve essa grande...
esse grande leap,
que foi a Databricks que trouxe
esse conceito.
Isso é da Databricks, tá?
A Databricks trouxe o conceito do
Lake House.
Ou do Delta Lake, que é
o que mais vocês conhecem.
A gente teve o prazer de
participar não só do lançamento disso,
mas eu fui um dos primeiros
no Brasil a trabalhar com Delta
em clientes,
realmente, em 2017, eu acho.
No meio para o final.
Quando isso era feature primária.
Enfim, só existia no Databricks assim
por diante.
Então, eu vi a evolução de
toda essa tecnologia
que aconteceu ao longo desses quatro,
cinco anos.
Desde a entrada no Databricks, para
comemoramento, para fim.
E o que é o Lake
House?
De novo, assim como o Data
Lake é um conceito,
o Lake House é um conceito.
É o que?
O melhor do Data Warehouse com
o melhor do Data Lake.
Por quê?
Por causa disso aqui.
Nos anos 90, o que a
gente tinha?
Cara, a gente tinha bancos de
dados relacionais e processos GTE.
Era isso que tinha.
Então, você tinha um sistema de
Data Warehouse que previa o passado,
conseguia olhar o passado e o
presente.
Isso era uma arquitetura completamente cara,
completamente difícil de ser implementada
e que poucas pessoas tinham acesso.
Beleza?
Na época de 2000, a gente
teve a evolução disso.
Ou seja, agora eu faço com
que várias fontes de dados escrevam
e não façam mais ETL, mas
agora façam ELT.
Lembra do que eu falei para
vocês?
Em vez de você extrair, transformar,
e carregar com um significado diferente,
agora você extrai das fontes, carrega
no Data Lake,
na sua fonte de verdade crua,
e do Data Lake esse dado
vai para onde você quiser.
Aqui você começa a fazer o
Data Enablement.
Aqui você consegue garantir com que
a entrada seja unificada.
Teoricamente, esse seria o melhor dos
mundos.
Só que dentro do Data Lake,
a gente ainda tem alguns problemas,
porque é um huge pile of
shit.
Foi mal aí, mas é exatamente
isso.
Porque você começa a jogar o
dado lá dentro.
Beleza, você armazena o dado.
Mas será que você consegue constar
estado at scale?
A realidade é não.
Então, para quem nunca passou por
esse problema,
talvez nunca precise passar, porque vocês
vão conhecer o Lake House,
mas se você tiver milhares de
arquivos dentro de uma pasta,
se você tiver centenas de milhares
de arquivos,
ter dezenas de pastas para ler,
as suas ferramentas de processamento
não vão conseguir ser efetivas.
Não é porque não é.
Escalavelmente falando, é basicamente
impossível
por diversas coisas que acontecem por
debaixo das cortinas,
que, inclusive, vocês vão aprender aqui.
Então, gente, a gente precisava de
algo assim.
É um conceito completamente novo.
Foi trago em meados de 2020.
Então, a ideia é trazer o
quê?
É você ter um ambiente que,
no final das contas,
nada mais é do que um
sistema de arquivo,
é um arquivo, um formato, é
um tipo de dado,
porque o formato de arquivo é
o mesmo,
que a gente vai ver agora,
que habilita com que você faça
tudo isso.
E hoje, no mercado, nós temos
três Data Lake Houses de mercado
que a gente fala realmente abertamente
sobre eles,
que é o Apache Hudi, o
Delta Lake e o Apache Iceberg.
Beleza?
O Hudi e o Iceberg são
da Apache,
e o Delta também é open
source,
mas é da CNCF, da Linux
Foundation.
Todos são open source.
Beleza?
Então, isso aqui é literalmente o
santo grau da engenharia de dados
hoje.
Se a gente olha esses três
Lake Houses em relação à comparação,
o que a gente tem?
No final das contas, são tipos
de tecnologias
que vão resolver o mesmo problema.
O problema que o Lake Housing
resolve.
Qual é?
Problemas de inconsistência no Data Lake,
problemas de não estruturar o dado
no Data Lake,
problemas de não conseguir escalabilidade na
leitura escrita no Data Lake
de forma eficiente,
problemas de leitura em grande escala
dentro do Data Lake.
É isso que o Lake Housing
resolve.
E ambas as tecnologias que eu
estou mostrando aqui
resolvem esse mesmo problema.
Por quê?
Porque são Lake Houses.
A diferença entre eles é o
que a gente chama de casamento
das tecnologias.
Então, quando você vai começar a
arquitetar em Big Data,
em engenharia de dados,
você precisa saber qual ferramenta funciona
com outra
e como elas se unem no
ecossistema.
Porque é isso que vai fazer
com que o seu pipeline de
dados
seja efetivamente bom.
É como você junta todas as
peças da melhor forma.
Como a gente está falando de
Spark,
claro que a gente vai ter
uma preferida aqui,
mas todas elas resolvem o mesmo
problema de formas diferentes.
O Douglas perguntou o seguinte,
o Iceberg está ganhando bastante tração,
né?
Sim, o Iceberg tem ganhado bastante
tração.
Inclusive, na sexta -feira, a gente
vai soltar um episódio com o...
Quem foi que a gente entrevistou
ontem?
O Alex Mercer.
O cara da Dremio, só que
engenheiro da Dremio.
É um developer advocate da Dremio.
E a Dremio utiliza agora o
Iceberg como formato de arquivo,
Snowflake e várias outras ferramentas.
Sim, o Iceberg tem ganhado bastante
tração.
É uma ótima tecnologia para você
olhar.
Mas se você estiver falando de
Spark, indiscutivelmente você vai usar Delta.
Mas sim, o Iceberg tem ganhado
bastante tração.
Então, eu diria que hoje, para
a perspectiva do Spark,
para o Spark, beleza?
De 1 a 3 seria primeiro
Delta, segundo Iceberg, terceiro Hudi.
O Hudi é muito mais utilizado
e muito bom para desenhos de
soluções de dados.
E para pipelines primitivos para Apache
Hadoop.
Não que ele não funcione com
outras tecnologias,
mas ele é o melhor endereçado
para isso até hoje.
Pela necessidade mesmo da Uber como
um todo.
Então, no final das contas, o
que você vai fazer?
Você vai pegar o dado que
está no Data Lake
e você vai trazer para dentro
do Lake House.
E a gente vai discutir isso
em detalhes.
Só para vocês entenderem, existem várias
empresas por detrás disso aqui.
O que é legal.
Então, Apache Hudi, quem acelera,
quem acelera ela é a Uber.
O Apache Iceberg, quem acelera ela
é a Netflix.
E o Delta, quem acelera é
a Databricks.
Por isso que a gente fala
que indiscutivelmente,
a gente vai, sempre que estiver
falando de Spark,
é impossível você optar por outro
a não ser o Delta.
Se você for louco de fazer
isso, tá?
Ele vai ser o melhor formato
para que você possa trabalhar com
o Spark.
Por quê?
São dos criadores dele.
Só para você ter ideia hoje,
dentro da Databricks,
se existem dois times completamente separados.
O time de Spark, os comitês
de Spark e o time de
Delta.
Não existe, cara, aceleração de Delta.
Hoje é um time completamente separado
do Spark, CNCF, enfim.
Ou seja, é um outro produto,
é um outro projeto acelerado por
milhares de empresas.
Tem uma adoção absurda.
Tanto é que hoje você, para
pipelines que começaram desde 2020 para
frente,
impossível você não ver formato Delta.
Impossível você não ver arquiteturas novas.
Ele é a recomendação, ele é
a base do Databricks também,
como padrão e assim por diante,
tá?
Então, o Delta é o cara
mais usado para ele, sim.
Então, qual a diferença do Lake
para o Lake House?
Se vocês tiverem dúvida aqui, a
gente já está chegando à demonstração.
A diferença, pensa o seguinte, no
Data Lake embaixo,
um ambiente cru, um ambiente que
não tem os dados, né?
Estão totalmente separados ali.
Você não tem Skinforce.
Ou seja, você não vai gravar
dentro de uma tabela no Lake.
Você grava e você joga um
arquivo JSON lá dentro.
Depois você joga um outro arquivo
e assim por diante.
E aí, eventualmente, a gente vai
ter Swamp e problemas de qualidade.
O que é um Lake House?
É um metadado, é um layer
de metadado
que possui um engine inteligente
para que possa ser performático consultar
os dados num formato arquivo.
Quem usa hoje?
Todas as maiores empresas do mundo,
tá?
Uber, Netflix, Apple.
Inclusive, em 2019 tem uma apresentação
do time da Apple sobre isso,
que inclusive eu vou trazer aqui
para vocês terem ideia de como
isso é foda.
Facebook, Airbnb, Verizon, BMW, Mercedes, Check
Point.
Cara, mais o quê?
New York Times, Spotify.
Caraca, sei lá.
Walmart, caraca, enfim, tudo, velho.
Tudo utiliza Lake House hoje, tá?
Beleza.
Só que o Lake House, ele
resolve, de fato,
vários problemas que vocês vão entender
durante o treinamento.
Para que vocês não cheguem e
falem o seguinte,
cara, mas tá usando Lake House,
por quê?
Beleza, você não sabe o porquê.
Você sabe que é foda, que
tem um novo formato,
mas por que você usa ele
realmente, tá?
Então, a melhor forma da gente
entender
isso é olhar quais são as
features que ele traz
e como que ele é usado,
por quais clientes e por que
isso aconteceu.
Então, o Delta Lake, ele, o
quê que ele é?
Ele é um formato open source.
Ele é um engine de storage,
tá?
Baseado em o melhor de banco
de dados relacional,
ou seja, o melhor do Data
Warehouse,
de todas as semânticas, enfim, de
tabelas realmente,
com a abertura do Data Lake.
Então, ele fica em cima do
Data Lake.
É como se você tivesse ele
em cima do Data Lake.
O Bruno está perguntando qual seria
a diferença
do reflection do Dremel para os
metadados do Delta.
Fazem de formas diferentes?
São coisas completamente diferentes.
O Delta, ele tem um arquivo
inteligente de indexação metadado
que o Spark, que as engines
de processamento usam
para consultar eficientemente o Data Lake.
Mas eles batem no Data Lake.
O reflection...
O Dremel é você pegar o
dado que está no Data Lake
e você compactar, colonizar, segmentar,
comprimir,
botar dentro de um sistema que
troca mensagens em memória,
que é o Arrow, e socar
isso na memória.
Então, sempre zero milissegundos, tá?
Essa é a diferença, basicamente.
Então, o que que o Delta
faz?
O Delta traz vários benefícios aqui
para a gente.
Coisas que a gente não conseguiria
nunca no Data Lake.
Então, vamos lá.
Primeiro, transação ACID.
Porra!
Tem noção que você fazer transação
ACID dentro de um arquivo, velho?
Isso é foda, tá?
Metadado escalável.
Então, cara, um dos grandes problemas
de você escrever dado
é você ter metadado que escale
de acordo com o dado.
Então, ele usa a engine do
Spark para escalonar.
Fica tranquilo que a gente vai
ver tudo isso em detalhe, tá?
Time travel, tá?
Imagina que você pode voltar no
tempo.
Então, imagina que alguém fez um
insert errado
ou imagina que você tem uma
versão que você queria voltar da
tabela
ou imagina que você quer voltar
no tempo
para saber o que aconteceu uma
semana atrás naquela tabela.
Você consegue fazer isso, tá?
Quando você fala de ACID, você
fala de insert em um parquet
file,
quando eu falo de ACID, eu
não estou falando de...
Eu estou falando do conceito ACID.
Quando ele acontece, sim, ele vai
escrever dentro do parquet
de forma atômica,
tá?
Estou ouvindo muita gente falar de
ACID, mas o que é?
Ah, boa pergunta.
Para quem não conhece ACID, boa
pergunta, Matheus.
Bom, o ACID, ele é o
conceito mais importante de banco de
dados relacional.
Então, o que é o ACID?
O ACID são quatro características que
fazem com que bancos de dados
relacionais sejam bancos de dados relacionais.
Então, qual é a grande característica
de um banco de dados relacionais
e por que todas as empresas
do planeta utilizam bancos de dados
relacionais?
Porque ele é confiável.
E por que ele é confiável?
Por causa do teorema ACID.
Então, bancos de dados relacionais, todos
eles, Oracle, MySQL, PostgreSQL,
MariaDB, tudo que você vê que
é um banco de dados relacional,
eles são obrigatoriamente regidos pelo mesmo
teorema,
que é o teorema ACID, que
foi criado por Jim Gray,
que ganhou um Turing Award e
foi velejar de felicidade
e nunca mais voltou, infelizmente.
Trabalhou na Microsoft, um gênio da
computação,
infelizmente, nós perdemos, tá?
É porque ele foi velejar lá
em Seattle e simplesmente desapareceu.
O que que o ACID faz,
tá?
O ACID stands for Atomicidade.
Ou seja, quando eu vou escrever
num banco de dados,
ou acontece ou não acontece.
Então, se eu estou escrevendo 10
linhas dentro daquele processo atômico,
daquele pacote que eu mandei, ele
não vai escrever 9,
ele não vai escrever 5, ele
vai escrever 10 ou ele não
vai escrever.
Beleza?
Segundo, consistente.
Consistente é a visão com que
eu comecei
é a mesma visão que eu
irei realmente terminar.
Então, a consistência, ela garante o
quê?
Que quando eu fiz uma transação
e esse processo está acontecendo,
aquela informação, ela vai estar disponível
após todos aqueles outros protocolos serem
atendidos.
Então, eu sou consistente ao processo,
tá?
Não posso ter interferências naquela transação.
Eu sou consistente.
Isolável, ela é isolada de outras.
Então, quando eu vou escrever numa
tabela,
só eu estou escrevendo naquela tabela
naquele momento.
Ninguém mais está fazendo isso.
Só eu estou fazendo isso.
Tá?
Principalmente por causa do teu NEMA
ACID.
Então, eu estou isolado.
E durável é o quê?
Na hora que o dado foi
escrito e o banco de dados
retornou para você,
ei, sucesso.
A partir desse momento, se acontecer,
teoricamente,
um outage, um earthquake ou qualquer
coisa que acontecer
e o seu sistema voltar,
é para que o que foi
escrito, o que foi comitado no
log,
esteja possível para que você possa
consumir depois.
Então, essa é a maior característica
de um banco de dados relacional
que a gente tem no mercado.
É a característica ACID que a
gente nunca teve isso dentro do
data lake.
Então, trazer isso para dentro de
um formato arquivo é muito foda.
Outras três coisas.
Formato aberto.
Então, você pode, qualquer hora, não
utilizar mais o data lake.
E você pode utilizar ele em
vários outros lugares, porque ele é
aberto.
Ele não é proprietário de uma
tecnologia específica.
Uma coisa muito fora do comum
para mim, isso aqui.
Isso aqui é foda.
Eu faço isso.
Isso aqui é muito foda.
É você escrever batch streaming na
mesma tabela.
Então, imagina que você pode ter
dados chegando em tempo real do
Kafka
e você fazer um insert de
um bilhão de linhas na mesma
tabela ao mesmo tempo.
E você consegue unificar isso junto
com o Spark.
E você pode ter evolução de
esquema.
Ou seja, coisas que você não
tinha nunca num data lake.
É você conseguir evoluir o esquema
e forçar aquele esquema.
Então, eu quero uma tabela dentro
do data lake que tenha isso,
isso e isso.
E ela vai ser escrita desta
forma.
Beleza?
Você tem processo de auditoria.
Então, você tem todo o metadado
rastreável.
E você consegue fazer updates e
deletes e utilizar hoje a Surface
Chess Caring.
Então, você tem todo o metadado
toda inteira para fazer várias coisas.
Priestalmente, você pode acessar hoje o
Delta Lake com Scala, Java, Python,
com todas as APIs totalmente integrado
com o Spark.
Será que isso aqui merece um
foda, pessoal?
Merecemos um foda aqui?
É isso! Merece demais.
Boa!
Gostei da galera.
galera. Beleza.
A gente vai dissecar isso aqui
agora
no nível foda, tá? Juliano,
Daniel, Vel, que a gente fica
tranquilo.
Todo mundo aí que falou, a
gente vai dissecar.
Alguns quick stats pra vocês já
ficarem animados aí, tá? Um exabyte
de dados processados
todos os dias no Delta
de todos os clientes que usam
hoje. Coisa
tranquila, pouca coisa.
75 % de redução de acesso
de dados dentro do Data Lake.
Coisa
tranquila também. Então, assim, o cara
pra poder ter um analítico dele
ele tinha que consultar um terabyte
de dados.
75 % só vai ser
consumido desse um terabyte de dados,
tá?
Então, sei lá, se a gente
tá falando de um terabyte de
dados
a gente tá falando
de
250
um peta
250, tá? Vamos falar de um
tera
ele vai escanear somente
250 gigas pra dar acesso a
esse
data. Puta que pariu, isso é
foda, tá? A gente vai ver
porque que ele faz isso.
E mais de 3 mil clientes
em produção hoje. Se isso é
tão
complexo, foda assim,
por que ele seria tão mais
barato que o Data Warehouse?
Caraca, Matheus,
olha a pergunta do Lucas
às 8h55 do dia 2.
Lucas, foda a pergunta.
Gostei. Segura
ela pra quinta -feira
que a demonstração
e tudo que a gente vai
falar vai exatamente
elucidar exatamente isso que você perguntou.
Eu gostei da provocação
de você bater na tua cabeça
e falar
uai, esse cara tá falando de
um
sistema de storage, open source
não sei o que. Por que
que eu preciso de um Data
Warehouse então?
Então, lembrem que o Data Warehouse
é um conceito, não é uma
tecnologia.
Então é muito bom, Lucas.
Isso ainda vai ser reduzido com
Delta 2 .0
a redução de date scanning. É,
o date scanning
já foi também reduzido pela melhoria
do processo de metadado que a
gente vai ver aqui
também, beleza?
Outra coisa muito foda do Delta
o Delta Sharing. É um novo
protocolo de compartilhamento de dados.
Isso aconteceu muito durante a pandemia.
Que basicamente
foi o seguinte, empresas trabalhando
para a solução do Covid, né?
A gente saiu de uma pandemia
a gente ainda tá na pandemia,
enfim, uma loucura.
Perdemos muito,
ganhamos pouco,
ganhamos algumas coisas, perdemos muito mais,
mas enfim, é um processo
muita gente sofreu durante isso.
Famílias, enfim.
Falar sobre isso é muito chato,
é muito triste.
E a gente teve
algumas coisas,
que saíram disso, sabe?
Uma das coisas aqui foi a
gente conseguir
em tempo recorde, descobrir
certas coisas que a gente não
conseguiria
há 20, 30 anos atrás. Uma
das coisas
por causa do Delta Sharing, tá?
O Delta Sharing foi utilizado pra
uma das maiores empresas farmacêuticas
do planeta, entre elas, pra compartilhar
informações. Então as empresas
trabalhavam com Pfizer e assim por
diante,
trabalhavam em como
entender o Covid
e aí, na verdade,
eles tinham os próprios data lakes,
deles, e eles precisavam compartilhar
essas informações. E, cara, isso não
era possível
pela internet, isso não era possível
pro cliente servidor, porque a gente
tava de teras de dados,
a gente tava de data sets
gigantescos.
Então o que o Delta fez
foi criar
um protocolo de compartilhamento eficiente.
Basicamente, pra eu reduzir aqui,
porque a gente não vai entrar
em muito detalhe,
quando você manda uma query do
recipiente
do cara que vai fazer
ou do cara que recebe,
ele não vai mais carregar esse
dado,
ele não vai mandar mais essa
query pro servidor
carregar no cliente pra te mostrar,
na verdade, ele vai dentro
de uma tabela Delta e ele
escaneia somente o que ele precisa
fazer passando pelo protocolo
Delta. E isso é muito
foda. Então agora eu posso chegar,
por exemplo,
se eu tenho duas empresas diferentes
e eu quero compartilhar o data
set,
eu dou acesso pra ela, eu
faço uma query
que vai consumir terabytes de dados
e, na verdade, ela vai dentro
do ambiente do cara,
faz o data scanning,
traz somente
os links que são necessários pra
serem
acessados, processa
do lado do servidor e envia
somente
o resultado pra esse cara. Então
o protocolo
é completamente proprietário do Delta,
que é open source e hoje
você pode
ter um ambiente onde você
compartilha os seus data sets e
cada
vez mais existem recipientes
integrados a esse tipo de cara.
Panda já é, Tableau já
funciona, Spark também, cada vez
mais o sistema tem crescido. Isso
é uma coisa
muito legal pra gente olhar pros
próximos
anos. É um protocolo de como
você vai
compartilhar entre empresas,
entre, talvez, distribuições diferentes de
Spark dentro da sua empresa e
assim
por diante.
Tá, mas, e como
eu faço a ingestão dos dados
dentro
do Delta Lake? Existem algumas formas.
Hoje, a gente tem várias
formas de trazer o dado pro
Delta Lake.
Então a gente pode ter ali
ferramentas de parceiro que já
funcionam. Stream Sets,
Syncsoft, Data Factory,
5E -Trim,
Clique, já são ferramentas
que escrevem diretamente
em formato Delta, tá? Você não
precisa,
você não precisa passar pro Data
Lake
e depois eventualmente fazer isso. A
gente
vai ver aqui em detalhes, fica
tranquilo. Mas
você pode utilizar isso. O Databricks
tem um
carregador
proprietário pra otimizar
a escrita do dado
dentro do Delta Lake, que é
o
Auto Loader, tá? Que a gente
vai
ver também. É uma feature específica
do Databricks que a gente vai
ver, porque é interessante.
E você também pode trazer os
dados lá do Carfico pra dentro
do Delta.
Você também consegue fazer isso pelo
Spark,
utilizando Structural Streaming. A gente vai
ver também no treinamento.
Alguns casos
de uso, só pra vocês terem
ideia
da agressividade de quando
isso foi realmente feito e o
que
isso aconteceu. Vocês podem usar o
Foda
hoje mais uma vez, porque
esses casos de uso são pra
vocês terem
realmente a ideia
do quão isso é
significativo pra perspectiva
do negócio, como eu sempre falei
pra vocês.
Olha só, se
alguém tem um celular desse,
um iPhone ou qualquer
aparelho Apple, o que que acontece
hoje, tá? Hoje, todas
as informações de saúde
do que tá acontecendo no celular
são enviadas pro Data Lake da
Apple.
O Data Lake da Apple, em
2019,
essas são informações de 2019,
tá?
Meados de 2019 pra 2020.
Ainda é utilizado, logicamente.
Essa infraestrutura da Apple tá dentro
da Amazon.
Essa parte de telemetria.
Eles recebem essas informações,
tá? E eles ingerem essas informações
numa tabela delta, que a gente
chama de bronze,
tá? Tabelas de ingestão. A gente
vai
ver em detalhes isso aqui.
Logo depois, eles criam
várias tabelas diferentes, refinadas,
melhoradas, onde vários
analistas trabalham em modelos de
machine learning pra entender as
características
saúde do telefone e
assim por diante, criar técnicas,
software, aprimoramentos, pra que a gente
não tenha dor de cabeça utilizando
o celular.
Só que, o que que acontecia?
Eles tinham,
na época, 504
terabytes de dados
de telemetria. A minha
pergunta pra você é a seguinte.
Você realmente, de forma
naiva e de forma muito inocente,
acredita que existe alguma
ferramenta que consiga rapidamente
consultar 504 terabytes
de dados
de um Data Lake?
Me responda aí se vocês acham
que isso é efetivamente,
realmente possível.
O que que vocês acham?
Exatamente. Nunca. Essa é a palavra.
Não é efetivo.
Ou seja,
não é bem assim.
Tá?
O que que eles fizeram?
Antes, até esse momento dos 504
terabytes, é, além de ser muito
caro,
até o
momento dos 504 terabytes, eles tinham
um time de 20 engenheiros
de dados. O que que eles
fizeram?
Pegaram esse processo, criaram
as tabelas deltas, ou seja, os
dados
caíam dentro do Data Lake, eles
ingeriam
pras tabelas deltas, refinavam
essas informações, e aí
de 504
terabytes,
por causa do delta,
eles agora trabalhavam num
dataset aproximadamente de 36
terabytes em vez de 504.
Por quê? Porque agora, na hora
que eu fazia query em cima
das tabelas,
eu tinha o quê? Uma tecnologia,
uma inteligência que reduzia
a quantidade de consultas que eu
Isso eu ia fazer. Por quê?
Antigamente, o que que eu tinha
que fazer?
Pegar os 504 terabytes,
teoricamente, se eu fosse ler tudo
do Data Lake,
né? Pegar os 504 terabytes,
jogar na memória do Spark
e depois consultar. Com o Delta
Lake,
antes de eu carregar
pra memória, eu tenho
uma inteligência que vai olhar o
metadado, só vai consultar
o que eu preciso trazer,
vai carregar isso de forma eficiente
na memória, pra que eu possa
depois fazer a query.
Então, agora, eu tenho,
a possibilidade de fazer o que
a gente chama
de Date Skipping, que é
ignorar o que eu não preciso
ler.
De 93 %
no cenário deles. Então, eles conseguiram
sair de queries que demoravam 7
ou 9 horas pra serem
executadas, pra consultas que demoravam
entre 3 a 5 minutos.
Esse é o caso de sucesso
da
Apple com o Delta Lake.
Ele foi testado antes da adoção
de mainstream do Delta.
Outra empresa que fez isso,
a Glacian, pra quem não conhece
o produto, está a Glacian, né?
Existem vários.
Confluence,
Bitbucket, e assim por diante.
Existem várias ferramentas ali dentro.
A Glacian é um dos clientes
que utilizam o Delta Lake. Eles
têm
um ambiente de multi -petabyte,
eles ingerem 24 terabytes de
dados por dia. Existem mais de
3 mil clientes
e usuários internos
e mais de 200 engenheiros de
dados
dentro da Square, dentro do time
dos carros.
Grande, né?
Na primeira geração de tentativa
deles, pra lidar com a quantidade
de dados,
eles utilizavam Postgres e
Redshift. O problema
é que eles tinham sistemas diferentes.
No Postgres, eles tinham
vários problemas de escalabilidade,
de flexibilidade, por causa da quantidade
de informação. E no Redshift
ficou muito caro pra eles
processarem exabytes de dados.
Então eles foram pra segunda geração.
Na segunda geração, eles fizeram o
quê?
Jogaram as informações inteiras dentro do
Delta Lake
e utilizavam o Hive
e o Presto pra fazer queries
em cima disso. Isso resolveu muito
problema, mas as queries
eventualmente ficavam muito
mais demoradas, porque
não existia uma estrutura. A terceira
geração, que é o que tá
acontecendo agora na
transmissão deles, eles estão utilizando
o Databricks e todo o conceito
de Lake House pra trazer as
tabelas
pra dentro do Lake House e
fazer query.
A comparação
de tempo da segunda geração pra
terceira
geração é de aproximadamente 70 %
a 80 % de ganho
na Front Analytics, por causa disso.
Beleza. Então a gente
viu dois casos de uso de
como, do que eles fazem.
Mas como que a gente utiliza
a arquitetura
Delta? Bem, quem tava na minha
live
passada, pra quem já passou nas
minhas lives, viu a gente falar
dessa arquitetura Medallion, né? Eu vi
vocês
aí falando sobre isso. O que
que é a arquitetura Medallion? A
arquitetura
Medallion, que antigamente era a arquitetura
Delta, tá? Eles fizeram um rebranding
do nome, principalmente por causa
do Change Data Feed, que é
uma das features que traz a
possibilidade
de você capturar somente o diferencial
entre as tabelas Delta.
É uma arquitetura que a gente
usa
quando a gente tá utilizando Spark
e Delta.
Então quando a gente tá utilizando
Spark e Delta,
a gente utiliza essa arquitetura Medallion
porque ela funciona
muito bem para o Spark
com o Delta. E
basicamente a gente vai construir
nossa arquitetura Medallion de
batch na terça, depois a gente
vai construir nossa arquitetura Medallion
de streaming na quarta,
pra vocês verem um pipeline
fim a fim no Spark.
Vocês vão ver hoje vários
processos, tá? Então basicamente
você tem os dados no Data
Lake,
a gente vai trazer esses dados,
vai escolher
hoje, hoje a gente vai trabalhar
com a camada
batch, mas a gente vai trabalhar
com a camada
streaming. Lembrando que eu
prometi pra vocês que o Spark
é um engine
o quê? Unificado. Você unifica
o processo de streaming batch
no mesmo pipeline. Então a gente
vai
ver se realmente isso é uma
verdade, beleza?
Então a gente vai trazer,
essas informações pra uma tabela
bronze, que é só o nome,
que é uma tabela crua, ou
seja, o que tá
no Data Lake vai tá no
Delta,
vai tá ali na tabela bronze.
A gente vai discutir sobre isso
enquanto a gente
joga esse código pra ser
executado.
A gente tem tabelas silvers, que
são
tabelas enriquecidas, que são
tabelas melhoradas, a gente vai ver
isso.
E posteriormente essas informações
são prontas pra serem colocadas, o
quê?
Pra visualização do negócio.
E isso,
monta a arquitetura que a gente
vai
trabalhar ao longo de terça e
quarta
feira, beleza? Então
agora a gente vai construir
um sistema inteiro
utilizando o Spark, todas
as melhores práticas que a gente
viu, todos
os conceitos de Data Lake, todo
o processo
fim a fim.
Então hoje a gente vai construir
a nossa camada
batch, beleza? São
nove e sete, então
vamos voltar, nove
e dez,
nove e vinte, deixa eu ver,
dez, dezessete, nove
e vinte. Então a gente volta
nove e vinte
e a gente vai construir
essa camada inteira, beleza?
Aí a gente vai ter umas
dez e vinte, mais ou
menos,
pra poder entregar todo
esse processo, beleza?
Então nove e vinte a gente
começa
Spark, tá? Nove e vinte em
ponto, eu já vou
começar aqui, beleza?
Então vejo vocês daqui a pouco.
Bebe água, relaxa, faz um café,
se prepara.
Bebe água, relaxa, faz um café.
Bebe água, relaxa, faz um café.
Bebe água, relaxa, faz um café.
Bebe água, relaxa, faz um café.
Bebe água, relaxa, faz um café.
Bebe água, relaxa, faz um café.
Bebe água, relaxa, faz um café.
Bebe água, relaxa, faz um café.
Bebe água, relaxa, faz um café.
Bebe água, relaxa, faz um café.
Bebe água, relaxa.
Bebe água, relaxa.
Bebe água, relaxa.
Bebe água, relaxa.
Bebe água,
Bebe água, relaxa, faz um café.
Bebe água,
relaxa, faz um café.
Bebe água, relaxa, faz um café.
Bebe água, relaxa, faz um café.
Bebe água, relaxa, faz um café.
Bebe água, relaxa, faz um café.
Bebe água, relaxa, faz um café.
Bebe água,
relaxa, faz um café.
Legenda Adriana Zanotto
Legenda Adriana Zanotto
Legenda Adriana Zanotto
Legenda Adriana Zanotto
Legenda Adriana Zanotto
Legenda Adriana Zanotto
Legenda Adriana Zanotto
Legenda Adriana Zanotto
Legenda Adriana Zanotto
Legenda Adriana Zanotto
Legenda Adriana Zanotto
Legenda Adriana Zanotto
Legenda Adriana Zanotto
Legenda Adriana Zanotto
Legenda Adriana Zanotto
Legenda Adriana Zanotto
Legenda Adriana Zanotto
Legenda Adriana Zanotto
Legenda Adriana Zanotto
Legenda Adriana Zanotto
Legenda Adriana Zanotto
Legenda Adriana Zanotto
Legenda Adriana Zanotto
Legenda Adriana Zanotto
Legenda Adriana Zanotto
Legenda Adriana Zanotto
Legenda Adriana Zanotto
Legenda Adriana Zanotto
que vocês pegaram essa ideia.
The Spark of Spark.
Eu quero que vocês entendam
que a gente, de fato,
vai fazer hoje.
Então, vamos lá.
O que a gente vai ver?
O que a gente viu?
A gente viu que no Spark
eu posso escrever local
e eu tenho vários outros lugares
onde eu posso fazer o deployment
daquele código que eu escrevi local.
Ontem a gente viu
a possibilidade de colocar esse código
no Databricks e no Kubernetes
e também no Synapse Analytics.
Tem um ponto muito importante também
que é o seguinte,
a experiência de notebook,
que é uma coisa muito usada.
Então, ali a gente escreveu na
tela preta,
numa IDE, enfim,
e tem o que a maioria
das pessoas conhecem
pela adoção do Databricks
ou a experiência do notebook
que a gente vai ver hoje,
que é muito mais fácil realmente.
Então, o que acontece?
Vamos dar uma olhada aqui
em algumas coisas muito fodas.
Então, em qualquer nuvem,
hoje o Databricks é disponibilizado.
Então, você tem acesso a ter
o Databricks
em qualquer ambiente
e para fazer isso dentro do
Azure
você pode vir aqui no Databricks.
Você tem que digitar Databricks,
vai ser Azure Databricks
e aí você tem aqui um
Create
e você pode criar um ambiente
do Databricks.
Na hora que você terminar a
criação,
ele vai te dar acesso a
um ambiente,
a um workspace, né?
E esse workspace é exatamente isso
aqui.
É isso aqui que vocês vão
ver.
É recente o que aconteceu,
aconteceu toda uma reformulação aqui dentro
e hoje o que a gente
tem
é um ambiente bem segregado,
eu gosto bastante,
bastante de como eles fizeram isso,
Então, eles dividiram em três grandes
chunks.
Eles dividiram em Data Engineering,
Data Science e Data Engineering,
em Machine Learning e SQL.
A gente vai ver Data Science
e Engineering
e a gente vai ver SQL
também, beleza?
Por causa do Databricks SQL.
A gente vai falar sobre ele
lá na quinta -feira.
Mas a gente vai falar aqui
do processo normal de engenharia.
Tá, Luan, qual é a primeira
coisa
que eu faço dentro do Databricks?
Bem, o Databricks, lembra o que
ele é?
Ele é a solução.
Ele é a solução Enterprise
para fazer com que o Spark
seja muito mais fácil de você
utilizar.
Tá?
Então, ele abstrai toda a complexidade
inicial
para você trabalhar com o Spark.
Qual é a primeira complexidade
que eu tenho inicial
para trabalhar com o Spark?
A criação de um cluster.
Então, a primeira coisa
que a gente faz aqui no
Databricks
é vir na aba de Compute
para criar um quê?
Um cluster de Spark.
Então, aqui eu venho em Create
Cluster
e eu tenho a possibilidade
de criar um cluster de Spark
com Next Next Finish.
Cara, isso na época
quando foi realmente colocado
em Mainstream,
que a gente utilizou isso,
cara, é uma coisa fenomenal.
Imagina.
Cara, você não tem
time to market gigantesco.
Em 20, 30 minutos,
você tem ali
o cluster criado.
Porra, sensacional.
Então, eu vou chamar ele
de OWSHQ TRN Spark.
Esse é o nome do meu
cluster.
Eu tenho três modos de criação.
Eu tenho o
Single Node,
que não existia antes.
Eu tenho o Standard
e eu tenho o High Concurrency.
Beleza?
Vamos explicar os três.
Single Node.
Agora, o Spark traz a possibilidade
de você ter um nó.
Isso quer dizer o quê?
Cara, basicamente,
a mesma ideia
de ter tudo num local só.
Para que eles trouxeram o Single
Node?
Para desenvolvedores.
Para fazer com que fique mais
fácil ainda.
Você tem um ambiente pequenininho ali
onde você vai testar algumas coisas,
vai fazer uma POC,
vai fazer um processo fim a
fim.
E fica fácil de você trabalhar.
O Standard, o padrão.
Então, normalmente,
é um cluster normal de Spark.
Lembra, quando eu falei para vocês,
que toda vez que você submete
um código para o Spark,
ele está dentro de uma sessão?
O Spark, ele foi desenhado
para estar dentro de uma sessão.
E, cara, ele não é um
sistema desenhado
para receber 300 bilhões de sessões
ao mesmo tempo, enfim.
Então, ele não é desenhado
para alta concorrência.
Ou seja, para você ter várias
pessoas
trabalhando.
Então, ele não é um sistema
ele geralmente executa
grandes batches de jobs.
É assim que ele é desenhado.
Ele é um sistema de processamento
escalável
onde você recebe o código do
processo.
A Databricks tem um tipo de
cluster específico
que você vai ver nela,
que se chama de High Concurrency.
Que ele é otimizado para várias
pessoas
trabalharem no mesmo ambiente.
Só que, notem, ele suporta SQL,
Python e R.
Ele não suporta escala.
Porque ele é contenhizado.
Então, ele é um sistema de
um sistema um pouquinho diferente
do conceito aqui.
Ah, Luan, qual que você usa?
Cara, 95 % dos casos
você vai usar Standard.
Ah, Luan, quando que você usa
um High Concurrency?
O Velker perguntou.
Então, em uma equipe
teremos mais ou menos
um cluster por usuário?
Não.
Você pode ter por alguns usuários.
Eu só estou falando
que ele não é desenhado
para ter concorrências múltiplas.
Vários pipelines
executando ao mesmo tempo.
Ele não é feito para isso.
Ele é feito para você
executar ali em sessões, tá?
Se você quer, por exemplo,
se você tem um time grande
e você quer utilizar um cluster
só,
aí eu opto a te recomendar
o High Concurrency.
Ele vai funcionar muito bem.
Contanto que você não utilize escala,
né?
Beleza.
Mas, ah, Luan, eu quero utilizar
o Standard.
Você consegue usar?
Consegue usar muito bem.
Só estou falando que você não
vai ter
200, 300 sessões
executando ao mesmo tempo.
Você vai ter problema
de out of memory, com certeza.
Porque ele foi desenhado
para processos de ETL massivos, tá?
Não precisa ser um por pessoa,
mas é muito importante
que você entenda
que não é um sistema desenhado
para ter trilhões de computações
acontecendo ao mesmo tempo
por causa do processo como um
todo.
Beleza?
Aqui que vem um ponto muito
importante.
O que é que a Databricks
fez?
Ela encapsulou os problemas
que a gente tem de dor
de cabeça.
Então, o que é que ela
faz?
Ela tem esse conceito
de Databricks Front Time Version, tá?
Que é o quê?
Ela pega tudo, encapsula tudo
para você não ter dor de
cabeça.
Então, ela chega para você
e fala o seguinte, olha,
a versão mais atual do Spark
é 3 .3 .0.
Se você clicar aqui,
você vai ter um cluster 3
.3 .0
com tudo, com os bits, com
os bytes,
tudo funcionando bonitinho
e com algumas entregações
já funcionando.
Então, ele abstrai essa complexidade
de você ter que pegar os
jars,
configurar os jars, com delta, enfim,
já está tudo bonitinho,
está cotado aqui.
E ele tem dois tipos de
runtime.
Um runtime vinculado ao standard
para engenharia de dados
e runtimes vinculados a machine learning.
Qual é a grande diferença aqui?
É que você vai ver que
no standard
você não tem a utilização de
GPU.
Por quê?
Porque, cara,
o GPU faz muito sentido
para machine learning,
não faz tanto sentido
para engenharia de dados.
Por quê?
Porque dentro de GPU
você vai processar muito mais rápido
em placas de vídeo
o seu algoritmo de machine learning
do que em CPUs.
Então, ele usa
esses GPUs para acelerar.
E claro que vai ser mais
caro,
obviamente, tá?
Beleza?
Tá.
Deixa eu mudar aqui
para o standard.
Aqui, você escolhe
o minho máximo.
Então, aqui você fala, olha,
worker e driver.
O worker é o quê?
É o cara que trabalha, né?
Então, quantos workers eu quero?
Eu quero ter o mínimo de
4 workers
e eu quero ter boa minho,
né?
E eu quero ter o máximo
de 15 workers.
Então, automaticamente,
ele vai andar entre esse min
e max.
Você também pode escolher
e utilizar spot instances, tá?
Para quem não conhece
esse conceito de spot instance,
é o seguinte,
spot instance, para alguns casos,
é muito legal,
que é o seguinte,
são máquinas pré -aquecidas,
máquinas que estão em configurações
mais um pouco, às vezes, ultrapassadas,
máquinas que são utilizadas
por vários sistemas ao mesmo tempo
e que não tem nenhum dono
dentro da infraestrutura da AWS,
do Azure ou dentro da GCP.
Por isso que se chama
de máquinas de spot.
Então, você tem um bid que
você faz
para ter o acesso a processar
ela.
O lado positivo é que isso
salva
muito dinheiro,
porque são recursos compartilhados.
O lado negativo disso
é que pode acontecer
no meio de um processo,
essa spot instance teve um bid
maior
de um outro cara que pegou
e ela para o processamento aqui
e vai para o outro cara.
Então, já entendeu, né?
Se você está rodando certos pipelines
que são críticos,
normalmente, a gente não usa,
usa spot instance.
Quando a gente tem pipelines
que podem flutuar,
podem demorar um pouquinho mais,
mas vão ser mais baratos,
a gente usa spot, beleza?
Enable outscaling
para ele escalar automaticamente.
Se eu tirar o enable outscaling,
ele vai só me dar a
quantidade de workers.
E isso é uma das grandes
características
do Databricks que ele trouxe
e essa é a grande diferença
dele.
E aqui você tem o terminate
after, né?
Ou seja, ele desliga o cluster
depois de um certo tempo.
Então, se eu tivesse 120 minutos,
de inatividade,
se eu não tiver nada executando,
isso aqui é importante.
Muita gente pergunta o seguinte,
ah, mas se eu tiver 30
abas
do Databricks aqui abertas
e tiver um termination after 120
minutes,
ele vai fazer o shutdown?
Vai.
A não ser que você esteja
alguma coisa rodando.
Se tiver alguma coisa running, né?
Não, porque ele está rodando.
Porque é justamente 120 minutos
de inatividade, beleza?
Criou o cluster,
no final você vai ter aqui,
o seu cluster provisionado por quem,
qual foi a fonte,
qual a quantidade de DBUs,
que é Databricks Units,
você paga por isso aqui,
qual é o runtime,
qual a quantidade de memória.
Então, olha que coisa linda.
Em 5, 7 minutos,
eu criei um cluster de Spark
que tem 16 cores e 112
GB de RAM.
Atualmente, eu estou utilizando 3 máquinas,
ou seja, o mínimo.
A gente vai ver que ao
longo
que eu vou processando,
ele vai automaticamente escalando.
Então, está vendo?
A primeira impressão,
a impressão do Databricks é muito
bonita,
porque faz muita facilidade para você.
Qual é a primeira coisa que
a gente vai fazer aqui,
gente, no nosso processo?
Cara, a primeira coisa que a
gente vai fazer
é criar um notebook.
E o que é um notebook?
É a experiência de você escrever
dentro de um caderno.
Para quem nunca viu, isso é
excelente.
Você escolhe qual linguagem de programação
que você quer escrever.
Isso aqui é uma vantagem do
notebook,
você pode escrever em todas ao
mesmo tempo,
a gente vai ver aqui,
mas você pode escolher qual é
a primária, tá?
A primária é Python.
E aí você vai ter um
ambiente de notebook.
Olha que coisa linda.
Aqui você tem um ambiente onde
ele te mostra qual é o
notebook,
qual é a linguagem primária, Python,
tá vendo?
The full language.
E aqui você tem um ambiente
onde você pode documentar,
onde você pode trazer links,
onde você pode escrever código e
ter uma experiência.
E você pode versionar isso e
assim por diante.
Então, aqui a gente tem algumas
coisas como as informações do arquivo,
clonar, enfim, eu posso editar esses
arquivos,
eu posso ter várias visões diferentes,
eu posso rodar todas as células
ao mesmo tempo,
eu posso limpar os resultados.
Aqui eu posso ver quais são
os comandos que eu posso utilizar
para melhorar o meu processo.
Eu posso ter um ambiente colaborativo
onde eu posso...
Matheus, faz um favor para mim,
entra nesse notebook aqui,
para o pessoal ver que você
entrou.
Beleza.
Então, o Matheus pode entrar nesse
notebook aqui comigo,
ele tendo acesso, isso é uma
outra das coisas muito fodas,
ele pode colaborar comigo aqui nesse
processo.
Você tem a parte de experimentação
do MLflow,
que é vinculado para a engenharia,
para a ciência de dados.
Você pode já, por aqui, pedir
para agendar esse notebook,
ele já vai agendar para você,
para você executar de tempos em
tempos,
facilidade para ti.
E aqui você pode voltar no
histórico de atualizações que aconteceu dentro
dele.
Então, ele traz, cara, um ambiente
super legal.
E quando o Matheus entrar nesse
primeiro notebook aqui,
o que vai acontecer é que
o Matheus vai aparecer aqui,
a carinha dele, enfim, e a
gente pode colaborar junto aqui.
Então, vejam que o Databricks traz
muita coisa legal para facilitar o
seu dia a dia.
Então, ele faz, cara, com que
times possam colaborar,
times possam trabalhar.
Está aqui o Matheus, o Matheus
entrou aqui, está vendo?
Então, eu consigo saber onde o
Matheus está,
ele pode também fazer um comentário,
ele pode clicar aqui e, cara,
fazer um...
Olha lá, ele está aqui no
celular.
Então, eu consigo trabalhar juntamente com
ele, se eu quiser,
num ambiente colaborativo.
Inclusive, eu posso pedir para ele
fazer alguma coisa especificamente.
Então, eu posso chegar aqui,
select some code from notebook to
start commenting.
Então, eu posso pegar, por exemplo,
esse cara aqui
e eu posso vir aqui...
Eu posso comentar essa parte.
Esse history do notebook é provido
100 % pelo Databricks
ou utiliza uma conta do Git,
por exemplo?
Boa pergunta.
Eu posso integrar ao Git?
Hoje, na verdade, eu posso integrar
ele com, cara, a parada toda,
né?
Então, hoje eu posso adicionar no
Git?
Eu posso adicionar no Enterprise, no
GitLab.
Hoje eu posso fazer, cara, basicamente
tudo.
Então, eu já posso deixar totalmente
integrado
com o meu processo de CICD
e assim por diante.
Beleza?
Então, a primeira coisa que a
gente vai fazer é
o que é o Spark?
O Spark é um engine de
processamento de memória.
O Databricks foge disso?
Não.
O Databricks encapsula o Spark para
ficar fácil.
Qual é a primeira coisa que
a gente vai fazer?
Conectar no sistema de disco.
Quem é o sistema de disco?
Nesse caso, o nosso.
É o Data Lake.
E o meu Data Lake está
dentro de um Blob Storage,
dentro do Azure.
Dentro do Azure, eu tenho um
Data Lake, né?
Que é o Blob Storage.
Eu já falei do Data Lake
de Intuit, a diferença entre eles.
E quando eu clico aqui, eu
tenho as minhas pastinhas, ó.
Eu tenho uma pastinha 0 .1
.3 .7 .9 .4 .800 .1
.0 .6.
Isso acontece na vida real.
Eu tenho uma pastinha chamada Processing.
Eu tenho outra pastinha chamada Stream.
Eu tenho outra pastinha chamada SVC,
tá vendo?
Primeiro, eventualmente o seu ambiente vai
ficar fucked up, né?
É uma questão de tempo.
Isso vai acontecer.
Mas o lado bonito, geralmente uma
prática muito boa de Data Lake,
é você ter uma Landing Zone,
uma zona de pouso.
Essa zona de pouso, geralmente, ela
pode ser, cara,
ela pode ser organizada de diversas
maneiras.
Ela pode ser organizada pelo nome
da aplicação.
Ela pode ser organizada por dia,
enfim.
Mas qual é a coisa legal
disso tudo?
É que você pode organizar os
seus arquivos.
Isso aqui.
É um caso real, de vida
real.
Essa é a realidade.
A realidade é que as empresas
e que as aplicações que estão
conectando no seu Data Lake,
elas não vão escrever no melhor
formato para o Spark.
Eu ia falar disso aqui.
Elas vão escrever em formatos aleatórios.
Geralmente CSV, TXT e JSON.
É o que você vai ver
muito dentro de um Data Lake.
E você, eventualmente, vai precisar processar
essas informações.
Beleza?
Então, aqui eu tenho uma zona
de Landing Zone.
Eu tenho informações de usuário caindo
de tempo.
Em tempos, eles caem em pequenos
arquivos de 87 .71 Kbytes.
Lembrando que, qual é o problema
que eu tenho aqui já?
Aqui eu já consigo, de cara,
entender que eu vou ter Small
Files Problems.
Porque eu tenho vários arquivos.
Boa, velho.
Eu tenho vários arquivos pequenos e
a minha partição tem 128 MB.
Então, eu vou ter, cara, problemas
aqui.
Pode perguntar, tá?
Geralmente, você usa vários Storage Accounts.
São Storage Accounts com vários containers.
Boa pergunta.
Depende.
A gente usa vários containers dentro
do mesmo Storage Account.
Geralmente, quando você está dentro do
mesmo Inner Circle.
Geralmente, empresas que utilizam, por exemplo,
um departamento,
às vezes, elas optam em ter,
o quê?
Um Storage Account e lá dentro,
vários containers, tá?
Se você tem uma empresa que
é de pequeno, médio, porte,
geralmente, você tem um container.
Ou você tem três diferentes.
Você tem um de produção, um
de débito, um de homologação.
Então, as pessoas gostam de dividir.
E ter...
Esses ambientes iguais.
Ou gostam de colocar tudo isso.
Vai depender do seu gosto.
Não existe certo e errado.
Como eu gosto de trabalhar, eu
gosto muito de trabalhar com o
conceito de zoning, tá?
O conceito de zoning é, basicamente,
trazer a ideia de você conseguir
segregar pelo business
como você quer estruturar o seu
dado.
E aí, o conceito de zoning
segue a ideia de você ter
vários containers diferentes
para propósitos diferentes.
Então, olha só que legal esse
conceito de zoning.
Tá?
É como se você fizesse assim
ó.
Vê se faz sentido para vocês.
Deixa eu criar aqui o conceito
de zoning, porque eu acho que
é legal.
Já ouviu falar desse conceito, Matheus?
O zoning...
É assim.
Então, olha só.
Imagina que você tem uma empresa,
né?
E aí, a sua empresa tem
várias áreas ou vários departamentos.
Entendeu?
Então, aqui você tem Sales, aqui
você tem HR e aqui você
tem Finance.
Gente, vamos interagir comigo, por favor?
A animação, me ajuda aí.
Então, vamos lá.
Fez sentido aqui?
Três áreas distintas.
Vocês entendem que cada área dessa
tem processos, tem tecnologias e tem
características diferentes?
Então, o que a gente pode
fazer?
Uma das características de você utilizar
Zoning é para você poder garantir
o quê?
Que o seu time de Sales
ou Business Perspective possa, dentro da
sua área, estruturar as formas,
estruturar o dado da forma com
que ela ache melhor.
Olha que legal.
Isso é legal.
Você dá uma certa liberdade para
a zona conseguir configurar isso.
Próprios métodos de acesso, próprias regras
de permissionamento e assim por diante.
Então, o time de Sales vai
ver o Zoning de Sales.
O time de HR vai ver
o Zoning de HR, que eu
já vou explicar agora.
E o time de Finance vai
ver o Zoning de Finance.
Beleza?
E aqui dentro desses Zonings, a
gente vê a ideia de sandbox
um pouquinho diferente, José.
Então, aqui dentro você vai ter
o quê?
Os Accounts.
Então, o que a gente pode
fazer aqui?
Deixar.
Isso é uma das formas de
depenar o gato.
Não existe certo e errado.
Só é um conceito muito utilizado
fora do país.
Beleza?
Então, o que acontece aqui?
O que a gente vai fazer?
A gente vai, agora, falar como
essa granularidade funciona.
Então, geralmente, aqui você tem o
quê?
O conceito de Storage Account.
Então, você tem um Storage Account
para todo mundo aqui.
Beleza?
Então, é um Storage Account aqui.
Na verdade, um não, né?
Três.
Então, está aqui o Storage Account.
Beleza?
Então, você tem ali.
Como eu tenho...
Vamos pegar aqui.
Esse é o Storage Account.
Vamos supor que o Sales era
o quê?
O WSHQ Sales.
Aqui, HR.
E aqui, Finance.
Dentro de cada um deles, você
pode ter o quê?
Os Containers.
E aí que vem uma parte
muito legal agora.
Que é, beleza.
Você tem a estrutura individual.
Você tem todo esse processo acontecendo
em cada um deles.
Mas aí, a sua pergunta é
o seguinte.
Legal, Luan.
E quando eu quiser...
Que esses caras se comuniquem, Matheus.
E quando eu quiser com que
os meus Containers, eles se comuniquem.
O que que eu faço?
Legal, né?
Então...
Começa a confundir.
Então, isso aqui é legal.
O conceito de Zoning vai me
falar o seguinte, olha.
Entre eles...
Eles têm as próprias...
Eles têm as próprias...
Eles têm as próprias regras.
Eles têm os próprios acessos, enfim.
Eu vou ter um processo que
vai entregar esse dado para uma...
Isso aqui é foda.
A gente usa isso lá fora,
tá?
Lembra, não é só?
Então, na verdade, eu vou ter
uma ferramenta.
Uma ferramenta aqui.
Que vai jogar o dado.
Ou os dados que são pertinentes
para a minha zona de processamento.
Então, a zona de processamento...
Olha que coisa linda.
Isso aqui é foda.
Essa zona de processamento, ela está
completamente desacoplada.
Ela não tem acesso a ser
intrometida em nenhuma das contas,
os permissionamentos e características de cada
uma das áreas que você está...
trabalhando, que isso é a organização
trabalhando.
E aqui é o Data Lake,
ou é o ambiente onde o
Spark vai olhar.
E já tem o formato do
arquivo do jeitinho, bonitinho, que ele
quer acessar.
E aí, vocês já viram isso?
Olha que legal.
Então, essa é uma das formas
que você pode utilizar.
O conceito de zoning é basicamente
isso aqui.
Então, para resumir, você tem...
Mas isso não é algum tipo
de silo, pois todos os dados
estariam ali.
Na verdade, sim e não.
A gente endereça a quebra justamente
trazendo o que é importante.
Então, o que a gente faz
aqui?
Aqui você tem um Data Lake
para o Sales,
você tem um Data Lake para
o HR e um Data Lake
para o Finance.
E aí você tem um time
de Data Engineering
que vai saber exatamente o que
é necessário extrair desses Storage Accounts
e criar o processo para disponibilizar
na camada de processamento.
Para que você processe somente os
arquivos que são necessários de você
ser trabalhado.
Isso traz muitos benefícios em relação
à empresa como um todo.
Granularidade de acesso,
permissionamento de você conseguir, de novo,
dar acesso à sua equipe,
para você conseguir...
É, isso aqui é foda.
Isso aqui você não vê...
O conceito no Brasil...
Realmente, você não vai ver isso.
Isso aqui é a vida real
de empresas grandes, tá, gente?
Principalmente quando uma empresa é muito
grande.
Ah, vou centralizar tudo num Data
Lake.
Não, tio, não funciona assim.
Cara, você tem os seus Compliances
em cada área.
Você tem várias coisas ali dentro
de cada área.
Então, isso aqui é uma das
coisas que ajudam bastante
a gente trazer o conceito de
Data Lake, né?
Mas o conceito de Data Lake
como zoning.
Tá? Que a gente chama de
Data Lake Zoning.
Que são zonas, beleza?
Isso seria o Data Mesh.
Olha só que legal.
Isso seria exatamente ali.
Seria o conceito de você começar
a utilizar as informações de uma
forma desacoplada
e acoplada ao mesmo tempo.
Ou seja, eu desacoplo em relação
a ter cada permissionamento,
cada configuração em cada área,
e eu acoplo em conseguir trazer
para as ferramentas de processamento
o que é necessário elas terem.
O que é necessário elas fazerem
no melhor formato que elas estão.
Essa Lending Zone, Process Zone, é
um outro Storage Account?
É.
Aí aqui, por exemplo, a gente
tá falando de, cara, um Storage
Account novo, por exemplo.
Né?
Poderia falar de um outro Storage
Account.
Muito bom.
E aí, gente?
Vamos lá?
Já tinham visto sobre isso alguma
vez?
Na vida de vocês?
Faz sentido?
A perspectiva do que a gente
tá conversando esses dias é uma
das formas.
É uma forma legal de trabalhar
o dado de forma segura dentro
do Data Lake.
Tá?
Isso é novo, mano.
Isso é uma coisa bem legal.
Eu acho que vale a pena
vocês pensarem nisso, tá?
Isso traz muitos benefícios para empresas
de grande porte, principalmente.
Uma empresa muito grande.
Você chegar e falar, cara, a
gente pega o Jogador, a gente
a gente pega o Jogador, a
A gente coloca tudo aqui no
lugar.
E tal.
Não.
Dá acesso aqui.
Daqui, a gente vai ter um
team, um processo, que tem as
permissões de trazer essas informações
que estão ali dentro, para dentro
da sua zona de pouso.
Uma das vantagens aqui também é
que, na hora que você trouxer
essas informações aqui,
você pode deixar elas num new...
New...
Jogador.
Jogador.
você pode deixar no novo formato
para ser processado que é um
formato para Big Data
e essa ideia não tô me
sentindo burro é se você pagar
um treinamento caro desse eu não
te ensinar
nada e realmente tá fora né
então essa ideia mesmo vamos se
sentir burro para depois se sentir
inteligente depois se sentir burro de
novo tem alguma documentação que você
indica para quem
quer implementar cara
Data Lake zoning
Existem algumas coisas que você vai
ver mas assim vai ser mais
mesmo eu não tenho nada assim
basicamente para te recomendar não tá
e no demais tem
por exemplo story de account de
sales posso ter todas as apólices
vendidas e nos demais
que pode ser de produtos específicos
beleza
pegou a ideia, tá? Ou seja,
você tá acoplando e desacoplando ao
mesmo tempo
sem ser intrusivo demais pra cada
área
do seu departamento, da sua empresa.
Então, isso funciona muito legal,
de novo, pra empresas de grande
porte
e pra médias grandes.
Às vezes, uma empresa pequena,
o processo como um todo faz,
você pode ter só um storage
account
e dividir aquilo ali em pastas.
Eu acho que, Marcelo,
gastei aqui 15 minutos.
Ficou top? Ficou claro?
É isso aí? Fez sentido? Isso
é uma outra versão.
Do que você pode fazer também.
100 % é o meu, não.
Vamos lá.
Legal. Então, a primeira coisa que
eu vou fazer
é conectar o que?
O Databricks com o
Blob Storage.
De novo, tem algumas
formas de você fazer isso. Existe
a
forma de um engenheiro de dados
júnior, não desmerecendo,
quem faz assim, até porque pra
você ser um
sênior, você tem que ter passado
a ser um júnior,
né? Você pode simplesmente
pegar o segredo,
o segredo do Blob Storage e
socar
aqui no notebook e ter esse
segredo, ou você pode criar
um secret, né?
Que é um segredo. Então, dentro
do Databricks, você tem a possibilidade
de ter um secret,
ou um key vault, tá?
Que é basicamente o quê? Pegar
o segredo
que é. O que é o
segredo?
Pra você conectar com o Blob
Storage, você
precisa saber qual é o access
key.
E você precisa socar
isso no Databricks pra você abstrair
a leitura desse dado pelo Databricks
File System, que eu vou explicar
agora.
Então, a primeira coisa que você
vai fazer
no Databricks, a primeira coisa é
meu dado está dentro do Spark?
Não. O Spark é um engine
de processamento.
Meu dado tá dentro da onde?
Do Data Lake.
Então, eu tenho que conectar no
Data Lake.
Beleza.
Então, o que que eu vou
fazer? Eu vou
criar o
mount point. De novo, eu falei
ontem
do mount point, né? O mount
point
faz o quê? É uma
camadinha que fica entre o Data
Lake
ou o Storage System que você
quer acessar
e o Databricks.
Isso é Databricks, tá? E aí
você acessa o mount point que
tá aqui.
Então, a gente vai criar os
mount points.
Então, olha o que que eu
vou falar. Olha, cria um
mount point pra mim que vai
estar montado no Databricks
barra MNT barra Lending.
Que, na verdade, se remete
a esse endereço do Data Lake.
Lending o WSHQ
Blob STG. Então,
o que basicamente eu tô falando
pro Databricks é que ele tá
olhando
pra aqui, ó. Lending. Ele tá
olhando
pra isso aqui. E assim, por
diante. Então, eu vou montar
tanto a Lending
Zone, a Processing Zone,
a Curated Zone, a Stage
Zone, Production Zone,
a Delta Architecture ou a
Medallion Zone e assim por diante.
E aqui eu posso listar
as zonas que eu criei
que estão relacionadas ao Databricks.
Então, na verdade, quando
eu faço uma consulta aqui
barra MNT,
eu não tô acessando o dado
que tá dentro do Databricks.
Eu tô passando pelo Databricks,
até o System e acessando
o Data Lake lá, ó. Então,
na verdade,
eu tô acessando o Data Lake,
ó.
Mas aí ele vai aparecer o
quê?
DBFS. Por quê? Porque eu configurei
o Mount Point utilizando
o utilitário do Databricks. Tá
correto, gente? Dá aí. Tá correto.
Esses detalhes vão estar no treinamento
de
Databricks, mastigado
cada bit e byte desenhado.
Então, por isso que eu não
vou ficar gastando
tempo com isso, porque é um
processo
que vai estar dentro do
Databricks.
Beleza?
E aí? Deixa claro
pra mim aí. Tá claro?
O permissionamento não fica internamente no
Secret, exatamente. Sim.
Perfeito.
Tá? E aí você pode criar
um Secret.
Tá vendo esses notebooks aqui que
vocês estão vendo?
Vocês vão ter acesso a todos
eles, tá?
Então, na verdade,
aqui eu explico em detalhes, ó.
Você vai acessar o Databricks Command
Line
Interface e aqui, ó,
ele te ensina como criar
qual.
Ele te ensina
o Docs. Ah, é porque tem
um Docs.
Não, tá certo. Caraca, o Docs
da Microsoft
tá fora. Show!
Que legal!
É isso mesmo? É isso mesmo.
Tá fora. Vamos ver se o
Docs
do Databricks tá off. Não, não
está. Então, olha só.
Então, aqui você vai ter todo
o processo
de configuração, tá? Você pode passar.
Então, é um pattern, gente.
Primeiro pattern de Databricks,
conectar na fonte. É isso aqui,
tá?
Ah, mas você tá me mostrando
só Data Lake. Calma que ainda
não acabei.
Vou mostrar várias coisas aqui.
Beleza. Gente,
primeiro ponto é isso. Segundo
ponto, mais importante que esse, é
o seguinte.
Entender que
existem dois tipos de storage
dentro do
Data Lake, dentro do
Azure hoje, tá? Então,
hoje, se a gente for trabalhar
com Big Data,
você pode, na hora que você
cria
um block storage, um storage,
você vai fazer o que?
Vai configurar e escolher em qual
protocolo você vai acessar.
Se você vai acessar o protocolo
WASB, que é o block storage,
ou se você vai acessar o
protocolo
pelo ADLS2
ou o ABFSS, que é
um protocolo criado
pela Microsoft, e eu digo pra
você,
o protocolo mais rápido pra trabalhar
com
dados que existem no mercado.
A Google não bate,
a S3 não bate,
ninguém bate esse protocolo.
O protocolo mais rápido de acesso,
de dado, de arquivo,
é o ABFSS.
Então, a primeira coisa que você
vai falar é o seguinte,
cara, tô trabalhando no Azure, é
ABFSS, é Azure Data Lake Storage
Gentil. Como que eu faço isso?
Quando você vai criar o seu
block storage,
quando você vai chegar na nuvem
e vai
criar o seu block storage aqui,
né, o seu container account,
o seu account storage, ele vai
falar
pra você o seguinte, ó, eu
quero
Cadê?
Storage account.
Eu vou criar o storage,
e aí eu vou falar pra
ele
durante a criação, se eu quero
que ele seja hierárquico, ou seja,
Data Lake Storage Gentil,
e aí eu marco ele
pra ser Gentil. A partir
disso, tá, depois de dois anos
de inovação,
uma flag, você tem um protocolo
mais rápido pro mercado. Luan,
por que que você fala que
é o protocolo mais
rápido do mercado? Porque eu trabalho
com as três nuvens, eu faço
o teste nas
três nuvens, e
além disso, não só eu, pode
no mercado pesquisar, comparação entre
eles, o protocolo mais rápido hoje
no mercado pra trabalhar, com isso
que a gente tá fazendo aqui,
é o ABFSS.
Tá indiscutivelmente, eu vou mostrar aqui,
inclusive. Se eu desejo implementar o
Delta Lake e preciso armazenar todos
os dados
brutos que chegam na mente, qual
seria a melhor opção?
Vou chegar lá, relaxa.
Vamos por partes aqui que a
gente chega
nesse processo do pipeline. Vamos ver
algumas coisas aqui. Então, ó,
eu posso, eu vou aumentar aqui
pra
ficar bem claro pra vocês, tá?
Todas essas configurações aqui, eu vou
explicar a maioria delas, mas muitas
você
vai abrir ali o notebook, você
vai ler, vai passar, tá tudo
muito documentado
pra vocês entenderem, tá?
Então, eu posso falar pro Spark
o seguinte, ei, Spark, leia
JSON, lembra que a gente viu
ontem?
Spark, leia JSON, e aí eu
passo o seguinte, prestem atenção, o
ASB
e o local
aonde ele tá.
Vejam aqui uma coisa muito importante
que eu quero deixar claro aqui.
Eu estou acessando o Databricks
File System? Não, eu tô indo
direto
no Blob Storage.
Por aqui. Ah, Luan, então eu
posso
fazer isso sem os mountpoints? Pode.
O que que eu perco aqui?
Cara, você perde uma camada de...
Isso é melhor prática, isso é
coisa
que você tem que entender pra
elevar
o nível do entendimento de Spark.
Principalmente com o Databricks.
O que que você perde aqui?
Você perde muito.
Por quê? Porque se você tem
um sistema de
abstração que até melhora o acesso
e faz
com que você encapsule,
se você tira isso e se
por algum
momento você vai migrar pra um
outro
local, esse arquivo vai mudar de
posição,
enfim, o seu código tá
acertado, esse local
o que é uma merda, porque
pode mudar
e muda, tá? Bastante.
Então, um novo sistema de gestão
entrou,
um novo processo entrou, enfim, você
roda o código
e o código quebra, tá? Por
quê? Porque ele
tá apontando especificamente pro local
no storage onde ele tá. Isso
pode mudar.
Isso é transiente. Se você bota
o Databricks File System pra poder
liderar
isso, você pode simplesmente mudar
e não vai quebrar o seu
código.
Você não tem major breaking changes.
Beleza? Então, presta
atenção. Quando eu executei esse código,
tá? Eu
demorei 17 segundos
pra fazer isso, mas eu queria
mostrar
pra vocês a diferença de você
acessar pelo protocolo
WASBIS e acessar
pelo protocolo ABFSS.
Eu demorei
aqui 2 .6
minutos pra fazer
o acesso desse arquivo de 4
gigas e demorei
o mesmo arquivo,
só que acessando pelo
protocolo ABFSS,
eu demorei 19
segundos.
Vocês estão entendendo o que eu
tô falando?
Mesmo
arquivo, um com
storage account,
outro com data lake
storage Gentoo habilitado.
Eu mudei o driver de
protocolo. É,
isso é foda. Vocês podem usar
foda
aqui de boa, só quero ver
se vocês estão animados
mesmo, porque são 10 horas e
a gente tem
que estar animado, né? Principalmente pro
demo,
demo é massa pra cacete.
Demo ainda mais ao vivo,
é muito doido. É, isso é
foda,
tá? É. Então, assim, é o
mesmo
arquivo, não mudou nada. Eu só
cheguei no Azure e botei
Azure pro Observe. Eu não achava
que
tinha muita diferença. Não, tem não.
Só, quantos por cento, Matheus?
De 2 minutos pra 19,
2, 4, 6, 8,
2, 4, 6,
3, 6,
6 vezes, tá?
Isso é o custo.
É, tem essa mesma diferença, é
o
mesmo custo, tá?
Então, você só mudou o protocolo
e ganhou 6 vezes mais de
velocidade, tá?
Tem limitação? Tem, você não usar.
Não, não tem limitação,
tá?
Literalmente é ilimitado.
Então, se você
tá trabalhando com
Databricks, vai trabalhar
com Databricks e não usa esse
protocolo, se você botar isso,
você já vai ganhar velocidade para
a caceta.
Então, existem várias formas de você
setar
isso aqui. Você seta na hora
que você
tá setando, como eu mostrei ali,
quando você tá setando os
protocolos de acesso.
Então, quando você tá fazendo Mountpoint,
em vez de
você fazer o Asbis, você vai
fazer
o acesso a BFSS e vai
montar como a BFSS.
Beleza? Ou você pode
acessar direto, mas eu expliquei
pra gente não acessar direto e
deixei aqui
como que você faz pra configurar,
tá?
Tá aqui a configuração.
A configuração que você faz pra
ter
esse diretório habilitado.
Fechou? Habilitei o
diretório, Luan. Agora eu tenho
realmente o acesso ao dado.
Qual é o primeiro ponto que
eu vou
fazer? Bem, na nossa arquitetura,
a gente vai trabalhar
o processo de construção
de um pipeline fim a fim
agora. Então, vamos lá? Primeiro
ponto que eu vou fazer, a
gente vai
fazer live coding aqui, tá?
A gente vai listar os dados
que
estão no MNT,
que na verdade
tá entrando, não se enganem,
tá indo no blog storage,
na landing zone
e tá olhando isso aqui.
Então, eu tô indo lá e
verificando
os arquivos que eu tenho, tá
vendo?
Ó, os arquivos que eu tenho
lá.
Então, aqui, listei o que tem
dentro
de bank, listei o que tem
dentro de credit card, listei
o que tem dentro de stripe,
listei o que tem dentro de
devices e assim
por diante, tá?
Gente, qual é a primeira coisa
que a gente faz no Spark?
Carregar o dado pro Spark,
né? A gente vai carregar
essa informação pro Spark.
Só uma coisa aqui,
rapidinho.
Que não é,
deixa eu ver se é isso
que eu quero
mostrar, é isso que eu quero
mostrar?
Só um minutinho.
Três, dois,
é, é isso que eu quero
mostrar.
Tá?
Deixa eu,
deixa eu ter certeza se é
essa sequência
que eu quero mostrar mesmo.
É, é isso, tá?
Então, vamos lá.
Então, eu vou carregar os dados.
Qual o primeiro processo?
Isso, Juliana, perfeito, os data frames.
Alguém coloca aqui pra mim, pra
ver se
vocês lembram do que eu falei.
O que que é um
data frame? Vamos ver se vocês
estão
realmente comitando a
informação na cachola.
O que que é um data
frame?
Vamos lá, gente. Todo mundo,
todo mundo participando, escrevam,
desacoplem, faz o flush aí pra
vocês comitarem essa informação realmente.
O que que é?
Opa, perfeito, uma tabela estruturada
em memória, distribuída,
boa, isso mesmo. Então, ela tá
distribuída
entre os workers, entre os
executors, né? Então, ela tá lá.
O que que eu tô fazendo
aqui? Eu tô falando
o seguinte, Spark, Leia,
JSON, tá vendo que não mudou?
É o mesmo código que tava
lá embaixo?
É o mesmo código que tá
aqui em cima? É a mesma
coisa.
Spark, Leia, JSON.
Passeio local. Bank,
asterisco, JSON. A gente já viu
isso ontem.
O que que ele vai fazer?
Ele vai entrar
dentro aqui do
DF Bank, tem como paralisar
essa leitura? Essa leitura está
sendo paralelizada automaticamente.
Tá, Velke?
Então, tudo, e é aí
que a parada é foda, porque
tudo que tá
acontecendo aqui é paralelizado,
mas pra você é transparente.
Tá? Tudo aqui que tá acontecendo
é paralelizado. Então,
o que que você tá
falando, Velke? Que o comando é
serializado?
Sim,
serializado o processo
de execução, mas quando ele
vai aqui, essa leitura
é distribuída, essa leitura é distribuída,
essa leitura é distribuída e assim
por diante. Ele vai executar linha
a linha,
só que cada processo que ele
tá
executando é distribuído,
beleza? Ele vai seguir um fluxo.
Se você quiser fazer
isso de uma forma diferente,
se você quiser paralelizar tudo isso,
você poderia ter várias, você poderia
dependendo disso, funciona, tá?
Você tem quatro aplicações Spark
que uma lê Bank,
uma lê Credit Card, uma lê
Stripe
e uma lê Device distribuída.
Aí você tem uma aplicação, uma
sessão
pra cada uma delas. Geralmente,
você coloca todos os data frames
em um único notebook
ou notebook para cada sócio? Boa
pergunta.
Geralmente, um pipeline,
um processo de pipeline
ou um processo como um todo
dentro
de um notebook, tá? Geralmente.
Então, geralmente, um notebook é um
processo
por isso que eu falo. O
notebook, ele é um
processo que é pra ser usado
pra exploração,
pra mangem de dados, pra descoberta
e pra você modelar
o seu processo de ETL. Após
isso,
você estrutura ele no notebook
ou dependendo das melhores práticas,
você quebra ele em processos. Então,
ele é o ponto inicial pra
você descobrir como
você vai quebrar isso, tá? Boa
pergunta, Marcelo.
Então, o que que eu vou
fazer aqui?
Eu vou lá em Bank
e eu vou simplesmente
ler todos esses arquivos.
Luan, como que ele vai ler
esses arquivos?
De forma distribuída. Como, Luan?
Ele vai entrar aqui, vai listar
a quantidade de arquivos que ele
tem,
vai ver a quantidade de executores
que ele tem, vai carregar,
vai distribuir isso entre os
executores e vai carregar isso dentro
das
partições. É assim que ele vai
fazer distribuidamente
esse acesso, tá?
Beleza. Quantos
registros eu tenho aqui? Aqui ele
vai contar
o último, né? Tanto faz aqui.
Só se eu botar um print
em cada um.
Luan, vai perguntando.
Vai perguntando porque é normal mesmo.
Vamos tirando
a dúvida que só vai evoluindo
aqui.
Luan, ao ler os arquivos
ele traz todo o dado para
memória ou neste
primeiro momento ele só traz o
esquema?
Ótima pergunta.
Fucking gut. Tá? Você viu que
aqui ele demorou 25 segundos, né?
Então a gente vai entender daqui
a pouco ali
que o Spark, ele é preguiçoso.
Ele é laser evaluation. Eu recomendo
demais, tá? Eu vou até
deixar aqui. Isso foi
uma série de artigos
que a gente, que eu gravei,
uma série de vídeos que eu
gravei aqui, ó.
Nessa playlist
que eu vou deixar aqui pra
vocês
que eu explico, cara,
todos os processos do que são
partições, do que é
stage, do que são transformações, do
que é
laser evaluation e tal. Então eu
acho
que é super, super válido
isso pra vocês. Gravem aí
que
vale a pena vocês olharem.
Tá? Então, na verdade
o Spark
como um todo, ele é laser
evaluator.
Ele é laser evaluation. Então o
que ele faz?
Ele só vai
processar quando ele tiver
necessidade de processar. Ou seja,
tem comandos de ações e tem
comandos de transformações. Eu vou explicar
daqui a pouco isso, tá? Quem
faz essa
distribuição dos arquivos entre os workers
é o
driver? Perfeito, Douglas. Então
o driver entende, a cabeça
entende e manda distribuir
entre eles e quem executa esse
cargo. Perfeito.
Tá? Então aqui, ó. Dentro de
um
data frame desse, 55 .900.
Bem,
vejam que eu trouxe o dado
JSON para dentro da memória. Ele
tá
dentro da memória agora. Ele tá
dentro de um data
frame. Beleza? E aqui
automaticamente eu pedi pra ele me
dar o que? O esquema
daquele data frame, tá vendo?
Só que a gente vai ver
que em alguns
a gente vai ter nesting. Eu
vou explicar
pra vocês como a gente vai
acessar.
Gente, vocês não falaram foda ainda,
né?
Vocês não perceberam isso aqui. Alguém
lê pra mim
aqui? Paispark .sql .dataframe.
Aonde o Paispark tá? Dentro de
SQL.
Lindo, né?
Olha aí a comprovação.
Pra quem não viu, olha
que detalhe, né?
Olha que detalhe bonitinho, gente.
Olha que detalhe. Foda, né? É,
isso aqui
quando a gente entende no treinamento,
é realmente
sair gritando louco, pelado
na rua, três horas da manhã
ou mais.
Porque tá dentro
do SQL. Eu vou mostrar pra
vocês, ó. Vocês já estão
vendo que, caraca, tá dentro do
SQL. Não acredito,
mano, que o cara processa terabyte
dentro
do SQL. Que louco. A vida
é louca mesmo, realmente. Beleza?
O que que geralmente a gente
faz quando a gente
não tá no Lakehouse?
Tá? Vamos lá. Antes
Lakehouse. Um pipeline antes
do Lakehouse. O que a gente
fazia
pra trazer pra landing zone,
né? Ou pra zona que você
vai processar,
sempre o formato
de arquivo parquê. Por que, Luan?
Porque o Spark escolheu
o parquê para ser o
match made in heaven, pra ser
os carinhas
que conversam felizmente dentro
do pipeline. Por que,
Luan? Por alguns motivos.
Por exemplo, estruturas complexas,
eficiência na compressão,
trabalha com formato colunar, assim como
na memória, menos overhead
de I .O., melhoria
de scanning de dados,
utilização melhor de threads e assim
por
diante. Então, o que a gente
recomenda é o que?
Cara, eu recebo o dado
do Data Lake, antes do
Data Lakehouse. Eu carrego
pro Spark e eu
escrevo em parquê em um outro
local pra depois começar a
processar. Se você fizer isso,
só você habilitando essa melhor
prática antes do Lakehouse, você vai
ganhar em torno de 3 a
5 vezes
mais velocidade no seu pipeline.
Então, se o seu pipeline processar
1 hora, ele pode chegar a
processar em alguns
minutos. Essa é uma melhor
prática. Por que? Nós não
processamos arquivos que não são
arquivos otimizados para
Big Data com Spark. Por que?
Porque
não é a melhor prática, porque
você
vai ter utilização errônea
de acesso a arquivo,
você vai ter overhead de
acesso, você vai ter várias coisas,
que você pode ganhar por um
formato
de arquivo. Existem vários,
ORC, parquê,
Avro e assim por diante. O
que funciona
melhor para o Spark é o
parquê.
Beleza?
E eu vou mostrar pra vocês
o quão é fácil escrever.
A gente viu ontem ler, né?
Spark leia. Agora a gente vai
falar o que? Spark escreva,
né? Então, assim,
pego o DataFrame e falo
DataFrame, escreva
qual o modo que você quer.
Você quer fazer o append, quer?
Só colocar? Você quer fazer o
override?
Você quer substituir tudo que tava
lá no arquivo?
Ah, eu quero fazer o
append, quero continuar colocando
dados novos ali dentro. Então
eu vou fazer o que? Um
append do dado
e eu vou escrever
dentro do que?
MNT Processing Parquê
Batch Banking. Então vamos lá. Ele
tá
escrevendo no Databricks? Não, ele tá
escrevendo no Storage. Então vamos
voltar aqui.
Não é Landing Zone, é
MNT Processing, se eu não tô
enganado, é Processing
Batch, não
Parquê, Batch
qual que eu tô escrevendo?
Bank.
E aí eu vou ver que
existe, talvez,
aqui, um cara que tá escrevendo
no dia 16 do 8
algumas coisas. Então eu tenho
esses caras entrando. Beleza?
Podemos fazer isso
com outras ferramentas? Boa pergunta,
Juliano.
Pode, claro. Você pode fazer com
Jupyter,
Jupyter,
Notebook.
Você pode fazer com a
parte Zeppelin,
que também é bem legal.
OpenSource.
Aí você pode fazer com
Zeppelin, que é OpenSource,
que é o Notebook, que traz
a mesma
ideia do Notebook.
Inclusive eu vou deixar ele aperto
aqui
pra fazer uma coisa com ele.
Jupyter
Notebook, que também você pode
conectar e fazer, tá? Então
sim, a resposta é você pode,
com
certeza, tá?
Não sei se eu tô falando
no momento, não. Pra entrega
no final, daí seria uma boa
gravar em
o RC, pois eu vi, por
exemplo, que no Trino
é mais
rápido constar no RC. Não,
você tá falando besteira não, é
isso mesmo. Então a gente vai
ver lá
daqui. É, mas essa é a
ideia. Chega
de um lado, coloca do outro,
do melhor que a outra
ferramenta vai realmente
consumir. Exemplo, usando
HDFS do hardware de processamento com
Jupyter? Sim. Saber o Delta
salvo em Parquet e compressão em
Snap?
Não é o Delta, é o
Parquet
salvo em Snap, isso mesmo.
Se eu entregar esse dado nesse
formato pra ser
processado usando esse codec, tem algum
ganho
de performance ou não?
Então, Júlio, a gente vai
ver que, na verdade, o Delta,
ele é uma camada
inteligente em cima do próprio
arquivo que já existe, que é
Parquet. Então,
por exemplo, quando eu olho aqui
e eu acesso uma pasta Parquet,
tá vendo? Eu tenho
os commits, o started e os
arquivos
em Snap, que é o padrão
de compressão
que ele usa, tá? Tá em
Snap.
Beleza.
Legal.
Quando eu utilizo o Lakehouse,
que agora a gente vai ver.
Tá, Luan,
mas agora eu quero utilizar o
conceito de Lakehouse,
eu não uso mais esses
outdated pipelines, pipelines que não
utilizam Lakehouse. Como eu faço
pra carregar pro Lakehouse? Então, vamos
ver como que é difícil. Então,
de novo,
arquitetura, Delta
ou a medalha, tá? O que
que eu vou
fazer? Eu vou pegar o data
frame
que tava lá em memória e
eu
vou jogar numa tabela Delta.
É isso. Uai, Luan, então, em
vez
de dar formato,
Parquet é formato Delta?
E aí eu já tô no
Lakehouse?
É, é isso.
Você tá falando sério, né?
Você vai escrever
no formato Delta
dentro de
OWS Delta.
Ó lá.
Então, vamos lá. Ah, não, você
tá zoando.
OWS HQ
MNT
Não precisa importar nada, exatamente.
Não. No Databricks, não. Se você
fizer isso no Open Source, você
tem
que fazer o download dos
Jars. Eu vou mostrar pra vocês
durante a semana.
Colocar dentro da pasta, carregar
e fazer. Beleza? Então, olha só
que legal. OWS HQ, Delta,
Batch,
Bronze,
Bank.
Olha só o que que apareceu
aqui agora.
O...
Quem foi que pegou?
Júnior. Delta Log.
Se eu abrir esse Delta Log,
Esse Delta Log, eu vou ver
algumas informações.
Uma das informações que eu vou
ver
é as informações de metadado.
Então, Delta, ele é
um cara em cima do parquê,
que ele salva em parquê.
Ó lá, parquê. Mesmo jeito, tá
vendo?
Mudou nada.
Só que ele traz uma camada
semântica,
uma camada inteligente em cima.
Ou seja, é um formato de
arquivo,
como parquê, enfim, é só uma
inteligência
semântica em cima disso.
O modo apende -se ao executar
mais de uma vez o mesmo
processo,
de gravação, ele sobrepõe ou duplica
o dado.
Apende -se. Você tá falando o
seguinte,
olha, escreve. Se você pedir 60
vezes,
ele vai ficar fazendo apende -se
do mesmo
que tá na memória e vai
ficar
colocando pra você, tá?
Compensa mudar de Snap para algum
formato de compressão
que gera um parquê menor antes
do Data Lakehouse
pra economizar espaço? Não.
99 % dos casos,
eu recomendo você utilizar Snap e
não
sair desse processo. Mas existem alguns
casos que talvez, dependendo
do que você usa, dependendo do
tipo
de carga que você vai fazer
pra uma ferramenta,
uma ferramenta externa, vale a pena
mudar
o nível. Mas, cara, é
algo muito, muito, muito
edgy, tá? Que aí tem que
ser analisado.
Não precisa importar nada? Não.
Foda, show. E qual o controle
para os
arquivos que você já carregou no
Delta?
A gente vai ver os controles,
tá? Existem algumas
informações aqui que ele entrega.
Que a gente vai ver ao
longo aqui do
processo. Ou seja,
qual é a recomendação? O dado
caiu no Data Lake e eu
vou trazer
pra uma tabela. Quando eu escrevo,
eu escrevo Delta, gente, ele vira
uma
tabela. Beleza?
Aqui, quando eu escrevo em Parquet,
ele não é uma tabela Parquet,
ele é
um arquivo Parquet. Quando
eu escrevo em Delta, ou seja,
quando eu falei Delta, ele é
uma
tabela estruturada.
Alguém quer colocar pra mim? Prova
isso.
Prove me. Já foi aqueles cartazinhos
tipo, Delta é foda, prove me
along. Se alguém quiser
que eu prove isso, coloca aí
que eu
provo agora na sua cara, tá?
Qual a diferença na prática? Tipo
isso.
Quero escutar a palavra, me prova.
Não, não pode acreditar
em mim não, galera.
Talvez eu esteja mentindo.
Tem que contestar, oi. Contesta?
Tô brincando, não. Não precisa
contestar, não, mas me prova
que o seu cabeça dura aí,
ó. Gosta aí. Então, olha
só. E eu gosto disso.
Pergunta, duvido. O cara bota
encaixado. Duvido, caralho.
Vamos lá, então. Pra vocês entenderem,
porque vocês vão sair daqui, velho,
entendendo
uma parada, tá?
Fica tranquilo, Vinícius. Eu vou
ensinar até sexta -feira, tá? Que
desafio.
Então, ó. Vamos chegar
aqui e
olhar
o... A gente
já fez isso, mas a gente
vai fazer de novo.
Então, a gente vai chegar aqui.
Landing. Desculpa.
Muita coisa. Processing.
Isso. Parque
Pet Bank.
Tá vendo que eu tenho arquivos
simples aqui? Ou seja, eu tenho
literalmente
os arquivos de commit que o
parque utiliza, mas ele é um
formato
de arquivo. Ele é um arquivo
que
tá ali dentro do Data Lake
e que você acessa esse cara.
O que que ele tem aqui
dentro? Por exemplo, se eu
pegar esse arquivo de committed
e eu fizer o download
desse arquivo,
quando eu abrir ele,
eu tenho basicamente...
Vocês não tão vendo, né?
Não tá vendo, né, Matheus?
Não, tá vendo só a tela
do
Web Storage.
Tem problema não, porque eu vou...
Eu vou resolver isso agora.
Tá? Ah, então...
É que eu já vou deixar
aqui pra vocês poderem ver
o RemiRemi.
Então, o que que acontece?
Quando vocês abrirem esse arquivo
que a gente fez o download
logo agora,
o que que vocês vão ver?
Vocês vão ver isso aqui, ó.
Então, eu vou parar de compartilhar.
Que bom que vocês duvidaram de
mim, porque isso é um teste
legal. Ó, o que que vocês
vão ver.
Vocês vão ver simplesmente
um arquivo de informação do
parquê dizendo o seguinte. Olha,
esse arquivo foi dividido
nessas seguintes partes e eles
contém esses arquivos que fazem
referências a ele. Beleza?
Ou seja, ainda é um arquivo.
Só que, olha só que foda
quando a gente pega
o arquivo parquê, o arquivo
delta, né? Ou seja, a pasta
delta.
Então, dentro da pasta delta,
se eu vier aqui em
OWSKQ
delta,
Batch,
Bronze, eu sei que vocês não
estão vendo ainda.
Bank.
Dentro do Banking
tem o Delta Log.
Eu vou mostrar pra ele, porque
ele pode falar que eu não
tô mostrando tudo.
Eu vou chamar de mentiroso, né?
Aqui, ó. Bronze.
Agora, como eu escrevi em Delta,
cadê aqueles arquivos de
committed, enfim? Tá vendo que eles
não estão mais
aqui? Só tem os arquivos
parquê, teoricamente,
aqui, que o dado tá aqui
dentro.
As partes do parquê dividida, tá
vendo?
Olha o tanto de arquivo. Onde
tá a estrutura
lógica, inteligente? Agora é a hora
do foda,
podem usar e eu deixo, tá?
Fica tranquilo que hoje a gente
tá bem generoso no foda.
Cara, tem vários foda que vocês
vão usar até
10 e... Eu tô aumentando um
pouquinho
mais, mas quem tá aqui quer
ver, né, o negócio
realmente tocar. Lembra que eu falei
do Time Travel? Olha só que
legal.
Zero, um, dois, três.
O que que é isso? A
cada vez que você escreve
no Delta, que você modifica
ele, ele vai te dar um
arquivo onde ele sabe o
posicionamento das mudanças e o que
aconteceu.
Então, se eu baixar esse JSON
aqui,
caraca, isso aqui é
muito foda quando você vê pela
primeira vez.
Olha só o que que vocês
vão achar aqui,
dentro desse JSON, gente.
Tá preparado, Matheus?
Olha aí, ó.
Olha aí que coisa
linda de se ver.
JSON formater. Deixa eu
formatar esse JSON aqui pra vocês.
Deixa ele certinho.
É, pra galera passar mal, né?
Porque a ideia é o cara
passar mal, né?
O cara não passar mal não
tá legal.
Então, olha só que legal.
Informações de commit,
notebook, cluster,
nível de serialização,
métricas de
operação, puta, dá vontade de chorar.
Quantidade de arquivos, número de bytes
entrados,
número de bytes saídos, meta dado
e... O que que é isso
aqui pra mim?
Lê pra mim, Matheus.
Você consegue ler?
Não sei, o esquema string.
Só o esquema.
Olha o esquema da tabela
que foda, ou seja, ele estruturou
o teu dado, mano.
Tá? Então,
te disse quais são os arquivos,
qual a referência de cada um.
Na hora que você faz a
query, ele entra nesse arquivo
e fala, opa, pelo min value,
pelas informações de
metadado, eu sei aonde tá aquele
dado.
Então, ele vai usar isso pra
se guiar
na hora de fazer a query.
A query em cima deles.
Resposta.
O que você geralmente faz com
os
corrupted files, você não vai
ter dado corrompido aqui dentro,
porque ele é atômico.
Ou vai, ou não.
O que pode acontecer é,
dependendo de como você
deu o seu acesso, por isso
que o zoning é foda,
se um animal de teta chegar
aqui
dentro do seu data lake e
excluir,
aí você corrompeu, né?
Aí você vai ter problemas de
como você vai reverter isso.
Mas, dado ao
ciclo normal, Marcelo, você não vai
ter
problema, é como um banco de
dados
relacionado, tá?
E aí, foda?
Digo quando o source manda um
formato diferente.
Quando o source manda um formato
diferente, você tá entendendo
o quê? Que o source é
o Spark.
Então, na verdade,
ele tá dentro de um data
frame. Quando ele for
escrever, se você tiver um
esquema diferente, você pode habilitar
a evolução desse esquema. Automaticamente
ele faz isso. Então, se você
tiver
um novo cara, ele habilita
isso. Você que configura o histórico,
o histórico
já é configurado por padrão.
Ele tem um tempo de retenção,
você pode
realmente escolher.
O campo boolean e o animal
manda
desconhecido. Ah, boa.
O campo é boolean, o animal
manda desconhecido.
O que vai acontecer numa tabela
de banco de dados
relacional, Matheus?
Não acho que deixa, né?
Não escreve.
Olha que lindo. Então, imagina
que você tem um banco de
dados
relacional num sistema distributivo,
fluído. É isso. Na prática, eu
tenho
umas doze parcelas de uma pólice.
Todas as doze estão pendentes.
Em um, paguei a primeira. No
Time to Travel,
a versão que vou ler, a
onze parcelas,
uma paga. Isso mesmo. Caso eu
veja a versão anterior,
eu terei as doze parcelas pendentes.
Perfeito.
Principalmente no seu caso aí, é
foda
uma esteira com change de era
finta.
Você vai consumir sempre o último
que mudou.
Os arquivos parqueiros são gravados
delta, terão 128 megas?
Não. Boa pergunta.
Ele vai tentar escrever
em 128 megas, mas se ele
não tem
128 megas na memória, ele vai
escrever
o que dá pra escrever, né?
Essa é uma boa pergunta. Você
tá vendo a minha tela, Matheus?
Tô vendo.
Mas tá vendo o que?
O do Data Lake? Não, tá
no
álbum. Ah, então,
vamos lá.
Eu amo
esse monitor, mas...
Esse aí é o
PetPip. Esse é o PetPip.
Então, vamos lá.
Eu gosto quando a galera começa
a explodir
de pergunta, mas se
você identifica esses registros que não
foram
processados, aí vai ser a sua
lógica
que vai dizer os
registros. Sim, você pode falar o
que é mal
formido, você pode configurar o que
vai. Até porque
quando você tentar escrever dentro do
data frame
lá, ele vai explodir um erro,
como explode
num banco de dados relacionado. Olha,
coluna
tal, tá tentando escrever tal e
não conseguiu.
É atômico, tá?
É possível gravar arquivos delta nomeados
ou
particionados por coluna? É, na sexta
-feira
já veio, isso é melhor prática,
isso já é mais avançado,
beleza? O que que acontece aqui,
ó?
Tá vendo que os meus
tamanhos estão
diferentes? Isso é culpa
do parquê? Não, isso é culpa
do mundo, né? Então, assim,
o que que acontece aqui? Não
é porque
a partição é 128 megas que
você tem que usar 128 megas
o resto da sua vida. É
que você
sempre precisa se preocupar, você sempre
precisa se preocupar.
A questão que é importante você
entender é a seguinte, ele vai
carregar pra memória e quando ele
for colocar
o dado embaixo, ele vai tentar
tentar fazer uma melhor
configuração automática.
Tem formas de como você forçar
isso. Você pode dar um repartition
pra poder escrever, mas
a gente não vai chegar
lá porque é avançado, tá?
Até aqui, vamos lá,
gente, a gente tem muita coisa
pra ver, acredite
que vai quebrar ainda muita cabeça
de
muita gente. Tudo certo até aqui?
Matheus?
Tá. Crystal clear.
Então, crystal clear, né?
Então, aqui a gente já leu,
que bom que vocês estão entendendo
em detalhe, né? Essa é a
ideia. A gente pausa um pouquinho
mais do horário, mas vale a
pena.
Alô, eu quero ler o S3,
o dado.
Eu posso fazer? Claro
que você pode fazer. Olha o
que a gente
tá fazendo aqui. Passando
configurações adicionais
para o meu
notebook e
ah, Luan, mas e se eu
quiser fazer isso
fora o Databricks, eu consigo? Claro.
Você vai ter que ter as
bibliotecas pra acessar
o S3. Basicamente, aqui eu tô
acessando o meu S3, que tá
em algum
lugar, é um data lake
que está dentro do Kubernetes, que
é
o minha I .O.
E olha que legal, depois que
eu seto isso,
tá? Pode ficar tranquilo,
Júnior. Todos esses conceitos mais
avançados vão estar na série do
Spark,
eu vou explicar aqui também
na sexta -feira, que a gente
vai construindo
o conceito, eu não vou chegar
com coisas
avançadas agora, tá? Pra gente, pra
ficar fácil pra quando chegar no
Repartition.
E quando a gente usa ele,
tá?
Mas eu vou passar por isso,
fica tranquilo.
Fica tranquilo que
Repartition vocês vão aprender. Repartition
e Coalesce são coisas que vocês
vão ficar
fudidos de bom, porque são dois
problemas
que puta merda. Tem tanto
conteúdo na internet difícil de entender,
tem tanto problema nisso
que Deus me livre, entendeu? Então
fica tranquilo
que a gente seca isso depois.
E no final ninguém sabe que
porra é, o que
Coalesce faz, o que Repartition faz,
faz a mesma merda. Será? Não.
Então a gente vai entender em
detalhe depois.
Então o que que eu vou
ler aqui, ó?
Eu vou falar o que? Spark
leia
o protocolo S3, ó.
Porque eu tô acessando o S3.
Eu vou carregar
os dados.
Vou criar uma view, como eu
já
mostrei pra vocês, e escrever esses
dados dentro
do meu Data Lake. Ou seja,
agora eu acessei um outro
storage, tá vendo? Ah, Luan,
então eu posso acessar outros lugares.
Pode, você pode fazer o que
você quiser.
Ah, eu quero acessar o SQL
Server.
Eu consigo fazer isso, Matheus?
Vamos ver se eu consigo fazer,
Matheus.
Consigo. Eu consigo acessar
isso pelo JDBC.
Então, meu querido, maravilhoso Matheus,
né? A gente tá passando
aqui em clear text,
mas eu posso fazer o que?
Pegar o secret
e botar o secret aqui
pra que isso fique bonitinho. Então
eu
vou ler, né? Tô montando o
meu JDBC,
você vai ter acesso a todos
esses caras
que estão lá. Eu quero trazer
o dado do SQL.
É a melhor forma?
Né? Não.
Por quê? A gente vai entrar
em detalhe agora.
Então quando eu faço uma query,
vamos supor
que eu quero ir lá no
SQL, tá? Vou fazer um query,
fazer uma query puxando alguns dados
específicos aqui e carregar pra
dentro do data frame. Então eu
falo o quê? Spark
e leia JDBC.
Vocês entenderam como data frame é
sexy?
Fala pra mim se sim. Sexy,
sim
ou não. Spark e
leia JSON. Spark e leia Parquet.
Spark e leia JDBC. Spark e
leia
escreva Delta. Spark
e escreva Parquet. É muito
tranquilo de você escrever.
Né? DF igual a
sexy. É muito sexy, tá?
É muito clean, é muito tranquilo.
Né? Isso seria similar a
Spark .sql? Exatamente.
Ele é foda porque ele democratiza
todas
as informações em uma única forma
de tratar.
Exatamente. Então olha só.
Trouxe o dado, fiz um
select,
trouxe para dentro
e li pra dentro de um
data frame.
Isso aqui já tá distribuído dentro
de um
data frame. Olha que coisa legal.
Tá?
É... queria mostrar
uma coisa aqui, Matheusinho.
É...
é...
é...
é...
é...
é...
é...
é...
é...
é...
é...
O problema é que 99 %
da galera que trabalha com Spark
não sabe.
Olha que delícia isso aqui.
Olha o tanto de partições que
você carregou.
Uma.
Pergunta, foi distribuído?
Não.
Não foi.
Lindo, né?
Então, pega o animal de teta,
mete uma query no SQL,
traz um bilhão de registros, demora
três horas e o nego fala
que o Spark é uma bosta.
Agora você vai sair daqui entendendo,
até sexta -feira, que não.
Não é assim.
Existem formas de depenar, né?
Mas eu quero que você entenda
que se você não souber escrever
certo,
você está carregando para uma partição
e você não está distribuindo o
seu sistema.
Beleza?
Quando você lê do banco direto,
você está usando o processamento do
próprio banco, correto?
Perfeito.
Então, eu estou pedindo a query
para ir lá, ele está fazendo
a query
e carregando para dentro do data
frame.
Em cima disso que você mostrou
sobre a criação do data frame,
depois eu tenho um creative view,
eu posso usar no Spark SQL,
posso deletar o data frame e
inicialmente limpar os dados da memória?
Automaticamente ele vai fazer isso, José.
Então, se não houver nenhuma dependência
e você vai adicionando processos que
não tem,
ele vai limpando, garba de coletor,
faz isso.
Você não precisa se preocupar com
isso, tá?
Então, se você estiver carregando coisas
que sobrepõem ele,
ele entende o que tem que
sair e o que não.
Então, você também não precisa fazer
isso no data frame.
Ele controla isso.
Se não for pedir demais, você
pode depois mostrar um exemplo usando
uma SQL,
eu já mostrei lá no primeiro
notebook, está aqui.
Aqui, então você vai chamar o
scope get e aí você pode
colocar o que você quiser dentro
daqui.
Então, na hora que você for
configurar o que você quiser dentro
do notebook,
você vai criar aqui, vai criar
o nome do scope e aqui,
vai colocar o que está aqui,
obfuscado e você vai chegar aqui.
Vai chamar get e aí vai
ter as informações de configuração
para você não botar isso dentro
do local, beleza?
Então, fique tranquilo.
Fica alguma coisa em cache?
É necessário ficar limpando o cache?
Ótima pergunta.
E se eu disser para você
que não?
Que legal, né?
Então, não, você não precisa com
o data frame se preocupar com
isso também, tá?
Você pode forçar com que esse
dado fique em cache.
Eu vou falar disso durante a
semana.
Mas, geralmente, a gente...
A gente não usa isso.
É tipo um pin table, tá
ligado?
Mas a gente não usa isso.
A gente deixa o Spark lidar
com o garbage collector, com ele.
Ele é foda nisso.
De novo, o Deleg que ele
é foda para isso.
Então, você não precisa se preocupar
com isso também.
Só escreve, só entrega, tá?
Vou falar dos pontos positivos e
negativos, só entrega.
Nesse caso, se for direto em
banco, vale a pena usar o
filter pushdown.
Existem algumas configurações que você pode
fazer aqui
que depois eu vou entrar em
detalhes.
Por exemplo, aqui, ó.
Aqui é uma configuração legal de
você ver.
Então, ó, vou carregar isso aqui
num outro.
No outro data frame.
Então, aqui eu posso passar algumas
configurações.
Olha isso aqui.
Lower bound, upper bound and new
partitions.
Agora sim, eu consigo começar a
trabalhar a minha informação
de distribuição de partições.
Simplesmente por adicionar um parâmetro.
Então, não sei se vai ter
suficiente aqui.
Olha só, que legal.
Vocês sabiam disso?
Sim.
Sim.
Sim.
Sim.
Adicionar um comando aqui, você escala
o seu JDBC.
Tcharam!
Né?
Se você não colocar, você se
lascou.
Isso é uma puta parada simples,
que a galera se lasca big
time
e que o cara fala mal
do Spark.
Mas eu ainda recomendo você não
fazer isso aqui.
Beleza?
Pode fazer?
Pode.
Tem ferramentas que fazem isso.
Ou seja, isso é pra estar
no nosso processo na camada de
ingestão, correto?
Então, entra na camada de ingestão
e depois...
ille .click e o Spark vai
Basicamente, você oastiando só em um
Arkansas.
Então, claro, esse projeto mantém muitos
Mas aqui nós estamos usando Spark!
E pra um riscoicano, eu vou
mas aqui nós estamos usando Otubre
lá sobre o meu aqui.
A présentação
porque essa Ohh não navegaram no
meu delicate
e puedes descobertar.
Isso !!!
Muito bem.
Meu amigo!
Pronto.
vocês sabem, se você estivesse usando
RDD, toda vez que você
tivesse que ler alguma coisa, você
tinha que falar
a quantidade de partições.
Então, eu estou lendo um text
file, eu tenho que falar
cara, eu quero que esse text
file esteja em tantas partições.
Você tem noção do que você
precisa nem
se preocupar com isso hoje? Você
só lê.
Então, fica a dica aí.
Vou dar uma outra dica pra
vocês
bem legal, mas
não recomendo vocês usarem, mas
você pode usar.
Com respeito do GCP, do GC,
do Garage Collector, para isso,
tem que colocar config específico?
Não. Você não precisa.
Ele vai controlar pra você automaticamente.
Não mexa no GC. Ele vai
fazer isso pra você.
É possível ler de
Docker? O Docker
tem uma aplicação, ele tem um
processo ali dentro.
Dependendo do que ele está fazendo,
se isso está
aberto ou não pra você acessar,
sim.
Uma
forma de você ler.
Se eu quiser fazer um API
request,
isso aqui é uma dica muito
importante.
Se eu quiser fazer um API
request,
se eu tiver que fazer um
API request, eu consigo
fazer isso de forma escalável?
Não sei.
Fica mais complicado.
Por que
fica complicado?
Porque, de novo, não é um
sistema pra isso.
Então, vamos
ver como a gente consegue fazer
isso de uma forma menos dolorosa.
Henrique,
você pega o mínimo e o
máximo
do seu,
do seu,
da sua tabela e você calcula
a quantidade de partições que você
queira que
cada pedaço daquele entre, basicamente,
tá? Mas fica tranquilo que eu
vou entrar em detalhe
lá na sexta -feira sobre isso,
tá?
Fica tranquilo. Então,
olha só. O que que a
gente vai fazer?
O Spark é uma arquitetura distribuída,
o Spark permite que você use
Python.
Python, tá? Então, isso aqui é
Python.
Requests e JSON são
bibliotecas de Python.
Então, o que que eu vou
fazer aqui, ó?
Eu vou fazer uma requisição
nessa URI,
URL, pedindo
100 e vou salvar isso dentro
de um data frame, beleza?
Eu tô salvando isso dentro
de um data frame em Python.
Isso tá dentro, na verdade, isso
tá dentro de um dicionário,
desculpe, Python. Isso é um dicionário
de
Python. Qual é o tipo dele?
Uma
lista, nem dicionário, é uma lista.
Então, a lista.
Pergunta, essa lista é
escalável?
Isso é uma lista no Python
que você colocou
no Spark. Isso é escalável?
Sim ou não?
Não, porque
não é um data frame, tá?
Ele só seria se ele fosse
um data frame.
Então,
o que que a gente vai
fazer? Olha só que foda
agora. Transformar o que...
Ah, e outra coisa muito básica
de vocês entenderem. Se vocês
fizerem, se vocês estiverem trabalhando
com Python, usando Spark, eu já
vi
gigantescos fazendo isso,
você não tá usando arquitetura distribuída,
você tá usando
a cabeça, só o driver.
Então, o que que vai acontecer
eventualmente é que você vai
explodir a memória do driver e
vai parar todos os
seus aplicações. Coisa tranquila, tá?
Então, isso aqui explode o
Spark, porque não é distribuído.
Tá? Então, pode fazer? Pode pra
algumas coisas, mas é melhor prática?
Absolutamente não.
A ideia é o quê? Trazer
os dados para o
Data Lake pra posteriormente processar
essa informação. Beleza?
E aí, o que que eu
vou fazer, gente?
Spark, olha só
que legal essa função, create
data frame. Eu vou passar a
lista
e o esquema que eu
esquematizei ela.
O que que vai acontecer agora?
Tchanam!
Vai Spark .sql .dataframe.
Agora você tem
isso escalado.
Então, se você quiser um dia
fazer uma
revisão por API ou algo desse
tipo,
pode fazer.
Mas tome cuidado, porque
você sabe que quando você traz
o dado, você tá carregando a
cabeça
do Spark, você não tá distribuindo.
Você vai ter que distribuir aqui.
Tá?
Ficou claro?
Vamos lá, show.
Agora, velho,
a pergunta
que não quer calar,
Matheus. O que que é pandas,
paz, Spark, koalas, o que que
tudo isso tem a ver? Vamos
chorar
agora? Você também pode
falar foda hoje pra gente terminar
às dez e cinquenta sem feliz?
Claro, claro.
Então, vamos lá. Fica tranquilo. Vamos
ver
isso aqui.
Isso aqui é lindo.
Gente,
eu fui no cliente gigante,
o cara utilizava Spark, ele falou,
Luan,
eu tô com esse problema de
out of memory,
não sei o que, tá explodindo
o Spark,
mas eu tenho um Spark com
cento e quanto, Matheus, aqui?
Com cento e doze giga de
RAM e eu
explodi o Spark, velho. Por que
que
isso tá acontecendo, mano?
Olha só, eu usei o pandas,
porque, porra,
pandas é usado em todo lugar,
eu amo
pandas.
O pandas, pandas é foda.
Legal. Então, eu cheguei lá,
import pandas aqui no Databricks,
SPD, dei um
pd .redspark
e tô tentando ler um arquivo
aqui
que tem meio bilhão de linhas
e
dá um display nele.
E, porra, por que que tá
dando running comment?
Pai, não tô entendendo.
Era pra, poxa, era pra funcionar.
Por quê?
Porque, na verdade,
eita,
Matheus,
this is often
caused by an OOM
error that causes the
connection to the Python wrapper to
be
closed. Check your query's
memory usage.
Why? Eu tenho cento e dezesseis
giga de RAM. Não é
possível que um parquê
tenha cento e vinte giga de
RAM,
Matheus. Eu explodi o Spark.
É, animal de teta.
O que que aconteceu aqui?
É que você usou os pandas.
Eu falei pra vocês ontem
que o pandas não era escalável.
Agora eu estou jogando
na cara de vocês aqui que
ele não é.
E você acabou de explodir
um cluster de cento e doze
giga de RAM. Por quê?
Porque pandas não é escalável.
Então,
a gente evita trabalhar
com isso porque não é escalável.
Então, você vai ter problema
de out of memory. Aí o
cara falava, eu assisti
o ambiente do cara, o cara
era tão grande que eu não
acreditei
que ele ia falar. Falei assim,
você tá
falando sério, né? Ele, por que
tá falando
sério? O cara tinha um notebook
gigante,
Matheus, com pandas. O cara fazia
uma
caraca, velho. Tudo com pandas.
Eu, tio, não dá, tio.
Ele, não, mas é
porque funciona. Eu falei, sim.
Morrer queimado aqui também é top.
Então, por ele
não ser escalável, ele vai rodar
em somente um nó,
pior, João. Ele vai rodar
na cabeça do Spark, que é
o
driver. Ele não vai nem
pegar no ZQ. Ele vai ler
no driver,
que é pior ainda.
Entendeu? Ou seja, ele vai rodar
só em lugar. E logo na
cabeça.
Coisa boa, melhor prática pra
engenharia de dados. É usar pandas
com um terabyte de dados.
Melhor prática.
Tá ligado? Aí,
o pai Spark veio e fez
tudo
virar um tesão. Você vem aqui,
Spark, leia parquê,
passa
o comando e dá um
display. E aí, aquele
cara que não executou,
ele vai executar, tiozão.
Você pode ter certeza que ele
vai
executar, porque ele é distribuído.
Tá vendo que tesão a gente
vê isso
funcionando de verdade? A gente tá
falando
de meio bilhão de linhas, ó.
Quatro, quatrocentos e
trinta e quatro milhões, cento e
dezessete
mil seiscentos e cinquenta e oito
registros.
A gente tem quase meio bilhão
de registros aqui. E eu li
em oito ponto vinte e
três segundos. Só que
você entende que pro cara que
escrevia
pandas, um animal ali
de teta, que tava todo feliz,
e tal, ele vai ter que
reescrever o código dele?
Então, sim. Mas agora,
depois do Spark três ponto dois
antes,
a gente tinha que abrir
uma biblioteca e escrever
da mesma forma. Bem -vindos
ao Spark três ponto dois.
Olha o que que você faz.
Importe
pa Spark
pontos pandas, Matheus. Chora,
chora. Cadê o choro aqui?
Não, não, não.
Eu não posso só fazer isso,
é funcionar.
Quatro ponto cinquenta e dois.
Então, tio, tchau.
Acabou, né?
Então, o cara que escreve
pandas lá embaixo agora. Boa noite.
Boa noite, estou indo embora.
O cara que escreve
pandas lá embaixo agora escreve
pandas aqui em cima, escalava. Então,
se
você for escrever pandas lá
na sua aplicação Pythonzinho, que não
dá nada,
e você fizer um import
um import pa Spark ponto
pandas e escrever sua aplicação inteira,
você só pega, copia e cola,
joga o código.
Código escala com pandas.
Isso é foda, isso é foda.
Mais foda é essa demo aqui.
Pra mim. Que é.
Vamos agora ao bicho de sete
cabeças pro último foda hoje do
dia pra gente entrar no
pipeline rapidinho que eu vou deixar
pra vocês.
Gente, eu escuto pessoas, eu posso
falar animal de
teta de novo, já tá muito
repetida, porque, velho, é massa quando
o cara fala
que pa Spark não executa igual
a escala, que escala é a
linguagem mais rápida do
pagode dentro do Spark. Então, vamos
ver e dá pra rodar o
SQL em cima do
data frame do pandas? Sim, cê
dá um create external view tal
e transforma em
SQL e usa, tá? Tranquilo. Consigo
fazer esses testes em Data Bricks
Community? Exatamente, tá?
Então, sim, consegue. Gente, olha só
onde o data frame tá. Agora,
você tá vendo isso aqui na
cara de
vocês, ó. Spark SQL, Data Frame
API e Catalyst Optimizer. Ele é
um query executor, ele é um
query plan, ele é o cara
mais inteligente que vai criar um
plano de execução pra executar o
teu
comando. Acho que podemos fazer um
merge entre o animal de tetas
e morrer queimado, o famoso animal
de tetas. Tô, tô.
Curti, curti, ficou bom. Então, iremos
falar de animal de teta tostado,
um verdadeiro churrasquinho.
Então, vamos lá. Quatrocentos e trinta
e quatro bilhões cento e setenta
milhões seiscentos e
cinquenta e oito. Desculpa, setenta mil
seiscentos e cinquenta e oito. Vamos
ler em escala, tá? Então,
vou clicar aqui ó, ler em
escala, beleza? Então, eu tô falando
o Val, o nome do meu
data frame,
Spark, Layer Park, parece muito simples,
usando a escala pra ler,
né?
or replace temp view
display, que é uma ação.
Então, quando eu executar uma ação,
o Spark vai lá e vai
executar alguma coisa
que eu pedi pra ele de
fato mostrar
ou executar um processo que vai
gerar
alguma coisa. Então, vamos ver aqui
quanto tempo ele vai demorar.
Então, aqui eu tô lendo esse
data frame
em vinte e
nove segundos.
Beleza? Show. Vou executar de novo.
Vou executar duas
vezes, né? Porque, poxa,
vou executar duas vezes, né? Beleza?
Três segundos. Top.
Agora, eu vou ler com
mais Spark.
O mesmo meio bilhão.
Legal.
1 .77 segundos.
E agora eu vou ler com
SQL também.
Vou dar uma temporary view.
Select from.
2 .86 segundos.
Cadê o seu Deus aqui? Qual
a diferença entre eles?
Já tive quem escreveu
código que disseram que esse calo
era mais
performático. É, morrer queimado,
tostado, também, né?
Era muito legal. Tá aqui, gente.
Tá? O que que acontece?
Na verdade. A gente vai ver
que
existe alguns casos que você vai
usar
esse calo que você precisa porque
realmente é.
Mas a Microsoft, por exemplo,
trouxe o .NET pra pasta Spark.
Não sei se vocês sabem disso,
mas agora você pode
escrever C Sharp e F Sharp
no Spark.
Por quê? Porque eles criaram uma
distração de data frame.
Então, o que
o data frame faz? O data
frame
faz com que você consiga escrever
códigos
em qualquer linguagem que esteja
dentro do data frame em tempos
iguais. Então, não é
para você ter discrepâncias
extremamente loucas. É pra você
ter pipelines que executam
tão rápido quanto. Então, aqui, ó.
Escala, 377 segundos.
375. Python,
433. Net, 406.
Então, a ideia é que você
tem
coisas que executam isso, porque o
frame faz isso pra você.
Por isso que o Spark é
tão fucking...
Nossa! Beleza?
Então,
eu vou deixar vocês com isso
hoje. Amanhã
a gente entra às seis e
meia
pra ver um pipeline inteiro em
batch.
Por que que eu vou fazer
isso? Porque isso vai ficar
muito tarde, tá? E a gente
não
vai ver isso com cuidado. Mas
olha o tamanho
do batch que a gente vai
escrever.
E eu quero
de pouquinho em pouquinho passar
em detalhe com vocês pra gente
perder
nada e deixar isso
bonitinho. Tudo bem?
Me dá aí um tudo bem.
Ou vocês querem ir agora?
Eu não quero explicar agora, porque
eu acho que a gente já
explodiu a cabeça.
Vamos relaxar. Amanhã a gente
entra às seis e meia.
Eu quero que vocês façam um
favor pra mim.
Vão no LinkedIn antes de dormir
e, cara, fala o que você
mais
curtiu do dia de hoje lá
pra gente comentar.
Vamos explodir o LinkedIn
pra que a gente ganhe visibilidade
como um
todo e a gente possa curtir.
Então, vamos tirar uma fotinha aí,
Matheus.
Por quê?
Por que eu tô falando pra
gente fazer
isso? Beleza,
vou mostrar a tela do quatro,
nove, nove, nove.
Vou mostrar, vou deixar na tela
lá
da comparação do tempo, que é
legal.
Deixa eu ver se consigo deixar
aqui
de uma forma que vocês possam
pegar.
Escala e faz par que vocês
consigam
pegar o tempo dos dois também.
Reduz um pouco o tamanho,
mais um pouquinho.
Assim?
É, talvez dá.
Ah, não, também.
Não pegou, mas...
Fecha aí o negócio.
Não, eu queria fechar
o molde aqui.
Calma aí. Aqui demorou três e
tantos
e aqui eu não consigo ver.
Então, o que eu vou fazer?
Cadê, cadê, cadê?
Só um minutinho.
Ah, não, mas vai limpar o
tempo também.
É, não vai dar. Vai ter
que ficar assim.
É que eu queria...
É que eu queria entregar essa
imagem,
porque é uma imagem foda.
Pra quem não tá no treinamento,
vê isso.
Tipo, porque o cara
fica, tipo, caraca.
Né?
Aqui, ó.
3 .87 e 1 .7.
Spark versus tal.
Não conheço, não consegue abrir
em duas páginas, em duas guias.
Consigo.
Se você preferir, eu consigo sim.
Então, calma aí.
Já tamo aqui, né?
Não?
Em duas guias.
Ah, mas eu não sei se
vai aparecer pra vocês.
É, não, não vai.
Vocês tiraram o foco antes?
Na verdade, eu vou fazer melhor.
Eu vou tirar um print pra
vocês
e eu vou botar no WhatsApp.
Melhor ainda, né?
Né, Matheus?
Isso. Melhor.
Eu vou tirar aqui
e eu mando pra vocês agora
que a gente acabar.
Vai lá, posta, agita
pra galera já falar.
E tá bom.
Roda igual.
Então, acho que essa provocação é
legal
com as pessoas que te seguem
no LinkedIn.
Essa provocação é legal pra te
dar audiência.
A gente vai repostar.
A gente tem quase 15 mil
seguidores.
Então, isso também dá um boom.
Pra que a gente seja notado
também
e que a gente faça, mostre
esse conteúdo
que, cara, muita gente trabalha com
Spark
e não sabe dessas coisas.
Que são coisas básicas, teoricamente.
Mas, ninguém consegue.
Vamos gerar 400 e quê?
Cara, seria top isso, né?
Mas, beleza, gente.
Eu espero que vocês tenham gostado.
Ficou um curtido pra caramba.
Me diz aí o que vocês
acharam, se vocês curtiram ou não,
sim.
E a mãe...
E a mãe...
E a mãe...